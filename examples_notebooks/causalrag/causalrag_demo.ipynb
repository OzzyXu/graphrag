{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Causal Search Demo - Query and Context Analysis\n",
    "\n",
    "This notebook demonstrates how to use the Causal Search method in GraphRAG and inspect the context used to generate responses. Causal Search performs causal analysis on knowledge graphs through a two-stage process:\n",
    "\n",
    "1. **Stage 1**: Extract extended graph information (k + s nodes) and generate causal analysis report\n",
    "2. **Stage 2**: Use the causal report to generate final response to user query\n",
    "\n",
    "## Key Features\n",
    "\n",
    "- Extended node extraction beyond local search limits\n",
    "- Two-stage processing for comprehensive causal analysis\n",
    "- Automatic output saving to data folders\n",
    "- Configurable parameters for retrieval breadth and context proportions\n",
    "- Integration with existing GraphRAG pipeline\n",
    "- **Context inspection**: See exactly what data was used to generate responses\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "Before running this notebook, ensure you have:\n",
    "\n",
    "1. Run the GraphRAG indexing pipeline to generate entities, relationships, and community reports\n",
    "2. Set up your configuration in `settings.yaml` with causal search parameters\n",
    "3. Configured your language models and API keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import asyncio\n",
    "import json\n",
    "import pandas as pd\n",
    "import tiktoken\n",
    "from pathlib import Path\n",
    "from typing import Any, Dict, List\n",
    "\n",
    "# GraphRAG imports\n",
    "from graphrag.config.enums import ModelType\n",
    "from graphrag.config.load_config import load_config\n",
    "from graphrag.config.models.language_model_config import LanguageModelConfig\n",
    "from graphrag.language_model.manager import ModelManager\n",
    "from graphrag.query.context_builder.entity_extraction import EntityVectorStoreKey\n",
    "from graphrag.query.factory import get_causal_search_engine\n",
    "from graphrag.query.indexer_adapters import (\n",
    "    read_indexer_covariates,\n",
    "    read_indexer_entities,\n",
    "    read_indexer_relationships,\n",
    "    read_indexer_reports,\n",
    "    read_indexer_text_units,\n",
    ")\n",
    "from graphrag.query.structured_search.causal_search.search import CausalSearchError\n",
    "from graphrag.query.structured_search.causal_search.search import CausalSearch\n",
    "from graphrag.query.structured_search.local_search.mixed_context import (\n",
    "    LocalSearchMixedContext,\n",
    ")\n",
    "from graphrag.vector_stores.lancedb import LanceDBVectorStore\n",
    "\n",
    "# IPython display utilities\n",
    "from IPython.display import Markdown, display"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration Setup\n",
    "\n",
    "First, let's load the GraphRAG configuration and set up the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Configuration loaded successfully\n",
      "üìÅ Root directory: /Users/chuan/Projects/graphrag/ragtest\n",
      "üîß Causal search s_parameter: 3\n",
      "üîß Causal search top_k_entities: 10\n",
      "üîß Causal search max_context_tokens: 12000\n"
     ]
    }
   ],
   "source": [
    "# Configuration setup\n",
    "ROOT_DIR = Path(\"/Users/chuan/Projects/graphrag/ragtest\")  # Adjust this path to your project root\n",
    "CONFIG_FILE = None  # Use default settings.yaml\n",
    "\n",
    "# Load configuration\n",
    "try:\n",
    "    config = load_config(ROOT_DIR, CONFIG_FILE)\n",
    "    print(\"‚úÖ Configuration loaded successfully\")\n",
    "    print(f\"üìÅ Root directory: {ROOT_DIR}\")\n",
    "    print(f\"üîß Causal search s_parameter: {config.causal_search.s_parameter}\")\n",
    "    print(f\"üîß Causal search top_k_entities: {config.causal_search.top_k_mapped_entities}\")\n",
    "    print(f\"üîß Causal search max_context_tokens: {config.causal_search.max_context_tokens}\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Failed to load configuration: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading\n",
    "\n",
    "Load the required data from your GraphRAG pipeline outputs using the same functions as the visualization notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data loading setup\n",
    "INPUT_DIR = f\"{ROOT_DIR}/output\"\n",
    "LANCEDB_URI = f\"{INPUT_DIR}/lancedb\"\n",
    "\n",
    "COMMUNITY_REPORT_TABLE = \"community_reports\"\n",
    "COMMUNITY_TABLE = \"communities\"\n",
    "ENTITY_TABLE = \"entities\"\n",
    "RELATIONSHIP_TABLE = \"relationships\"\n",
    "COVARIATE_TABLE = \"covariates\"\n",
    "TEXT_UNIT_TABLE = \"text_units\"\n",
    "COMMUNITY_LEVEL = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load tables to dataframes\n",
    "\n",
    "#### Read entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Loaded 140 entities\n",
      "‚úÖ Loaded 21 communities\n"
     ]
    }
   ],
   "source": [
    "# read nodes table to get community and degree data\n",
    "entity_df = pd.read_parquet(f\"{INPUT_DIR}/{ENTITY_TABLE}.parquet\")\n",
    "community_df = pd.read_parquet(f\"{INPUT_DIR}/{COMMUNITY_TABLE}.parquet\")\n",
    "\n",
    "print(f\"‚úÖ Loaded {len(entity_df)} entities\")\n",
    "print(f\"‚úÖ Loaded {len(community_df)} communities\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read relationships"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Loaded 191 relationships\n"
     ]
    }
   ],
   "source": [
    "relationship_df = pd.read_parquet(f\"{INPUT_DIR}/{RELATIONSHIP_TABLE}.parquet\")\n",
    "relationships = read_indexer_relationships(relationship_df)\n",
    "\n",
    "print(f\"‚úÖ Loaded {len(relationship_df)} relationships\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read other data tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ÑπÔ∏è  No covariates found, proceeding without covariates\n",
      "‚úÖ Loaded 42 text units\n",
      "‚úÖ Loaded 21 community reports\n"
     ]
    }
   ],
   "source": [
    "# Load text units\n",
    "text_unit_df = pd.read_parquet(f\"{INPUT_DIR}/{TEXT_UNIT_TABLE}.parquet\")\n",
    "text_units = read_indexer_text_units(text_unit_df)\n",
    "\n",
    "# Load community reports\n",
    "report_df = pd.read_parquet(f\"{INPUT_DIR}/{COMMUNITY_REPORT_TABLE}.parquet\")\n",
    "reports = read_indexer_reports(report_df, community_df, COMMUNITY_LEVEL)\n",
    "\n",
    "# Load covariates if they exist\n",
    "try:\n",
    "    covariate_df = pd.read_parquet(f\"{INPUT_DIR}/{COVARIATE_TABLE}.parquet\")\n",
    "    claims = read_indexer_covariates(covariate_df)\n",
    "    covariates = {\"claims\": claims}\n",
    "    print(f\"‚úÖ Loaded {len(claims)} covariates\")\n",
    "except FileNotFoundError:\n",
    "    print(\"‚ÑπÔ∏è  No covariates found, proceeding without covariates\")\n",
    "    covariates = {}\n",
    "\n",
    "print(f\"‚úÖ Loaded {len(text_units)} text units\")\n",
    "print(f\"‚úÖ Loaded {len(reports)} community reports\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Setup\n",
    "\n",
    "Set up the language models and context builder using the same approach as the visualization notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Models and vector store setup complete\n"
     ]
    }
   ],
   "source": [
    "# Get model configurations from the loaded config\n",
    "chat_model_config = config.get_language_model_config(\"default_chat_model\")\n",
    "embedding_model_config = config.get_language_model_config(\"default_embedding_model\")\n",
    "\n",
    "# Create chat model\n",
    "chat_model = ModelManager().get_or_create_chat_model(\n",
    "    name=\"causal_search\",\n",
    "    model_type=chat_model_config.type,\n",
    "    config=chat_model_config,\n",
    ")\n",
    "\n",
    "# Create token encoder\n",
    "token_encoder = tiktoken.encoding_for_model(chat_model_config.model)\n",
    "\n",
    "# Create embedding model\n",
    "text_embedder = ModelManager().get_or_create_embedding_model(\n",
    "    name=\"causal_search_embedding\",\n",
    "    model_type=embedding_model_config.type,\n",
    "    config=embedding_model_config,\n",
    ")\n",
    "\n",
    "# Create vector store\n",
    "description_embedding_store = LanceDBVectorStore(\n",
    "    collection_name=\"default-entity-description\",\n",
    ")\n",
    "description_embedding_store.connect(db_uri=LANCEDB_URI)\n",
    "\n",
    "print(\"‚úÖ Models and vector store setup complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Context Builder Setup\n",
    "\n",
    "Create the context builder using the same parameters as the visualization notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Context builder setup complete\n"
     ]
    }
   ],
   "source": [
    "entities = read_indexer_entities(entity_df, community_df, COMMUNITY_LEVEL)\n",
    "\n",
    "# Context builder parameters (same as visualization notebook)\n",
    "context_builder_params = {\n",
    "    \"text_unit_prop\": 0.5,\n",
    "    \"community_prop\": 0.1,\n",
    "    \"conversation_history_max_turns\": 5,\n",
    "    \"conversation_history_user_turns_only\": True,\n",
    "    \"top_k_mapped_entities\": 10,  # Increased for causal search\n",
    "    \"top_k_relationships\": 10,     # Increased for causal search\n",
    "    \"include_entity_rank\": True,\n",
    "    \"include_relationship_weight\": True,\n",
    "    \"include_community_rank\": False,\n",
    "    \"return_candidate_context\": False,\n",
    "    \"embedding_vectorstore_key\": EntityVectorStoreKey.ID,\n",
    "    \"max_tokens\": 80_000,\n",
    "}\n",
    "\n",
    "# Create context builder\n",
    "context_builder = LocalSearchMixedContext(\n",
    "    community_reports=reports,\n",
    "    text_units=text_units,\n",
    "    entities=entities,\n",
    "    relationships=relationships,\n",
    "    covariates=covariates,\n",
    "    entity_text_embeddings=description_embedding_store,\n",
    "    embedding_vectorstore_key=EntityVectorStoreKey.ID,\n",
    "    text_embedder=text_embedder,\n",
    "    token_encoder=token_encoder,\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Context builder setup complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Causal Search Engine Setup\n",
    "\n",
    "Create the causal search engine with the same model parameters as the visualization notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-18 00:21:07.0491 - INFO - graphrag.query.structured_search.causal_search.search - Loaded causal discovery prompt from GraphRAG prompts\n",
      "2025-08-18 00:21:07.0492 - INFO - graphrag.query.structured_search.causal_search.search - Loaded causal summary prompt from GraphRAG prompts\n",
      "‚úÖ Causal search engine setup complete\n"
     ]
    }
   ],
   "source": [
    "# Model parameters (same as visualization notebook)\n",
    "model_params = {\n",
    "    \"max_tokens\": 4_000,  # Adjusted for gpt-4-turbo-preview\n",
    "    \"temperature\": 0.0,\n",
    "}\n",
    "\n",
    "# Create causal search engine directly (not using factory function)\n",
    "causal_search_engine = CausalSearch(\n",
    "    model=chat_model,\n",
    "    context_builder=context_builder,\n",
    "    token_encoder=token_encoder,\n",
    "    model_params=model_params,\n",
    "    context_builder_params=context_builder_params,\n",
    "    s_parameter=3,  # Additional nodes for causal analysis\n",
    "    max_context_tokens=12_000,\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Causal search engine setup complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Causal Search Example\n",
    "\n",
    "Now let's run a causal search query and inspect the context used to generate the response."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run causal search on sample queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Query: How can we use SAS Econometric products to help analyze the impact of different pricing strategies on business revenue?\n",
      "2025-08-18 00:21:32.0508 - INFO - graphrag.query.structured_search.causal_search.search - üöÄ Starting causal search for query: 'How can we use SAS Econometric products to help analyze the impact of different pricing strategies on business revenue?'\n",
      "2025-08-18 00:21:32.0508 - INFO - graphrag.query.structured_search.causal_search.search - üìä Parameters: s_parameter=3, max_context_tokens=12000\n",
      "2025-08-18 00:21:32.0509 - INFO - graphrag.query.structured_search.causal_search.search - üîç Step 1: Extracting extended nodes with k=10, s=3\n",
      "2025-08-18 00:21:32.0509 - INFO - graphrag.query.structured_search.causal_search.search - Requesting 26 nodes: (k=10 + s=3) * 2\n",
      "2025-08-18 00:21:33.0974 - WARNING - graphrag.query.context_builder.community_context - Warning: No community records added when building community context.\n",
      "2025-08-18 00:21:33.0979 - WARNING - graphrag.query.structured_search.local_search.mixed_context - Reached token limit - reverting to previous context state\n",
      "2025-08-18 00:21:33.0983 - INFO - graphrag.query.structured_search.causal_search.search - ‚úÖ Extracted 27 extended nodes (requested: 26)\n",
      "2025-08-18 00:21:33.0984 - INFO - graphrag.query.structured_search.causal_search.search - ‚úÖ Step 1 complete: Found 27 extended nodes\n",
      "2025-08-18 00:21:33.0984 - INFO - graphrag.query.structured_search.causal_search.search - üîç Step 2: Extracting graph information for 27 nodes\n",
      "2025-08-18 00:21:33.0991 - WARNING - graphrag.query.structured_search.local_search.mixed_context - Reached token limit - reverting to previous context state\n",
      "2025-08-18 00:21:33.0995 - INFO - graphrag.query.structured_search.causal_search.search - ‚úÖ Extracted graph information for 27 nodes\n",
      "2025-08-18 00:21:33.0995 - INFO - graphrag.query.structured_search.causal_search.search - Context length: 32880 chars\n",
      "2025-08-18 00:21:33.0995 - INFO - graphrag.query.structured_search.causal_search.search - Context records: 8 entities, 0 relationships, 3 text units\n",
      "2025-08-18 00:21:33.0995 - INFO - graphrag.query.structured_search.causal_search.search - ‚úÖ Step 2 complete: Graph context extracted\n",
      "2025-08-18 00:21:33.0996 - INFO - graphrag.query.structured_search.causal_search.search - üîç Step 3: Formatting network data for causal discovery prompt\n",
      "2025-08-18 00:21:34.0003 - INFO - graphrag.query.structured_search.causal_search.search - Formatted network data with 8 entities, 9 relationships\n",
      "2025-08-18 00:21:34.0003 - INFO - graphrag.query.structured_search.causal_search.search - ‚úÖ Step 3 complete: Network data formatted (76944 characters)\n",
      "2025-08-18 00:21:34.0003 - INFO - graphrag.query.structured_search.causal_search.search - üîç Step 4: Generating causal report using LLM\n",
      "2025-08-18 00:22:02.0860 - INFO - graphrag.query.structured_search.causal_search.search - Generated causal report of length 3869\n",
      "2025-08-18 00:22:02.0861 - INFO - graphrag.query.structured_search.causal_search.search - ‚úÖ Step 4 complete: Causal report generated (3869 characters)\n",
      "2025-08-18 00:22:02.0861 - INFO - graphrag.query.structured_search.causal_search.search - üîç Step 5: Generating final response using LLM\n",
      "2025-08-18 00:22:14.0289 - INFO - graphrag.query.structured_search.causal_search.search - Generated final response of length 2730\n",
      "2025-08-18 00:22:14.0290 - INFO - graphrag.query.structured_search.causal_search.search - ‚úÖ Step 5 complete: Final response generated (2730 characters)\n",
      "2025-08-18 00:22:14.0290 - INFO - graphrag.query.structured_search.causal_search.search - üîç Step 6: Saving outputs if configured\n",
      "2025-08-18 00:22:14.0291 - INFO - graphrag.query.structured_search.causal_search.search - Output saving disabled in configuration\n",
      "2025-08-18 00:22:14.0291 - INFO - graphrag.query.structured_search.causal_search.search - ‚úÖ Step 6 complete: Outputs saved\n",
      "2025-08-18 00:22:14.0303 - INFO - graphrag.query.structured_search.causal_search.search - üéâ Causal search completed successfully in 41.80s\n",
      "‚úÖ Causal search completed successfully!\n"
     ]
    }
   ],
   "source": [
    "# Sample query for causal analysis\n",
    "question = \"How can we use SAS Econometric products to help analyze the impact of different pricing strategies on business revenue?\"\n",
    "print(f\"üîç Query: {question}\")\n",
    "\n",
    "# Execute causal search\n",
    "try:\n",
    "    result = await causal_search_engine.search(question)\n",
    "    print(\"‚úÖ Causal search completed successfully!\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Causal search failed: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Display the response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìù Causal Search Response:\n",
      "==================================================\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "SAS Econometric products offer a robust suite of tools for analyzing the impact of various pricing strategies on business revenue. By leveraging these tools, businesses can gain insights into how different pricing models affect their financial outcomes and make informed decisions to optimize their pricing strategies. Here's how SAS Econometric products can be utilized for this purpose:\n",
       "\n",
       "### Data Preparation and Management\n",
       "\n",
       "Firstly, SAS Econometric products facilitate the preparation and management of data, which is crucial for any econometric analysis. Businesses can collect and integrate data from multiple sources, including sales, customer demographics, and market conditions. This comprehensive dataset is essential for analyzing the impact of pricing strategies, as it allows for a detailed examination of how different factors influence revenue.\n",
       "\n",
       "### Model Building and Estimation\n",
       "\n",
       "SAS Econometric products enable the building of sophisticated econometric models that can accurately capture the relationship between pricing strategies and business revenue. These models can include variables such as price levels, discount rates, and pricing structures (e.g., dynamic pricing, tiered pricing). By applying regression analysis and other statistical techniques, businesses can estimate the impact of these variables on revenue, taking into account potential confounders and interactions.\n",
       "\n",
       "### Scenario Analysis and Forecasting\n",
       "\n",
       "With the models built, businesses can use SAS Econometric products to conduct scenario analyses and forecasting. This involves simulating different pricing strategies and predicting their impact on revenue under various market conditions. Scenario analysis helps businesses understand the potential outcomes of their pricing decisions and assess the risks associated with each strategy.\n",
       "\n",
       "### Optimization and Decision Making\n",
       "\n",
       "Finally, SAS Econometric products support optimization techniques that can help businesses identify the optimal pricing strategy to maximize revenue. By analyzing the results from the econometric models and forecasts, businesses can determine the pricing strategy that offers the best balance between price and demand, considering factors such as customer price sensitivity and competitor pricing.\n",
       "\n",
       "### Conclusion\n",
       "\n",
       "In conclusion, SAS Econometric products provide a comprehensive toolkit for analyzing the impact of different pricing strategies on business revenue. Through data management, model building, scenario analysis, and optimization, businesses can leverage these tools to make data-driven pricing decisions. This approach not only helps in maximizing revenue but also in understanding the broader market dynamics and customer behaviors related to pricing."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Display as formatted Markdown\n",
    "print(\"\\nüìù Causal Search Response:\")\n",
    "print(\"=\" * 50)\n",
    "display(Markdown(result.response))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspecting the Context Data\n",
    "\n",
    "Now let's examine exactly what data was used to generate the response. This is the key part that shows the context filtering in action."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Context data overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîç Context Data Overview:\n",
      "==================================================\n",
      "üìä Entities: 8\n",
      "üîó Relationships: 0\n",
      "üìÑ Text Units: 0\n",
      "üèòÔ∏è  Community Reports: 0\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nüîç Context Data Overview:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "if hasattr(result, 'context_data') and result.context_data:\n",
    "    print(f\"üìä Entities: {len(result.context_data.get('entities', []))}\")\n",
    "    print(f\"üîó Relationships: {len(result.context_data.get('relationships', []))}\")\n",
    "    print(f\"üìÑ Text Units: {len(result.context_data.get('text_units', []))}\")\n",
    "    print(f\"üèòÔ∏è  Community Reports: {len(result.context_data.get('community_reports', []))}\")\n",
    "else:\n",
    "    print(\"‚ÑπÔ∏è  No context_data available in result\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspect entities used in context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üè∑Ô∏è  Entities Used in Context:\n",
      "==================================================\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>entity</th>\n",
       "      <th>description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SCROOGE</td>\n",
       "      <td>Ebenezer Scrooge is a central character known ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>EBENEZER SCROOGE</td>\n",
       "      <td>Ebenezer Scrooge, the main character of \"A Chr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>BOB CRATCHIT</td>\n",
       "      <td>Bob Cratchit, a central character in Charles D...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>MRS. CRATCHIT</td>\n",
       "      <td>Mrs. Cratchit, the wife of Bob Cratchit in \"A ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>FEZZIWIG</td>\n",
       "      <td>Fezziwig is a character renowned for his joyou...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>GHOST</td>\n",
       "      <td>The Ghost, a spectral entity, plays a pivotal ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>FRED</td>\n",
       "      <td>Fred, a character in \"A Christmas Carol,\" is S...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>TINY TIM</td>\n",
       "      <td>Tiny Tim, the youngest son of Bob Cratchit, is...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             entity                                        description\n",
       "0           SCROOGE  Ebenezer Scrooge is a central character known ...\n",
       "1  EBENEZER SCROOGE  Ebenezer Scrooge, the main character of \"A Chr...\n",
       "2      BOB CRATCHIT  Bob Cratchit, a central character in Charles D...\n",
       "3     MRS. CRATCHIT  Mrs. Cratchit, the wife of Bob Cratchit in \"A ...\n",
       "4          FEZZIWIG  Fezziwig is a character renowned for his joyou...\n",
       "5             GHOST  The Ghost, a spectral entity, plays a pivotal ...\n",
       "6              FRED  Fred, a character in \"A Christmas Carol,\" is S...\n",
       "7          TINY TIM  Tiny Tim, the youngest son of Bob Cratchit, is..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìà Total entities in context: 8\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nüè∑Ô∏è  Entities Used in Context:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "if hasattr(result, 'context_data') and result.context_data and 'entities' in result.context_data:\n",
    "    entities_df = result.context_data['entities']\n",
    "    if not entities_df.empty:\n",
    "        # Show key fields\n",
    "        display_cols = ['entity', 'description', 'rank', 'type']\n",
    "        available_cols = [col for col in display_cols if col in entities_df.columns]\n",
    "        \n",
    "        if available_cols:\n",
    "            display(entities_df[available_cols].head(10))\n",
    "        else:\n",
    "            display(entities_df.head(10))\n",
    "        \n",
    "        print(f\"\\nüìà Total entities in context: {len(entities_df)}\")\n",
    "    else:\n",
    "        print(\"‚ÑπÔ∏è  No entities found in context\")\n",
    "else:\n",
    "    print(\"‚ÑπÔ∏è  No entities data available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspect relationships used in context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîó Relationships Used in Context:\n",
      "==================================================\n",
      "‚ÑπÔ∏è  No relationships data available\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nüîó Relationships Used in Context:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "if hasattr(result, 'context_data') and result.context_data and 'relationships' in result.context_data:\n",
    "    relationships_df = result.context_data['relationships']\n",
    "    if not relationships_df.empty:\n",
    "        # Show key fields\n",
    "        display_cols = ['source', 'target', 'description', 'weight', 'rank']\n",
    "        available_cols = [col for col in display_cols if col in relationships_df.columns]\n",
    "        \n",
    "        if available_cols:\n",
    "            display(relationships_df[available_cols].head(10))\n",
    "        else:\n",
    "            display(relationships_df[available_cols].head(10))\n",
    "        \n",
    "        print(f\"\\nüìà Total relationships in context: {len(relationships_df)}\")\n",
    "    else:\n",
    "        print(\"‚ÑπÔ∏è  No relationships found in context\")\n",
    "else:\n",
    "    print(\"‚ÑπÔ∏è  No relationships data available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspect text units used in context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìÑ Text Units Used in Context:\n",
      "==================================================\n",
      "‚ÑπÔ∏è  No text units data available\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nüìÑ Text Units Used in Context:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "if hasattr(result, 'context_data') and result.context_data and 'text_units' in result.context_data:\n",
    "    text_units_df = result.context_data['text_units']\n",
    "    if not text_units_df.empty:\n",
    "        # Show key fields\n",
    "        display_cols = ['text', 'n_tokens']\n",
    "        available_cols = [col for col in display_cols if col in relationships_df.columns]\n",
    "        \n",
    "        if available_cols:\n",
    "            # Truncate text for display\n",
    "            display_df = text_units_df[available_cols].copy()\n",
    "            if 'text' in display_df.columns:\n",
    "                display_df['text'] = display_df['text'].str[:200] + '...'\n",
    "            display(display_df.head(10))\n",
    "        else:\n",
    "            display(text_units_df.head(10))\n",
    "        \n",
    "        print(f\"\\nüìà Total text units in context: {len(text_units_df)}\")\n",
    "    else:\n",
    "        print(\"‚ÑπÔ∏è  No text units found in context\")\n",
    "else:\n",
    "    print(\"‚ÑπÔ∏è  No text units data available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspect community reports used in context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üèòÔ∏è  Community Reports Used in Context:\n",
      "==================================================\n",
      "‚ÑπÔ∏è  No community reports data available\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nüèòÔ∏è  Community Reports Used in Context:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "if hasattr(result, 'context_data') and result.context_data and 'community_reports' in result.context_data:\n",
    "    community_reports_df = result.context_data['community_reports']\n",
    "    if not community_reports_df.empty:\n",
    "        # Show key fields\n",
    "        display_cols = ['community_id', 'summary', 'description']\n",
    "        available_cols = [col for col in display_cols if col in community_reports_df.columns]\n",
    "        \n",
    "        if available_cols:\n",
    "            # Truncate summary for display\n",
    "            display_df = community_reports_df[available_cols].copy()\n",
    "            if 'summary' in display_df.columns:\n",
    "                display_df['summary'] = display_df['summary'].str[:200] + '...'\n",
    "            display(display_df[available_cols].head(10))\n",
    "        else:\n",
    "            display(community_reports_df.head(10))\n",
    "        \n",
    "        print(f\"\\nüìà Total community reports in context: {len(community_reports_df)}\")\n",
    "    else:\n",
    "        print(\"‚ÑπÔ∏è  No community reports found in context\")\n",
    "else:\n",
    "    print(\"‚ÑπÔ∏è  No community reports data available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Context Filtering Analysis\n",
    "\n",
    "Let's analyze how the context filtering worked and compare it to the original data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare original vs. filtered data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîç Context Filtering Analysis:\n",
      "==================================================\n",
      "üìä Original data:\n",
      "   - Entities: 140\n",
      "   - Relationships: 191\n",
      "   - Text Units: 42\n",
      "   - Community Reports: 21\n",
      "\n",
      "üéØ Filtered context data:\n",
      "   - Entities: 8\n",
      "   - Relationships: 0\n",
      "   - Text Units: 0\n",
      "   - Community Reports: 0\n",
      "\n",
      "üìà Filtering ratios:\n",
      "   - Entities: 5.7% kept\n",
      "   - Relationships: 0.0% kept\n",
      "   - Text Units: 0.0% kept\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nüîç Context Filtering Analysis:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "print(f\"üìä Original data:\")\n",
    "print(f\"   - Entities: {len(entity_df)}\")\n",
    "print(f\"   - Relationships: {len(relationship_df)}\")\n",
    "print(f\"   - Text Units: {len(text_unit_df)}\")\n",
    "print(f\"   - Community Reports: {len(report_df)}\")\n",
    "\n",
    "if hasattr(result, 'context_data') and result.context_data:\n",
    "    print(f\"\\nüéØ Filtered context data:\")\n",
    "    print(f\"   - Entities: {len(result.context_data.get('entities', []))}\")\n",
    "    print(f\"   - Relationships: {len(result.context_data.get('relationships', []))}\")\n",
    "    print(f\"   - Text Units: {len(result.context_data.get('text_units', []))}\")\n",
    "    print(f\"   - Community Reports: {len(result.context_data.get('community_reports', []))}\")\n",
    "    \n",
    "    # Calculate filtering ratios\n",
    "    entity_ratio = len(result.context_data.get('entities', [])) / len(entity_df) * 100\n",
    "    relationship_ratio = len(result.context_data.get('relationships', [])) / len(relationship_df) * 100\n",
    "    text_unit_ratio = len(result.context_data.get('text_units', [])) / len(text_unit_df) * 100\n",
    "    \n",
    "    print(f\"\\nüìà Filtering ratios:\")\n",
    "    print(f\"   - Entities: {entity_ratio:.1f}% kept\")\n",
    "    print(f\"   - Relationships: {relationship_ratio:.1f}% kept\")\n",
    "    print(f\"   - Text Units: {text_unit_ratio:.1f}% kept\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Token usage analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üî¢ Token Usage Analysis:\n",
      "==================================================\n",
      "   - Entities: ~3,167 tokens\n",
      "\n",
      "üìä Total estimated tokens: ~3,167\n",
      "üéØ Target limit: 8,000 tokens (network data portion)\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nüî¢ Token Usage Analysis:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "if hasattr(result, 'context_data') and result.context_data:\n",
    "    # Estimate token usage for each data type\n",
    "    total_tokens = 0\n",
    "    \n",
    "    # Entities tokens\n",
    "    if 'entities' in result.context_data and not result.context_data['entities'].empty:\n",
    "        entities_text = result.context_data['entities'].to_string()\n",
    "        entities_tokens = len(token_encoder.encode(entities_text))\n",
    "        total_tokens += entities_tokens\n",
    "        print(f\"   - Entities: ~{entities_tokens:,} tokens\")\n",
    "    \n",
    "    # Relationships tokens\n",
    "    if 'relationships' in result.context_data and not result.context_data['relationships'].empty:\n",
    "        relationships_text = result.context_data['relationships'].to_string()\n",
    "        relationships_tokens = len(token_encoder.encode(relationships_text))\n",
    "        total_tokens += relationships_tokens\n",
    "        print(f\"   - Relationships: ~{relationships_tokens:,} tokens\")\n",
    "    \n",
    "    # Text units tokens\n",
    "    if 'text_units' in result.context_data and not result.context_data['text_units'].empty:\n",
    "        text_units_text = result.context_data['text_units'].to_string()\n",
    "        text_units_tokens = len(token_encoder.encode(text_units_text))\n",
    "        total_tokens += text_units_tokens\n",
    "        print(f\"   - Text Units: ~{text_units_tokens:,} tokens\")\n",
    "    \n",
    "    print(f\"\\nüìä Total estimated tokens: ~{total_tokens:,}\")\n",
    "    print(f\"üéØ Target limit: 8,000 tokens (network data portion)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced Context Inspection\n",
    "\n",
    "Let's look deeper into the context building process and see how the filtering decisions were made."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check context builder parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚öôÔ∏è  Context Builder Parameters Used:\n",
      "==================================================\n",
      "   - text_unit_prop: 0.5\n",
      "   - community_prop: 0.1\n",
      "   - conversation_history_max_turns: 5\n",
      "   - conversation_history_user_turns_only: True\n",
      "   - top_k_mapped_entities: 10\n",
      "   - top_k_relationships: 10\n",
      "   - include_entity_rank: True\n",
      "   - include_relationship_weight: True\n",
      "   - include_community_rank: False\n",
      "   - return_candidate_context: False\n",
      "   - embedding_vectorstore_key: EntityVectorStoreKey.ID\n",
      "   - max_tokens: 80000\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n‚öôÔ∏è  Context Builder Parameters Used:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "for key, value in context_builder_params.items():\n",
    "    print(f\"   - {key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check model parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ü§ñ Model Parameters Used:\n",
      "==================================================\n",
      "   - max_tokens: 4000\n",
      "   - temperature: 0.0\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nü§ñ Model Parameters Used:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "for key, value in model_params.items():\n",
    "    print(f\"   - {key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check causal search parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîç Causal Search Parameters:\n",
      "==================================================\n",
      "   - s_parameter: 3\n",
      "   - max_context_tokens: 12000\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nüîç Causal Search Parameters:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "print(f\"   - s_parameter: {causal_search_engine.s_parameter}\")\n",
    "print(f\"   - max_context_tokens: {causal_search_engine.max_context_tokens}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook demonstrates:\n",
    "\n",
    "1. **Data Loading**: Using the same functions as the visualization notebook\n",
    "2. **Model Setup**: Consistent with the visualization notebook approach\n",
    "3. **Context Building**: Same parameters and structure\n",
    "4. **Causal Search**: Extended node extraction and two-stage processing\n",
    "5. **Context Inspection**: Detailed analysis of what data was used\n",
    "6. **Filtering Analysis**: Understanding how context filtering works\n",
    "\n",
    "The key insight is that causal search uses **intelligent filtering** to ensure:\n",
    "- **LLM Compatibility**: Data fits within model context limits\n",
    "- **Relevance**: Most important entities/relationships are preserved\n",
    "- **Performance**: Efficient processing without context length errors\n",
    "\n",
    "The apparent \"loss\" of data (e.g., 40+ nodes ‚Üí 7 entities) is actually **smart optimization** that preserves the most relevant information while maintaining system stability."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
