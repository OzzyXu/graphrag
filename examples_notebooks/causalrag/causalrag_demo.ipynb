{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Causal Search Demo - Query and Context Analysis\n",
    "\n",
    "This notebook demonstrates how to use the Causal Search method in GraphRAG and inspect the context used to generate responses. Causal Search performs causal analysis on knowledge graphs through a two-stage process:\n",
    "\n",
    "1. **Stage 1**: Extract extended graph information (k + s nodes) and generate causal analysis report\n",
    "2. **Stage 2**: Use the causal report to generate final response to user query\n",
    "\n",
    "## Key Features\n",
    "\n",
    "- Extended node extraction beyond local search limits\n",
    "- Two-stage processing for comprehensive causal analysis\n",
    "- Automatic output saving to data folders\n",
    "- Configurable parameters for retrieval breadth and context proportions\n",
    "- Integration with existing GraphRAG pipeline\n",
    "- **Context inspection**: See exactly what data was used to generate responses\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "Before running this notebook, ensure you have:\n",
    "\n",
    "1. Run the GraphRAG indexing pipeline to generate entities, relationships, and community reports\n",
    "2. Set up your configuration in `settings.yaml` with causal search parameters\n",
    "3. Configured your language models and API keys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Copyright (c) 2024 Microsoft Corporation.\n",
    "# Licensed under the MIT License.\n",
    "\n",
    "import os\n",
    "import asyncio\n",
    "import json\n",
    "import pandas as pd\n",
    "import tiktoken\n",
    "from pathlib import Path\n",
    "from typing import Any, Dict, List\n",
    "\n",
    "# GraphRAG imports\n",
    "from graphrag.config.enums import ModelType\n",
    "from graphrag.config.load_config import load_config\n",
    "from graphrag.config.models.language_model_config import LanguageModelConfig\n",
    "from graphrag.language_model.manager import ModelManager\n",
    "from graphrag.query.context_builder.entity_extraction import EntityVectorStoreKey\n",
    "from graphrag.query.factory import get_causal_search_engine\n",
    "from graphrag.query.indexer_adapters import (\n",
    "    read_indexer_covariates,\n",
    "    read_indexer_entities,\n",
    "    read_indexer_relationships,\n",
    "    read_indexer_reports,\n",
    "    read_indexer_text_units,\n",
    ")\n",
    "from graphrag.query.structured_search.causal_search.search import CausalSearchError\n",
    "from graphrag.query.structured_search.local_search.mixed_context import (\n",
    "    LocalSearchMixedContext,\n",
    ")\n",
    "from graphrag.vector_stores.lancedb import LanceDBVectorStore\n",
    "\n",
    "# IPython display utilities\n",
    "from IPython.display import Markdown, display"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration Setup\n",
    "\n",
    "First, let's load the GraphRAG configuration and set up the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration setup\n",
    "ROOT_DIR = Path(\"./ragtest\")  # Adjust this path to your project root\n",
    "CONFIG_FILE = None  # Use default settings.yaml\n",
    "\n",
    "# Load configuration\n",
    "try:\n",
    "    config = load_config(ROOT_DIR, CONFIG_FILE)\n",
    "    print(\"‚úÖ Configuration loaded successfully\")\n",
    "    print(f\"üìÅ Root directory: {ROOT_DIR}\")\n",
    "    print(f\"üîß Causal search s_parameter: {config.causal_search.s_parameter}\")\n",
    "    print(f\"üîß Causal search top_k_entities: {config.causal_search.top_k_mapped_entities}\")\n",
    "    print(f\"üîß Causal search max_context_tokens: {config.causal_search.max_context_tokens}\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Failed to load configuration: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading\n",
    "\n",
    "Load the required data from your GraphRAG pipeline outputs using the same functions as the visualization notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data loading setup\n",
    "INPUT_DIR = f\"{ROOT_DIR}/output\"\n",
    "LANCEDB_URI = f\"{INPUT_DIR}/lancedb\"\n",
    "\n",
    "COMMUNITY_REPORT_TABLE = \"community_reports\"\n",
    "COMMUNITY_TABLE = \"communities\"\n",
    "ENTITY_TABLE = \"entities\"\n",
    "RELATIONSHIP_TABLE = \"relationships\"\n",
    "COVARIATE_TABLE = \"covariates\"\n",
    "TEXT_UNIT_TABLE = \"text_units\"\n",
    "COMMUNITY_LEVEL = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load tables to dataframes\n",
    "\n",
    "#### Read entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read nodes table to get community and degree data\n",
    "entity_df = pd.read_parquet(f\"{INPUT_DIR}/{ENTITY_TABLE}.parquet\")\n",
    "community_df = pd.read_parquet(f\"{INPUT_DIR}/{COMMUNITY_TABLE}.parquet\")\n",
    "\n",
    "print(f\"‚úÖ Loaded {len(entity_df)} entities\")\n",
    "print(f\"‚úÖ Loaded {len(community_df)} communities\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read relationships"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "relationship_df = pd.read_parquet(f\"{INPUT_DIR}/{RELATIONSHIP_TABLE}.parquet\")\n",
    "relationships = read_indexer_relationships(relationship_df)\n",
    "\n",
    "print(f\"‚úÖ Loaded {len(relationship_df)} relationships\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read other data tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load text units\n",
    "text_unit_df = pd.read_parquet(f\"{INPUT_DIR}/{TEXT_UNIT_TABLE}.parquet\")\n",
    "text_units = read_indexer_text_units(text_unit_df)\n",
    "\n",
    "# Load community reports\n",
    "report_df = pd.read_parquet(f\"{INPUT_DIR}/{COMMUNITY_REPORT_TABLE}.parquet\")\n",
    "reports = read_indexer_reports(report_df, community_df, COMMUNITY_LEVEL)\n",
    "\n",
    "# Load covariates if they exist\n",
    "try:\n",
    "    covariate_df = pd.read_parquet(f\"{INPUT_DIR}/{COVARIATE_TABLE}.parquet\")\n",
    "    claims = read_indexer_covariates(covariate_df)\n",
    "    covariates = {\"claims\": claims}\n",
    "    print(f\"‚úÖ Loaded {len(claims)} covariates\")\n",
    "except FileNotFoundError:\n",
    "    print(\"‚ÑπÔ∏è  No covariates found, proceeding without covariates\")\n",
    "    covariates = {}\n",
    "\n",
    "print(f\"‚úÖ Loaded {len(text_units)} text units\")\n",
    "print(f\"‚úÖ Loaded {len(reports)} community reports\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Setup\n",
    "\n",
    "Set up the language models and context builder using the same approach as the visualization notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get model configurations from the loaded config\n",
    "chat_model_config = config.get_language_model_config(\"default_chat_model\")\n",
    "embedding_model_config = config.get_language_model_config(\"default_embedding_model\")\n",
    "\n",
    "# Create chat model\n",
    "chat_model = ModelManager().get_or_create_chat_model(\n",
    "    name=\"causal_search\",\n",
    "    model_type=chat_model_config.type,\n",
    "    config=chat_model_config,\n",
    ")\n",
    "\n",
    "# Create token encoder\n",
    "token_encoder = tiktoken.encoding_for_model(chat_model_config.model)\n",
    "\n",
    "# Create embedding model\n",
    "text_embedder = ModelManager().get_or_create_embedding_model(\n",
    "    name=\"causal_search_embedding\",\n",
    "    model_type=embedding_model_config.type,\n",
    "    config=embedding_model_config,\n",
    ")\n",
    "\n",
    "# Create vector store\n",
    "description_embedding_store = LanceDBVectorStore(\n",
    "    collection_name=\"default-entity-description\",\n",
    ")\n",
    "description_embedding_store.connect(db_uri=LANCEDB_URI)\n",
    "\n",
    "print(\"‚úÖ Models and vector store setup complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Context Builder Setup\n",
    "\n",
    "Create the context builder using the same parameters as the visualization notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Context builder parameters (same as visualization notebook)\n",
    "context_builder_params = {\n",
    "    \"text_unit_prop\": 0.5,\n",
    "    \"community_prop\": 0.1,\n",
    "    \"conversation_history_max_turns\": 5,\n",
    "    \"conversation_history_user_turns_only\": True,\n",
    "    \"top_k_mapped_entities\": 10,  # Increased for causal search\n",
    "    \"top_k_relationships\": 10,     # Increased for causal search\n",
    "    \"include_entity_rank\": True,\n",
    "    \"include_relationship_weight\": True,\n",
    "    \"include_community_rank\": False,\n",
    "    \"return_candidate_context\": False,\n",
    "    \"embedding_vectorstore_key\": EntityVectorStoreKey.ID,\n",
    "    \"max_tokens\": 80_000,\n",
    "}\n",
    "\n",
    "# Create context builder\n",
    "context_builder = LocalSearchMixedContext(\n",
    "    community_reports=reports,\n",
    "    text_units=text_units,\n",
    "    entities=entities,\n",
    "    relationships=relationships,\n",
    "    covariates=covariates,\n",
    "    entity_text_embeddings=description_embedding_store,\n",
    "    embedding_vectorstore_key=EntityVectorStoreKey.ID,\n",
    "    text_embedder=text_embedder,\n",
    "    token_encoder=token_encoder,\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Context builder setup complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Causal Search Engine Setup\n",
    "\n",
    "Create the causal search engine with the same model parameters as the visualization notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model parameters (same as visualization notebook)\n",
    "model_params = {\n",
    "    \"max_tokens\": 4_000,  # Adjusted for gpt-4-turbo-preview\n",
    "    \"temperature\": 0.0,\n",
    "}\n",
    "\n",
    "# Create causal search engine\n",
    "causal_search_engine = get_causal_search_engine(\n",
    "    model=chat_model,\n",
    "    context_builder=context_builder,\n",
    "    token_encoder=token_encoder,\n",
    "    model_params=model_params,\n",
    "    context_builder_params=context_builder_params,\n",
    "    s_parameter=3,  # Additional nodes for causal analysis\n",
    "    max_context_tokens=12_000,\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Causal search engine setup complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Causal Search Example\n",
    "\n",
    "Now let's run a causal search query and inspect the context used to generate the response."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run causal search on sample queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample query for causal analysis\n",
    "question = \"What are the causal relationships in this dataset?\"\n",
    "print(f\"üîç Query: {question}\")\n",
    "\n",
    "# Execute causal search\n",
    "try:\n",
    "    result = await causal_search_engine.search(question)\n",
    "    print(\"‚úÖ Causal search completed successfully!\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Causal search failed: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Display the response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display as formatted Markdown\n",
    "print(\"\\nüìù Causal Search Response:\")\n",
    "print(\"=\" * 50)\n",
    "display(Markdown(result.response))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspecting the Context Data\n",
    "\n",
    "Now let's examine exactly what data was used to generate the response. This is the key part that shows the context filtering in action."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Context data overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nüîç Context Data Overview:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "if hasattr(result, 'context_data') and result.context_data:\n",
    "    print(f\"üìä Entities: {len(result.context_data.get('entities', []))}\")\n",
    "    print(f\"üîó Relationships: {len(result.context_data.get('relationships', []))}\")\n",
    "    print(f\"üìÑ Text Units: {len(result.context_data.get('text_units', []))}\")\n",
    "    print(f\"üèòÔ∏è  Community Reports: {len(result.context_data.get('community_reports', []))}\")\n",
    "else:\n",
    "    print(\"‚ÑπÔ∏è  No context_data available in result\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspect entities used in context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nüè∑Ô∏è  Entities Used in Context:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "if hasattr(result, 'context_data') and result.context_data and 'entities' in result.context_data:\n",
    "    entities_df = result.context_data['entities']\n",
    "    if not entities_df.empty:\n",
    "        # Show key fields\n",
    "        display_cols = ['entity', 'description', 'rank', 'type']\n",
    "        available_cols = [col for col in display_cols if col in entities_df.columns]\n",
    "        \n",
    "        if available_cols:\n",
    "            display(entities_df[available_cols].head(10))\n",
    "        else:\n",
    "            display(entities_df.head(10))\n",
    "        \n",
    "        print(f\"\\nüìà Total entities in context: {len(entities_df)}\")\n",
    "    else:\n",
    "        print(\"‚ÑπÔ∏è  No entities found in context\")\n",
    "else:\n",
    "    print(\"‚ÑπÔ∏è  No entities data available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspect relationships used in context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nüîó Relationships Used in Context:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "if hasattr(result, 'context_data') and result.context_data and 'relationships' in result.context_data:\n",
    "    relationships_df = result.context_data['relationships']\n",
    "    if not relationships_df.empty:\n",
    "        # Show key fields\n",
    "        display_cols = ['source', 'target', 'description', 'weight', 'rank']\n",
    "        available_cols = [col for col in display_cols if col in relationships_df.columns]\n",
    "        \n",
    "        if available_cols:\n",
    "            display(relationships_df[available_cols].head(10))\n",
    "        else:\n",
    "            display(relationships_df[available_cols].head(10))\n",
    "        \n",
    "        print(f\"\\nüìà Total relationships in context: {len(relationships_df)}\")\n",
    "    else:\n",
    "        print(\"‚ÑπÔ∏è  No relationships found in context\")\n",
    "else:\n",
    "    print(\"‚ÑπÔ∏è  No relationships data available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspect text units used in context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nüìÑ Text Units Used in Context:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "if hasattr(result, 'context_data') and result.context_data and 'text_units' in result.context_data:\n",
    "    text_units_df = result.context_data['text_units']\n",
    "    if not text_units_df.empty:\n",
    "        # Show key fields\n",
    "        display_cols = ['text', 'n_tokens']\n",
    "        available_cols = [col for col in display_cols if col in relationships_df.columns]\n",
    "        \n",
    "        if available_cols:\n",
    "            # Truncate text for display\n",
    "            display_df = text_units_df[available_cols].copy()\n",
    "            if 'text' in display_df.columns:\n",
    "                display_df['text'] = display_df['text'].str[:200] + '...'\n",
    "            display(display_df.head(10))\n",
    "        else:\n",
    "            display(text_units_df.head(10))\n",
    "        \n",
    "        print(f\"\\nüìà Total text units in context: {len(text_units_df)}\")\n",
    "    else:\n",
    "        print(\"‚ÑπÔ∏è  No text units found in context\")\n",
    "else:\n",
    "    print(\"‚ÑπÔ∏è  No text units data available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspect community reports used in context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nüèòÔ∏è  Community Reports Used in Context:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "if hasattr(result, 'context_data') and result.context_data and 'community_reports' in result.context_data:\n",
    "    community_reports_df = result.context_data['community_reports']\n",
    "    if not community_reports_df.empty:\n",
    "        # Show key fields\n",
    "        display_cols = ['community_id', 'summary', 'description']\n",
    "        available_cols = [col for col in display_cols if col in community_reports_df.columns]\n",
    "        \n",
    "        if available_cols:\n",
    "            # Truncate summary for display\n",
    "            display_df = community_reports_df[available_cols].copy()\n",
    "            if 'summary' in display_df.columns:\n",
    "                display_df['summary'] = display_df['summary'].str[:200] + '...'\n",
    "            display(display_df[available_cols].head(10))\n",
    "        else:\n",
    "            display(community_reports_df.head(10))\n",
    "        \n",
    "        print(f\"\\nüìà Total community reports in context: {len(community_reports_df)}\")\n",
    "    else:\n",
    "        print(\"‚ÑπÔ∏è  No community reports found in context\")\n",
    "else:\n",
    "    print(\"‚ÑπÔ∏è  No community reports data available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Context Filtering Analysis\n",
    "\n",
    "Let's analyze how the context filtering worked and compare it to the original data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare original vs. filtered data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nüîç Context Filtering Analysis:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "print(f\"üìä Original data:\")\n",
    "print(f\"   - Entities: {len(entity_df)}\")\n",
    "print(f\"   - Relationships: {len(relationship_df)}\")\n",
    "print(f\"   - Text Units: {len(text_unit_df)}\")\n",
    "print(f\"   - Community Reports: {len(report_df)}\")\n",
    "\n",
    "if hasattr(result, 'context_data') and result.context_data:\n",
    "    print(f\"\\nüéØ Filtered context data:\")\n",
    "    print(f\"   - Entities: {len(result.context_data.get('entities', []))}\")\n",
    "    print(f\"   - Relationships: {len(result.context_data.get('relationships', []))}\")\n",
    "    print(f\"   - Text Units: {len(result.context_data.get('text_units', []))}\")\n",
    "    print(f\"   - Community Reports: {len(result.context_data.get('community_reports', []))}\")\n",
    "    \n",
    "    # Calculate filtering ratios\n",
    "    entity_ratio = len(result.context_data.get('entities', [])) / len(entity_df) * 100\n",
    "    relationship_ratio = len(result.context_data.get('relationships', [])) / len(relationship_df) * 100\n",
    "    text_unit_ratio = len(result.context_data.get('text_units', [])) / len(text_unit_df) * 100\n",
    "    \n",
    "    print(f\"\\nüìà Filtering ratios:\")\n",
    "    print(f\"   - Entities: {entity_ratio:.1f}% kept\")\n",
    "    print(f\"   - Relationships: {relationship_ratio:.1f}% kept\")\n",
    "    print(f\"   - Text Units: {text_unit_ratio:.1f}% kept\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Token usage analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nüî¢ Token Usage Analysis:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "if hasattr(result, 'context_data') and result.context_data:\n",
    "    # Estimate token usage for each data type\n",
    "    total_tokens = 0\n",
    "    \n",
    "    # Entities tokens\n",
    "    if 'entities' in result.context_data and not result.context_data['entities'].empty:\n",
    "        entities_text = result.context_data['entities'].to_string()\n",
    "        entities_tokens = len(token_encoder.encode(entities_text))\n",
    "        total_tokens += entities_tokens\n",
    "        print(f\"   - Entities: ~{entities_tokens:,} tokens\")\n",
    "    \n",
    "    # Relationships tokens\n",
    "    if 'relationships' in result.context_data and not result.context_data['relationships'].empty:\n",
    "        relationships_text = result.context_data['relationships'].to_string()\n",
    "        relationships_tokens = len(token_encoder.encode(relationships_text))\n",
    "        total_tokens += relationships_tokens\n",
    "        print(f\"   - Relationships: ~{relationships_tokens:,} tokens\")\n",
    "    \n",
    "    # Text units tokens\n",
    "    if 'text_units' in result.context_data and not result.context_data['text_units'].empty:\n",
    "        text_units_text = result.context_data['text_units'].to_string()\n",
    "        text_units_tokens = len(token_encoder.encode(text_units_text))\n",
    "        total_tokens += text_units_tokens\n",
    "        print(f\"   - Text Units: ~{text_units_tokens:,} tokens\")\n",
    "    \n",
    "    print(f\"\\nüìä Total estimated tokens: ~{total_tokens:,}\")\n",
    "    print(f\"üéØ Target limit: 8,000 tokens (network data portion)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced Context Inspection\n",
    "\n",
    "Let's look deeper into the context building process and see how the filtering decisions were made."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check context builder parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n‚öôÔ∏è  Context Builder Parameters Used:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "for key, value in context_builder_params.items():\n",
    "    print(f\"   - {key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check model parameters"
   ]
  },
  {
   "cell_type": "code",
    "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nü§ñ Model Parameters Used:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "for key, value in model_params.items():\n",
    "    print(f\"   - {key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check causal search parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nüîç Causal Search Parameters:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "print(f\"   - s_parameter: {causal_search_engine.s_parameter}\")\n",
    "print(f\"   - max_context_tokens: {causal_search_engine.max_context_tokens}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook demonstrates:\n",
    "\n",
    "1. **Data Loading**: Using the same functions as the visualization notebook\n",
    "2. **Model Setup**: Consistent with the visualization notebook approach\n",
    "3. **Context Building**: Same parameters and structure\n",
    "4. **Causal Search**: Extended node extraction and two-stage processing\n",
    "5. **Context Inspection**: Detailed analysis of what data was used\n",
    "6. **Filtering Analysis**: Understanding how context filtering works\n",
    "\n",
    "The key insight is that causal search uses **intelligent filtering** to ensure:\n",
    "- **LLM Compatibility**: Data fits within model context limits\n",
    "- **Relevance**: Most important entities/relationships are preserved\n",
    "- **Performance**: Efficient processing without context length errors\n",
    "\n",
    "The apparent \"loss\" of data (e.g., 40+ nodes ‚Üí 7 entities) is actually **smart optimization** that preserves the most relevant information while maintaining system stability."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".ipynb",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
