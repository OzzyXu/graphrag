{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualizing the knowledge graph with `yfiles-jupyter-graphs`\n",
    "\n",
    "This notebook is a partial copy of [local_search.ipynb](../../local_search.ipynb) that shows how to use `yfiles-jupyter-graphs` to add interactive graph visualizations of the parquet files  and how to visualize the result context of `graphrag` queries (see at the end of this notebook)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright (c) 2024 Microsoft Corporation.\n",
    "# Licensed under the MIT License."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "import tiktoken\n",
    "\n",
    "from graphrag.config.enums import ModelType\n",
    "from graphrag.config.models.language_model_config import LanguageModelConfig\n",
    "from graphrag.language_model.manager import ModelManager\n",
    "from graphrag.query.context_builder.entity_extraction import EntityVectorStoreKey\n",
    "from graphrag.query.indexer_adapters import (\n",
    "    read_indexer_covariates,\n",
    "    read_indexer_entities,\n",
    "    read_indexer_relationships,\n",
    "    read_indexer_reports,\n",
    "    read_indexer_text_units,\n",
    ")\n",
    "from graphrag.query.structured_search.local_search.mixed_context import (\n",
    "    LocalSearchMixedContext,\n",
    ")\n",
    "from graphrag.query.structured_search.local_search.search import LocalSearch\n",
    "from graphrag.vector_stores.lancedb import LanceDBVectorStore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Debug logging enabled for GraphRAG\n",
      "🔧 Current log level: INFO\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'VERBOSE_MODE' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 17\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m✅ Debug logging enabled for GraphRAG\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m🔧 Current log level: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlogging\u001b[38;5;241m.\u001b[39mgetLevelName(LOG_LEVEL)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 17\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m🔧 Verbose mode: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[43mVERBOSE_MODE\u001b[49m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'VERBOSE_MODE' is not defined"
     ]
    }
   ],
   "source": [
    "# Configure debug level logging for GraphRAG\n",
    "import logging\n",
    "from graphrag.logger.standard_logging import init_loggers\n",
    "\n",
    "# ===== LOGGING CONFIGURATION =====\n",
    "# Change this variable to control logging level throughout the notebook\n",
    "# Options: logging.DEBUG, logging.INFO, logging.WARNING, logging.ERROR\n",
    "LOG_LEVEL = logging.INFO  # Set to logging.INFO for less verbose output\n",
    "\n",
    "# Set specific logger levels using the configured LOG_LEVEL\n",
    "logging.getLogger(\"graphrag\").setLevel(LOG_LEVEL)\n",
    "logging.getLogger(\"graphrag.query\").setLevel(LOG_LEVEL)\n",
    "logging.getLogger(\"graphrag.query.structured_search\").setLevel(LOG_LEVEL)\n",
    "\n",
    "print(\"✅ Debug logging enabled for GraphRAG\")\n",
    "print(f\"🔧 Current log level: {logging.getLevelName(LOG_LEVEL)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Local Search Example\n",
    "\n",
    "Local search method generates answers by combining relevant data from the AI-extracted knowledge-graph with text chunks of the raw documents. This method is suitable for questions that require an understanding of specific entities mentioned in the documents (e.g. What are the healing properties of chamomile?)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load text units and graph data tables as context for local search\n",
    "\n",
    "- In this test we first load indexing outputs from parquet files to dataframes, then convert these dataframes into collections of data objects aligning with the knowledge model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load tables to dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_DIR = \"/home/chuaxu/projects/graphrag/ragsas/output\"\n",
    "LANCEDB_URI = f\"{INPUT_DIR}/lancedb\"\n",
    "\n",
    "COMMUNITY_REPORT_TABLE = \"community_reports\"\n",
    "COMMUNITY_TABLE = \"communities\"\n",
    "ENTITY_TABLE = \"entities\"\n",
    "RELATIONSHIP_TABLE = \"relationships\"\n",
    "COVARIATE_TABLE = \"covariates\"\n",
    "TEXT_UNIT_TABLE = \"text_units\"\n",
    "COMMUNITY_LEVEL = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read nodes table to get community and degree data\n",
    "entity_df = pd.read_parquet(f\"{INPUT_DIR}/{ENTITY_TABLE}.parquet\")\n",
    "community_df = pd.read_parquet(f\"{INPUT_DIR}/{COMMUNITY_TABLE}.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read relationships"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "relationship_df = pd.read_parquet(f\"{INPUT_DIR}/{RELATIONSHIP_TABLE}.parquet\")\n",
    "relationships = read_indexer_relationships(relationship_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualizing nodes and relationships with `yfiles-jupyter-graphs`\n",
    "\n",
    "`yfiles-jupyter-graphs` is a graph visualization extension that provides interactive and customizable visualizations for structured node and relationship data.\n",
    "\n",
    "In this case, we use it to provide an interactive visualization for the knowledge graph of the [local_search.ipynb](../../local_search.ipynb) sample by passing node and relationship lists converted from the given parquet files. The requirements for the input data is an `id` attribute for the nodes and `start`/`end` properties for the relationships that correspond to the node ids. Additional attributes can be added in the `properties` of each node/relationship dict:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install yfiles_jupyter_graphs --quiet\n",
    "from yfiles_jupyter_graphs import GraphWidget\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# converts the entities dataframe to a list of dicts for yfiles-jupyter-graphs\n",
    "def convert_entities_to_dicts(df):\n",
    "    \"\"\"Convert the entities dataframe to a list of dicts for yfiles-jupyter-graphs.\"\"\"\n",
    "    def clean_value(value):\n",
    "        \"\"\"Clean a value to make it JSON serializable.\"\"\"\n",
    "        # Handle arrays first (before checking for NaN)\n",
    "        if isinstance(value, (np.ndarray, list)):\n",
    "            # Convert arrays to strings or take first element if single value\n",
    "            if len(value) == 0:\n",
    "                return None\n",
    "            elif len(value) == 1:\n",
    "                return str(value[0])\n",
    "            else:\n",
    "                return str(value)\n",
    "        # Now check for NaN on scalar values\n",
    "        elif pd.isna(value):\n",
    "            return None\n",
    "        elif isinstance(value, (np.integer, np.floating)):\n",
    "            # Convert numpy numbers to Python numbers\n",
    "            if np.isnan(value) or np.isinf(value):\n",
    "                return None\n",
    "            return value.item()\n",
    "        elif isinstance(value, float):\n",
    "            # Handle Python floats that might be NaN or inf\n",
    "            if pd.isna(value) or np.isinf(value):\n",
    "                return None\n",
    "            return value\n",
    "        else:\n",
    "            return value\n",
    "    \n",
    "    nodes_dict = {}\n",
    "    for _, row in df.iterrows():\n",
    "        # Create a dictionary for each row and collect unique nodes\n",
    "        node_id = row[\"title\"]\n",
    "        if node_id not in nodes_dict:\n",
    "            # Clean all properties to make them JSON serializable\n",
    "            cleaned_properties = {k: clean_value(v) for k, v in row.to_dict().items()}\n",
    "            nodes_dict[node_id] = {\n",
    "                \"id\": node_id,\n",
    "                \"properties\": cleaned_properties,\n",
    "            }\n",
    "    return list(nodes_dict.values())\n",
    "\n",
    "\n",
    "# converts the relationships dataframe to a list of dicts for yfiles-jupyter-graphs\n",
    "def convert_relationships_to_dicts(df):\n",
    "    \"\"\"Convert the relationships dataframe to a list of dicts for yfiles-jupyter-graphs.\"\"\"\n",
    "    def clean_value(value):\n",
    "        \"\"\"Clean a value to make it JSON serializable.\"\"\"\n",
    "        # Handle arrays first (before checking for NaN)\n",
    "        if isinstance(value, (np.ndarray, list)):\n",
    "            # Convert arrays to strings or take first element if single value\n",
    "            if len(value) == 0:\n",
    "                return None\n",
    "            elif len(value) == 1:\n",
    "                return str(value[0])\n",
    "            else:\n",
    "                return str(value)\n",
    "        # Now check for NaN on scalar values\n",
    "        elif pd.isna(value):\n",
    "            return None\n",
    "        elif isinstance(value, (np.integer, np.floating)):\n",
    "            # Convert numpy numbers to Python numbers\n",
    "            if np.isnan(value) or np.isinf(value):\n",
    "                return None\n",
    "            return value.item()\n",
    "        elif isinstance(value, float):\n",
    "            # Handle Python floats that might be NaN or inf\n",
    "            if pd.isna(value) or np.isinf(value):\n",
    "                return None\n",
    "            return value\n",
    "        else:\n",
    "            return value\n",
    "    \n",
    "    relationships = []\n",
    "    for _, row in df.iterrows():\n",
    "        # Create a dictionary for each row\n",
    "        cleaned_properties = {k: clean_value(v) for k, v in row.to_dict().items()}\n",
    "        relationships.append({\n",
    "            \"start\": row[\"source\"],\n",
    "            \"end\": row[\"target\"],\n",
    "            \"properties\": cleaned_properties,\n",
    "        })\n",
    "    return relationships\n",
    "\n",
    "\n",
    "w = GraphWidget()\n",
    "w.directed = True\n",
    "w.nodes = convert_entities_to_dicts(entity_df)\n",
    "w.edges = convert_relationships_to_dicts(relationship_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure data-driven visualization\n",
    "\n",
    "The additional properties can be used to configure the visualization for different use cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show title on the node\n",
    "w.node_label_mapping = \"title\"\n",
    "\n",
    "\n",
    "# map community to a color\n",
    "def community_to_color(community):\n",
    "    \"\"\"Map a community to a color.\"\"\"\n",
    "    colors = [\n",
    "        \"crimson\",\n",
    "        \"darkorange\",\n",
    "        \"indigo\",\n",
    "        \"cornflowerblue\",\n",
    "        \"cyan\",\n",
    "        \"teal\",\n",
    "        \"green\",\n",
    "    ]\n",
    "    return (\n",
    "        colors[int(community) % len(colors)] if community is not None else \"lightgray\"\n",
    "    )\n",
    "\n",
    "\n",
    "def edge_to_source_community(edge):\n",
    "    \"\"\"Get the community of the source node of an edge.\"\"\"\n",
    "    source_node = next(\n",
    "        (entry for entry in w.nodes if entry[\"properties\"][\"title\"] == edge[\"start\"]),\n",
    "        None,\n",
    "    )\n",
    "    if source_node is None:\n",
    "        return None\n",
    "    # Handle missing community property gracefully\n",
    "    source_node_community = source_node[\"properties\"].get(\"community\", None)\n",
    "    return source_node_community if source_node_community is not None else None\n",
    "\n",
    "\n",
    "w.node_color_mapping = lambda node: community_to_color(node[\"properties\"].get(\"community\", None))\n",
    "w.edge_color_mapping = lambda edge: community_to_color(edge_to_source_community(edge))\n",
    "# map size data to a reasonable factor\n",
    "w.node_scale_factor_mapping = lambda node: 0.5 + node[\"properties\"].get(\"size\", 0) * 1.5 / 20\n",
    "# use weight for edge thickness\n",
    "w.edge_thickness_factor_mapping = \"weight\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Automatic layouts\n",
    "\n",
    "The widget provides different automatic layouts that serve different purposes: `Circular`, `Hierarchic`, `Organic (interactiv or static)`, `Orthogonal`, `Radial`, `Tree`, `Geo-spatial`.\n",
    "\n",
    "For the knowledge graph, this sample uses the `Circular` layout, though `Hierarchic` or `Organic` are also suitable choices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the circular layout for this visualization. For larger graphs, the default organic layout is often preferrable.\n",
    "w.circular_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Display the graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3197f98870f648dc96bccf81aafbd546",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "GraphWidget(layout=Layout(height='800px', width='100%'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualizing the result context of `graphrag` queries\n",
    "\n",
    "The result context of `graphrag` queries allow to inspect the context graph of the request. This data can similarly be visualized as graph with `yfiles-jupyter-graphs`.\n",
    "\n",
    "## Making the request\n",
    "\n",
    "The following cell recreates the sample queries from [local_search.ipynb](../../local_search.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Covariate file not found, proceeding without covariates\n"
     ]
    }
   ],
   "source": [
    "# setup (see also ../../local_search.ipynb)\n",
    "entities = read_indexer_entities(entity_df, community_df, COMMUNITY_LEVEL)\n",
    "\n",
    "description_embedding_store = LanceDBVectorStore(\n",
    "    collection_name=\"default-entity-description\",\n",
    ")\n",
    "description_embedding_store.connect(db_uri=LANCEDB_URI)\n",
    "\n",
    "# Comment out covariates for now if file doesn't exist\n",
    "try:\n",
    "    covariate_df = pd.read_parquet(f\"{INPUT_DIR}/{COVARIATE_TABLE}.parquet\")\n",
    "    claims = read_indexer_covariates(covariate_df)\n",
    "    covariates = {\"claims\": claims}\n",
    "except FileNotFoundError:\n",
    "    print(\"Covariate file not found, proceeding without covariates\")\n",
    "    covariates = {}\n",
    "\n",
    "report_df = pd.read_parquet(f\"{INPUT_DIR}/{COMMUNITY_REPORT_TABLE}.parquet\")\n",
    "reports = read_indexer_reports(report_df, community_df, COMMUNITY_LEVEL)\n",
    "text_unit_df = pd.read_parquet(f\"{INPUT_DIR}/{TEXT_UNIT_TABLE}.parquet\")\n",
    "text_units = read_indexer_text_units(text_unit_df)\n",
    "\n",
    "# Load configuration from settings.yaml\n",
    "from graphrag.config.load_config import load_config\n",
    "from pathlib import Path\n",
    "\n",
    "config_path = Path(\"/home/chuaxu/projects/graphrag/ragsas\")\n",
    "config = load_config(config_path)\n",
    "\n",
    "# Get model configurations from the loaded config\n",
    "chat_model_config = config.get_language_model_config(\"default_chat_model\")\n",
    "embedding_model_config = config.get_language_model_config(\"default_embedding_model\")\n",
    "\n",
    "chat_model = ModelManager().get_or_create_chat_model(\n",
    "    name=\"local_search\",\n",
    "    model_type=chat_model_config.type,\n",
    "    config=chat_model_config,\n",
    ")\n",
    "\n",
    "token_encoder = tiktoken.encoding_for_model(chat_model_config.model)\n",
    "\n",
    "text_embedder = ModelManager().get_or_create_embedding_model(\n",
    "    name=\"local_search_embedding\",\n",
    "    model_type=embedding_model_config.type,\n",
    "    config=embedding_model_config,\n",
    ")\n",
    "\n",
    "context_builder = LocalSearchMixedContext(\n",
    "    community_reports=reports,\n",
    "    text_units=text_units,\n",
    "    entities=entities,\n",
    "    relationships=relationships,\n",
    "    covariates=covariates,\n",
    "    entity_text_embeddings=description_embedding_store,\n",
    "    embedding_vectorstore_key=EntityVectorStoreKey.ID,  # if the vectorstore uses entity title as ids, set this to EntityVectorStoreKey.TITLE\n",
    "    text_embedder=text_embedder,\n",
    "    token_encoder=token_encoder,\n",
    ")\n",
    "\n",
    "local_context_params = {\n",
    "    \"text_unit_prop\": 0.5,\n",
    "    \"community_prop\": 0.25,\n",
    "    \"conversation_history_max_turns\": 5,\n",
    "    \"conversation_history_user_turns_only\": True,\n",
    "    \"top_k_mapped_entities\": 10,\n",
    "    \"top_k_relationships\": 10,\n",
    "    \"include_entity_rank\": True,\n",
    "    \"include_relationship_weight\": True,\n",
    "    \"include_community_rank\": False,\n",
    "    \"return_candidate_context\": False,\n",
    "    \"embedding_vectorstore_key\": EntityVectorStoreKey.ID,  # set this to EntityVectorStoreKey.TITLE if the vectorstore uses entity title as ids\n",
    "    \"max_tokens\": 80_000,  # change this based on the token limit you have on your model (if you are using a model with 8k limit, a good setting could be 5000)\n",
    "}\n",
    "\n",
    "model_params = {\n",
    "    \"max_tokens\": 16_384,  # change this based on the token limit you have on your model (if you are using a model with 8k limit, a good setting could be 1000=1500, the model supports at most 16384 completion tokens)\n",
    "    \"temperature\": 0.0,\n",
    "}\n",
    "\n",
    "search_engine = LocalSearch(\n",
    "    model=chat_model,\n",
    "    context_builder=context_builder,\n",
    "    token_encoder=token_encoder,\n",
    "    model_params=model_params,\n",
    "    context_builder_params=local_context_params,\n",
    "    response_type=\"multiple paragraphs\",  # free form text describing the response type and format, can be anything, e.g. prioritized list, single paragraph, multiple paragraphs, multiple-page report\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run local search on sample queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10:59:53 - graphrag.query.structured_search.local_search.search - DEBUG - GENERATE ANSWER: 1755529192.9559348. QUERY: Tell me about Agent Mercer\n",
      "I'm sorry, but I don't have any information about Agent Mercer in the provided data tables. If you have any other questions or need information on a different topic, feel free to ask!\n",
      "I'm sorry, but I don't have any information about Agent Mercer in the provided data tables. If you have any other questions or need information on a different topic, feel free to ask!\n"
     ]
    }
   ],
   "source": [
    "result = await search_engine.search(\"Tell me about Agent Mercer\")\n",
    "print(result.response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10:59:53 - graphrag.query.structured_search.local_search.mixed_context - WARNING - Reached token limit - reverting to previous context state\n",
      "10:59:53 - graphrag.query.structured_search.local_search.search - DEBUG - GENERATE ANSWER: 1755529193.7935176. QUERY: How can we use SAS Econometric products to help analyze the impact of different pricing strategies on business revenue?\n",
      "10:59:53 - graphrag.query.structured_search.local_search.search - DEBUG - GENERATE ANSWER: 1755529193.7935176. QUERY: How can we use SAS Econometric products to help analyze the impact of different pricing strategies on business revenue?\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "# Analyzing Pricing Strategies with SAS Econometric Products\n",
       "\n",
       "SAS Econometric products offer a comprehensive suite of tools and procedures that can be instrumental in analyzing the impact of different pricing strategies on business revenue. These products are designed to facilitate advanced econometric analysis, providing users with the ability to model complex economic relationships and forecast outcomes based on various pricing scenarios.\n",
       "\n",
       "## Key Procedures for Pricing Strategy Analysis\n",
       "\n",
       "One of the standout features of SAS Econometrics is its inclusion of procedures such as PROC DEEPPRICE, which is specifically designed for deep learning and policy evaluation in the context of price and demand data analysis. PROC DEEPPRICE is adept at estimating demand curves and optimal revenue per user by accounting for heterogeneous price effects based on user characteristics. This capability allows businesses to tailor their pricing strategies to maximize revenue by understanding how different prices affect demand across various customer segments [Data: Entities (1762)].\n",
       "\n",
       "Additionally, SAS Econometrics includes procedures like CQLIM, which are used for limited dependent variable models. These models are essential when dealing with outcomes that are categorical or otherwise constrained, such as purchase decisions influenced by pricing. By employing these procedures, businesses can gain insights into how pricing changes might affect consumer behavior and, consequently, revenue [Data: Entities (290); Sources (30)].\n",
       "\n",
       "## Integration with SAS Cloud Analytic Services\n",
       "\n",
       "SAS Econometrics operates in a cloud environment, which provides users with the flexibility and scalability needed for large-scale data analysis. This cloud-based approach ensures that users can access the software's powerful features from anywhere, facilitating collaboration and efficiency in data-driven decision-making. The integration with SAS Cloud Analytic Services (CAS) enhances the capabilities of SAS Econometrics by enabling efficient data processing and analytics across distributed computing environments. This integration is particularly beneficial for handling large datasets and complex analytical tasks, which are common in pricing strategy analysis [Data: Reports (33); Entities (2)].\n",
       "\n",
       "## Advanced Analytical Techniques\n",
       "\n",
       "SAS Econometrics supports Bayesian analysis through procedures like CQLIM and CNTSELECT, employing advanced algorithms such as the random walk Metropolis (RWM) algorithm. These techniques allow for sophisticated modeling and simulation steps, providing businesses with predictive insights into how different pricing strategies might impact revenue. The ability to perform Bayesian inference with precision and efficiency makes SAS Econometrics a valuable tool for businesses looking to optimize their pricing strategies based on robust statistical analysis [Data: Entities (2); Sources (10)].\n",
       "\n",
       "## Conclusion\n",
       "\n",
       "In summary, SAS Econometric products offer a robust and versatile toolkit for analyzing the impact of different pricing strategies on business revenue. By leveraging procedures like PROC DEEPPRICE and CQLIM, businesses can gain valuable insights into demand dynamics and optimize their pricing strategies to maximize revenue. The integration with SAS Cloud Analytic Services further enhances these capabilities, providing a scalable and efficient platform for conducting complex econometric analyses. As a result, SAS Econometrics stands out as an invaluable resource for businesses seeking to make data-driven decisions in their pricing strategies."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import Markdown, display\n",
    "\n",
    "question = \"How can we use SAS Econometric products to help analyze the impact of different pricing strategies on business revenue?\"\n",
    "# question = \"Which published studies in our knowledge base used both panel data methods and cointegration analysis on emerging market economies?\"\n",
    "result = await search_engine.search(question)\n",
    "\n",
    "# Display as formatted Markdown instead of plain text\n",
    "display(Markdown(result.response))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspecting the context data used to generate the response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>entity</th>\n",
       "      <th>description</th>\n",
       "      <th>number of relationships</th>\n",
       "      <th>in_context</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>290</td>\n",
       "      <td>SAS ECONOMETRICS PROCEDURES</td>\n",
       "      <td>SAS Econometrics Procedures are a comprehensiv...</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>SAS ECONOMETRICS</td>\n",
       "      <td>SAS Econometrics is a comprehensive software s...</td>\n",
       "      <td>31</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1102</td>\n",
       "      <td>SAS/ETS SOFTWARE</td>\n",
       "      <td></td>\n",
       "      <td>2</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>SAS/ETS</td>\n",
       "      <td>SAS/ETS is a comprehensive software suite with...</td>\n",
       "      <td>14</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1762</td>\n",
       "      <td>PROC DEEPPRICE</td>\n",
       "      <td>PROC DEEPPRICE is a versatile procedure design...</td>\n",
       "      <td>24</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     id                       entity  \\\n",
       "0   290  SAS ECONOMETRICS PROCEDURES   \n",
       "1     2             SAS ECONOMETRICS   \n",
       "2  1102             SAS/ETS SOFTWARE   \n",
       "3     4                      SAS/ETS   \n",
       "4  1762               PROC DEEPPRICE   \n",
       "\n",
       "                                         description number of relationships  \\\n",
       "0  SAS Econometrics Procedures are a comprehensiv...                       1   \n",
       "1  SAS Econometrics is a comprehensive software s...                      31   \n",
       "2                                                                          2   \n",
       "3  SAS/ETS is a comprehensive software suite with...                      14   \n",
       "4  PROC DEEPPRICE is a versatile procedure design...                      24   \n",
       "\n",
       "   in_context  \n",
       "0        True  \n",
       "1        True  \n",
       "2        True  \n",
       "3        True  \n",
       "4        True  "
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.context_data[\"entities\"].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>source</th>\n",
       "      <th>target</th>\n",
       "      <th>description</th>\n",
       "      <th>weight</th>\n",
       "      <th>in_context</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>247</td>\n",
       "      <td>SAS</td>\n",
       "      <td>SAS ECONOMETRICS PROCEDURES</td>\n",
       "      <td>SAS Econometrics Procedures are part of the SA...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    id source                       target  \\\n",
       "0  247    SAS  SAS ECONOMETRICS PROCEDURES   \n",
       "\n",
       "                                         description weight  in_context  \n",
       "0  SAS Econometrics Procedures are part of the SA...    1.0        True  "
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.context_data[\"relationships\"].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing the result context as graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e5bc54cb69bd4209ad9d49808733d2b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "GraphWidget(layout=Layout(height='500px', width='100%'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Helper function to visualize the result context with `yfiles-jupyter-graphs`.\n",
    "\n",
    "The dataframes are converted into supported nodes and relationships lists and then passed to yfiles-jupyter-graphs.\n",
    "Additionally, some values are mapped to visualization properties.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def show_graph(result):\n",
    "    \"\"\"Visualize the result context with yfiles-jupyter-graphs.\"\"\"\n",
    "    from yfiles_jupyter_graphs import GraphWidget\n",
    "\n",
    "    if (\n",
    "        \"entities\" not in result.context_data\n",
    "        or \"relationships\" not in result.context_data\n",
    "    ):\n",
    "        msg = \"The passed results do not contain 'entities' or 'relationships'\"\n",
    "        raise ValueError(msg)\n",
    "\n",
    "    # converts the entities dataframe to a list of dicts for yfiles-jupyter-graphs\n",
    "    def convert_entities_to_dicts(df):\n",
    "        \"\"\"Convert the entities dataframe to a list of dicts for yfiles-jupyter-graphs.\"\"\"\n",
    "        nodes_dict = {}\n",
    "        for _, row in df.iterrows():\n",
    "            # Create a dictionary for each row and collect unique nodes\n",
    "            node_id = row[\"entity\"]\n",
    "            if node_id not in nodes_dict:\n",
    "                nodes_dict[node_id] = {\n",
    "                    \"id\": node_id,\n",
    "                    \"properties\": row.to_dict(),\n",
    "                }\n",
    "        return list(nodes_dict.values())\n",
    "\n",
    "    # converts the relationships dataframe to a list of dicts for yfiles-jupyter-graphs\n",
    "    def convert_relationships_to_dicts(df):\n",
    "        \"\"\"Convert the relationships dataframe to a list of dicts for yfiles-jupyter-graphs.\"\"\"\n",
    "        relationships = []\n",
    "        for _, row in df.iterrows():\n",
    "            # Create a dictionary for each row\n",
    "            relationships.append({\n",
    "                \"start\": row[\"source\"],\n",
    "                \"end\": row[\"target\"],\n",
    "                \"properties\": row.to_dict(),\n",
    "            })\n",
    "        return relationships\n",
    "\n",
    "    w = GraphWidget()\n",
    "    # use the converted data to visualize the graph\n",
    "    w.nodes = convert_entities_to_dicts(result.context_data[\"entities\"])\n",
    "    w.edges = convert_relationships_to_dicts(result.context_data[\"relationships\"])\n",
    "    w.directed = True\n",
    "    # show title on the node\n",
    "    w.node_label_mapping = \"entity\"\n",
    "    # use weight for edge thickness\n",
    "    w.edge_thickness_factor_mapping = \"weight\"\n",
    "    display(w)\n",
    "\n",
    "\n",
    "show_graph(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== CONTEXT DATA ANALYSIS ===\n",
      "\n",
      "📊 Entities in context: 7 entities\n",
      "   - Columns: ['id', 'entity', 'description', 'number of relationships', 'in_context']\n",
      "   - Sample entity: SAS ECONOMETRICS PROCEDURES\n",
      "📊 Relationships in context: 1 relationships\n",
      "   - Columns: ['id', 'source', 'target', 'description', 'weight', 'in_context']\n",
      "   - Sample relationship: SAS -> SAS ECONOMETRICS PROCEDURES\n",
      "📊 Community reports in context: 2 reports\n",
      "📊 Text sources in context: 3 text units\n",
      "\n",
      "=== TOKEN ANALYSIS ===\n",
      "\n",
      "🔢 Token Breakdown:\n",
      "   - System prompt: 604 tokens\n",
      "   - Context data: 6,978 tokens\n",
      "   - User question: 21 tokens\n",
      "   - Total INPUT: 7,603 tokens\n",
      "   - Response: 601 tokens\n",
      "   - TOTAL MESSAGE: 8,204 tokens\n",
      "\n",
      "📏 Model Capacity:\n",
      "   - Model limit: 128,000 tokens\n",
      "   - Used: 8,204 tokens (6.4%)\n",
      "   - Remaining: 119,796 tokens\n",
      "   ✅ Token count is within safe limits\n",
      "\n",
      "================================================================================\n",
      "=== FULL CONTEXT SENT TO LLM ===\n",
      "================================================================================\n",
      "\n",
      "� SYSTEM PROMPT (Skipped)\n",
      "----------------------------------------\n",
      "\n",
      "🔵 USER QUESTION:\n",
      "----------------------------------------\n",
      "How can we use SAS Econometric products to help analyze the impact of different pricing strategies on business revenue?\n",
      "\n",
      "🔵 CONTEXT DATA (6,978 tokens):\n",
      "----------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Relevant Entities:\n",
       "\n",
       "**SAS ECONOMETRICS PROCEDURES** (Rank: N/A)\n",
       "Description: SAS Econometrics Procedures are a comprehensive suite of statistical analysis tools designed for conducting a wide range of econometric analyses. These procedures are particularly useful for the levelization of classification variables, which is a critical step in preparing data for further econometric modeling. The suite includes a variety of statistical methods tailored to address different aspects of econometric analysis, ensuring that users have the flexibility and precision needed for their specific research or business needs.\n",
       "\n",
       "Among the key procedures included in SAS Econometrics are CNTSELECT, CPANEL, CQLIM, CSPATIALREG, and SEVSELECT. Each of these methods serves a distinct purpose within the econometric analysis framework. CNTSELECT is typically used for selecting the best model from a set of candidate models, ensuring that the chosen model is the most appropriate for the data at hand. CPANEL is designed for panel data analysis, allowing users to handle data that involves observations over multiple time periods for the same entities, which is common in economic and financial studies.\n",
       "\n",
       "CQLIM is another critical procedure, used for limited dependent variable models, which are essential when dealing with outcomes that are categorical or otherwise constrained. CSPATIALREG is employed for spatial regression analysis, enabling users to account for spatial dependencies in their data, which is particularly important in fields such as regional economics and real estate. Lastly, SEVSELECT is used for selection models, helping to correct for selection bias in econometric models, which can otherwise lead to inaccurate estimates and conclusions.\n",
       "\n",
       "Overall, SAS Econometrics Procedures provide a robust and versatile toolkit for econometricians and analysts, facilitating the execution of complex statistical analyses with precision and efficiency. These procedures are integral to the SAS software suite, known for its powerful data analysis capabilities, and are widely used in academia, government, and industry for their reliability and comprehensive approach to econometric challenges.\n",
       "\n",
       "**SAS ECONOMETRICS** (Rank: N/A)\n",
       "Description: SAS Econometrics is a comprehensive software suite developed by SAS Institute Inc., designed to facilitate advanced econometric analysis. This suite is part of the broader SAS software ecosystem and provides a wide array of tools and procedures tailored for statistical and econometric modeling. It is particularly noted for its capabilities in time series analysis, econometric modeling, and Bayesian inference.\n",
       "\n",
       "The suite includes a variety of specialized procedures such as PROC UCM for time series analysis, PROC SEVSELECT and PROC CNTSELECT for econometric analysis, and the ECM procedure for developing economic capital models. These procedures are instrumental in implementing modeling and simulation steps in economic capital modeling. SAS Econometrics also offers the CCDM and CCOPULA procedures, which are used for modeling compound distributions and copulas, respectively.\n",
       "\n",
       "One of the standout features of SAS Econometrics is its support for Bayesian analysis, which is facilitated through procedures like CQLIM, CNTSELECT, and SMC. These procedures employ advanced algorithms such as the random walk Metropolis (RWM) algorithm, which is enhanced with self-tuning capabilities to optimize Bayesian inference processes. The suite also includes tools for predictive modeling and various econometric methods, making it a versatile choice for econometricians and data analysts.\n",
       "\n",
       "SAS Econometrics operates in a cloud environment, providing users with the flexibility and scalability needed for large-scale data analysis. This cloud-based approach ensures that users can access the software's powerful features from anywhere, facilitating collaboration and efficiency in data-driven decision-making.\n",
       "\n",
       "Overall, SAS Econometrics is a robust and versatile toolset that empowers users to conduct sophisticated econometric analyses, develop predictive models, and perform Bayesian inference with ease and precision. Its comprehensive suite of procedures and cloud-based accessibility make it an invaluable resource for professionals in the field of econometrics.\n",
       "\n",
       "**SAS/ETS SOFTWARE** (Rank: N/A)\n",
       "Description: \n",
       "\n",
       "**SAS/ETS** (Rank: N/A)\n",
       "Description: SAS/ETS is a comprehensive software suite within the SAS software ecosystem, specifically designed to provide robust tools for econometric and time series analysis. This suite includes a variety of procedures that cater to different analytical needs, making it a versatile choice for professionals in the field of econometrics and time series data analysis.\n",
       "\n",
       "One of the key features of SAS/ETS is its inclusion of the UCM (Unobserved Components Model) procedure, which is instrumental in time series analysis. This procedure allows users to decompose time series data into components such as trend, seasonal, and irregular components, facilitating a deeper understanding of the underlying patterns in the data.\n",
       "\n",
       "In addition to UCM, SAS/ETS offers a wide array of other procedures that enhance its functionality. These include ARIMA (AutoRegressive Integrated Moving Average), which is widely used for forecasting and understanding time series data; ESM (Exponential Smoothing Models), which provides a framework for smoothing time series data; VARMAX (Vector Autoregressive Moving-Average with Exogenous Inputs), which is useful for multivariate time series analysis; STATESPACE, which is used for state space modeling; and PANEL, which is designed for panel data analysis.\n",
       "\n",
       "SAS/ETS also includes procedures specifically for count-data modeling, such as COUNTREG and HPCOUNTREG. These procedures are essential for estimating count regression models, which are used when the data being analyzed are counts or non-negative integers. This capability is particularly useful in fields such as biostatistics, epidemiology, and social sciences, where count data is prevalent.\n",
       "\n",
       "Overall, SAS/ETS stands out as a powerful and flexible toolset for econometric and time series analysis, offering a wide range of procedures that cater to both basic and advanced analytical needs. Its integration within the broader SAS software suite ensures that users have access to a comprehensive set of tools for data analysis, making it a valuable resource for analysts and researchers alike.\n",
       "\n",
       "**PROC DEEPPRICE** (Rank: N/A)\n",
       "Description: PROC DEEPPRICE is a versatile procedure designed for deep learning and policy evaluation, particularly in the context of analyzing price and demand data. It is equipped to handle continuous outcome variables by utilizing the identity function for G in the outcome model, which simplifies the process of modeling these types of variables. This procedure is instrumental in estimating demand curves, allowing users to specify the correct functional form to accurately capture the relationship between price and demand. Additionally, PROC DEEPPRICE is adept at estimating optimal revenue per user by accounting for heterogeneous price effects based on user characteristics, thereby enabling more personalized and effective pricing strategies.\n",
       "\n",
       "The procedure offers several options to enhance its functionality and adaptability. Users can specify the minibatch size, which is crucial for managing computational resources and optimizing the learning process in deep learning applications. Furthermore, PROC DEEPPRICE includes options for random seed generation, ensuring reproducibility and consistency in results across different runs. It also provides mechanisms for handling missing values, which is essential for maintaining the integrity and accuracy of the data analysis.\n",
       "\n",
       "In the context of price effect estimation, PROC DEEPPRICE saves the estimation details, facilitating a comprehensive analysis of how price changes impact demand. This feature is particularly useful for businesses and researchers aiming to understand and predict consumer behavior in response to pricing strategies. By integrating these capabilities, PROC DEEPPRICE serves as a powerful tool for those seeking to leverage data-driven insights to optimize pricing and maximize revenue.\n",
       "\n",
       "Overall, PROC DEEPPRICE stands out as a robust procedure that combines deep learning techniques with advanced policy evaluation methods to deliver precise and actionable insights into price and demand dynamics. Its ability to handle continuous outcome variables, estimate demand curves, and account for heterogeneous price effects makes it an invaluable asset for analysts and decision-makers in various industries.\n",
       "\n",
       "**SAS INSTITUTE INC.** (Rank: N/A)\n",
       "Description: SAS Institute Inc. is a prominent software company headquartered in Cary, North Carolina, USA. Renowned for its expertise in analytics software, SAS Institute Inc. has established itself as a leader in the development of advanced statistical and econometric tools. The company is particularly well-known for its comprehensive statistical software suite, which is widely used across various industries for data analysis and decision-making processes.\n",
       "\n",
       "In addition to its statistical software, SAS Institute Inc. offers a range of analytics services that cater to diverse business needs. Among its notable offerings is SAS Econometrics, a sophisticated toolset designed to provide predictive modeling capabilities and econometrics procedures. This suite of tools is instrumental for businesses and researchers who require robust analytical solutions to forecast trends, analyze economic data, and make informed decisions based on quantitative insights.\n",
       "\n",
       "SAS Institute Inc.'s commitment to innovation and excellence in analytics software has made it a trusted partner for organizations seeking to leverage data for strategic advantage. By continuously enhancing its software capabilities and expanding its service offerings, SAS Institute Inc. remains at the forefront of the analytics industry, empowering users to harness the power of data for improved outcomes.\n",
       "\n",
       "**SAS SOFTWARE** (Rank: N/A)\n",
       "Description: SAS Software is a suite of statistical software tools used for data analysis, including regression modeling and other statistical procedures\n",
       "\n",
       "\n",
       "## Relevant Relationships:\n",
       "\n",
       "**SAS → SAS ECONOMETRICS PROCEDURES** (Weight: 1.0)\n",
       "Description: SAS Econometrics Procedures are part of the SAS software suite for econometric analysis\n",
       "\n",
       "\n",
       "## Relevant Community Reports:\n",
       "\n",
       "**PROC CSSM and SAS Cloud Analytic Services** (Rank: N/A)\n",
       "# PROC CSSM and SAS Cloud Analytic Services\n",
       "\n",
       "The community is centered around PROC CSSM, a versatile procedure within SAS software, which is integral for model fitting, scoring, and forecasting in state space models. It is closely related to SAS Cloud Analytic Services, which it requires to run, and interacts with various data tables and procedures within the SAS ecosystem. The community's entities are involved in complex statistical analyses and simulations, contributing significantly to the field of econometrics and time series analysis.\n",
       "\n",
       "## PROC CSSM's Role in Statistical Modeling\n",
       "\n",
       "PROC CSSM is a key procedure within SAS software, designed for model fitting, scoring, and forecasting in state space models. It is particularly adept at handling complex statistical analyses and simulations, offering solutions that complement several SAS/ETS procedures and the MIXED procedure in SAS/STAT software. This procedure is essential for statisticians and data analysts, providing a comprehensive suite of tools for detailed statistical analysis [Data: Entities (1272); Relationships (1415, 1556, 1581, 1659, 1781, +more)].\n",
       "\n",
       "## Integration with SAS Cloud Analytic Services\n",
       "\n",
       "PROC CSSM requires SAS Cloud Analytic Services to run, highlighting its integration within the SAS ecosystem. This integration allows PROC CSSM to efficiently handle large data sets and perform complex analyses in a cloud environment, enhancing its utility for users who need to conduct what-if analysis and incorporate new information into their models [Data: Relationships (1415)].\n",
       "\n",
       "## Applications in Time Series and Econometric Analysis\n",
       "\n",
       "PROC CSSM is extensively used in time series and econometric analysis, as evidenced by its application in modeling the Nile River's water levels and the sales of mink and muskrat furs. These applications demonstrate PROC CSSM's capability to fit models to various types of data, providing valuable insights into the dynamics of complex systems [Data: Relationships (1779, 1538)].\n",
       "\n",
       "## Complementary Procedures and Data Tables\n",
       "\n",
       "PROC CSSM complements several other procedures and data tables within the SAS software suite, such as the MIXED procedure, STATESPACE, and various MYLIB data tables. This complementarity enhances its functionality, allowing users to perform more general or detailed analyses and simulations [Data: Relationships (1556, 1549, 1554, 1792, 1659, 1781, +more)].\n",
       "\n",
       "## Significance of the Nile River Data\n",
       "\n",
       "The Nile River data, used in conjunction with PROC CSSM, provides a rich dataset for historical data analysis. This data is crucial for understanding patterns and fluctuations in the river's flow over time, which have significant impacts on agriculture, water management, and regional planning. The analysis of this data using PROC CSSM highlights the procedure's capability in handling real-world data for meaningful insights [Data: Entities (1476, 1270); Relationships (1779, 1781)].\n",
       "\n",
       "**SAS Cloud Analytic Services and SAS Viya** (Rank: N/A)\n",
       "# SAS Cloud Analytic Services and SAS Viya\n",
       "\n",
       "The community is centered around SAS Cloud Analytic Services (CAS) and SAS Viya, which are integral components of the SAS software suite. CAS serves as a cloud-based platform for advanced data analytics, while SAS Viya provides a distributed environment for analytics. These entities are interconnected with various SAS procedures and components, enhancing the capabilities of the SAS ecosystem in data management and analysis.\n",
       "\n",
       "## SAS Cloud Analytic Services as a central analytic platform\n",
       "\n",
       "SAS Cloud Analytic Services (CAS) is a comprehensive cloud-based platform integral to SAS Viya, designed to facilitate advanced data analytics and management. CAS supports distributed computing environments, enabling efficient data processing and analytics across clusters of machines or in single-machine mode. This service is particularly adept at exploiting available cores and threads, thereby optimizing computational efficiency and minimizing data movement. CAS is essential for running various procedures such as PROC CSSM, PROC ECM, and the CQLIM procedure, which benefit from its ability to distribute optimization tasks across multiple nodes. This distributed nature allows for enhanced performance and scalability, making it suitable for handling large datasets and complex analytical tasks [Data: Entities (5); Relationships (244, 243, 354, 384, 463)].\n",
       "\n",
       "## SAS Viya's role in distributed analytics\n",
       "\n",
       "SAS Viya is a cloud-enabled platform developed by SAS, providing a distributed environment for advanced analytics. As part of the comprehensive SAS software suite, SAS Viya serves as a powerful analytics engine designed to facilitate cloud-based analytics. This product is specifically engineered to leverage the capabilities of cloud technology, providing users with advanced tools for data analysis and insights generation. SAS Viya's cloud-enabled nature allows for scalable and flexible analytics solutions, making it an ideal choice for organizations seeking to harness the power of cloud computing in their data-driven decision-making processes [Data: Entities (3); Relationships (3, 237)].\n",
       "\n",
       "## Integration of SAS procedures with CAS and SAS Viya\n",
       "\n",
       "The integration of various SAS procedures with CAS and SAS Viya enhances the analytical capabilities of the SAS ecosystem. Procedures such as PROC ECM, PROC CARIMA, and PROC TIMEDATA are designed to leverage the distributed computing environment provided by CAS and SAS Viya. This integration allows for efficient data processing and analysis, enabling users to perform complex statistical analyses and generate insights from large datasets. The ability to run these procedures in parallel across multiple nodes significantly accelerates analytical operations, making SAS a powerful tool for organizations dealing with large-scale data [Data: Entities (300, 401, 3380); Relationships (2100, 384, 3452)].\n",
       "\n",
       "## SASEMOOD's integration with SAS for enhanced data access\n",
       "\n",
       "SASEMOOD is an interface engine specifically designed for use within SAS, facilitating the retrieval of data from Moody's Analytics Data Buffet. By integrating SASEMOOD into SAS, users are empowered to seamlessly access and utilize time series data from Moody's Analytics, thereby enhancing their analytical capabilities and decision-making processes. This integration allows users to leverage the robust data sets available in the Data Buffet, which include historical and forecasted data across various sectors and geographies. The ease of access provided by SASEMOOD is a significant advantage for organizations that rely on timely and accurate data to drive their business strategies and operations [Data: Entities (2981); Relationships (2917, 2900, 2928)].\n",
       "\n",
       "## Role of SAS Econometrics Procedures in financial analysis\n",
       "\n",
       "SAS Econometrics Procedures are a comprehensive suite of statistical analysis tools designed for conducting a wide range of econometric analyses. These procedures are particularly useful for the levelization of classification variables, which is a critical step in preparing data for further econometric modeling. The suite includes a variety of statistical methods tailored to address different aspects of econometric analysis, ensuring that users have the flexibility and precision needed for their specific research or business needs. Among the key procedures included in SAS Econometrics are CNTSELECT, CPANEL, CQLIM, CSPATIALREG, and SEVSELECT, each serving distinct purposes within the econometric analysis framework [Data: Entities (290); Relationships (247)].\n",
       "\n",
       "\n",
       "## Relevant Text Sources:\n",
       "\n",
       "**Source 30** (Rank: N/A)\n",
       " and associated cloud services in SAS Viya. This section describes how to create a CAS session and set up a CAS engine libref that you can use to connect to the CAS session. It assumes that you have a CAS server already available; contact your system administrator if you need help starting and terminating a server. This CAS server is identifed by specifying the host on which it runs and the port on which it listens for communications. To simplify your interactions with this CAS server, the host information and port information for the server are stored as SAS option values that are retrieved automatically whenever this CAS server needs to be accessed. You can examine the host and port values for the server at your site by using the following statements: \n",
       "proc options option=(CASHOST CASPORT); run; \n",
       "In addition to starting a CAS server, your system administrator might also have created a CAS session and a CAS engine libref for your use. You can defne your own sessions and CAS engine librefs that connect to the CAS server as shown in the following statements: \n",
       "cas mysess; libname mylib cas sessref=mysess; \n",
       "The CAS statement creates the CAS session named mysess, and the LIBNAME statement creates the mylib CAS engine libref that you use to connect to this session. It is not necessary to explicitly name the CASHOST and CASPORT of the CAS server in the CAS statement, because these values are retrieved from the corresponding SAS option values. \n",
       "If you have created the mysess session, you can terminate it by using the TERMINATE option in the CAS statement as follows: \n",
       "cas mysess terminate; \n",
       "For more information about the CAS statement and the LIBNAME statement, see SAS Cloud Analytic Services: Users Guide. For general information about CAS and CAS sessions, see SAS Cloud Analytic Services: Fundamentals. \n",
       "\n",
       "Loading a SAS Data Set onto a CAS Server \n",
       "Procedures in this book require the input data to reside on a CAS server. To work with a SAS data set, you must frst load the data set onto the CAS server. Data loaded on the CAS server are called data tables. This section lists three methods of loading a SAS data set onto a CAS server. In this section, mylib is the name of the caslib that is connected to the mysess CAS session. \n",
       "\u000f You can use a single DATA step to create a data table on the CAS server as follows: \n",
       "data mylib.Sample; input y x @@; datalines; \n",
       ".461 .472 .573.61 4.62 5.68 6.69 7 ; \n",
       "Note that DATA step operations might not work as intended when you perform them on the CAS server instead of the SAS client. \n",
       "\u000f You can create a SAS data set frst, and when it contains exactly what you want, you can use another DATA step to load it onto the CAS server as follows: \n",
       "data Sample; input y x @@; datalines; \n",
       ".461 .472 .573.61 4.62 5.68 6.69 7.788 ; data mylib.Sample; \n",
       "set Sample; run; \n",
       "\u000f You can use the CASUTIL procedure as follows: \n",
       "proc casutil sessref=mysess; load data=Sample casout=\"Sample\"; quit; \n",
       "The CASUTIL procedure can load data onto a CAS server more effciently than the DATA step. For more information about the CASUTIL procedure, see SAS Cloud Analytic Services: Users Guide. \n",
       "The mylib caslib stores the Sample data table, which can be distributed across many machine nodes. You must use a caslib reference in procedures in this book to enable the SAS client machine to communicate with the CAS session. For example, the following SEVSELECT procedure statements use a data table that resides in the mylib caslib: \n",
       "proc sevselect data = mylib.Sample; ...statements...; run; \n",
       "You can delete your data table by using the DELETE procedure as follows: \n",
       "proc delete data = mylib.Sample; run; \n",
       "The Sample data table is accessible only in the mysess session. When you terminate the mysess session, the Sample data table is no longer accessible from the CAS server. If you want your Sample data table to be available to other CAS sessions, then you must promote your data table. For more information about data tables, see SAS Cloud Analytic Services: Users Guide. \n",
       "\n",
       "\n",
       "Syntax Common to SAS Econometrics Procedures \n",
       "CLASS Statement \n",
       "CLASS variable < (options) >:::< variable < (options) >>< / global-options > ; \n",
       "This section applies to the following procedures: CNTSELECT, CPANEL, CQLIM, CSPATIALREG, and SEVSELECT. \n",
       "The CLASS statement names the classifcation variables to be used as explanatory variables in the analysis. These variables enter the analysis not through their values, but through levels to which the unique values are mapped. For more information about these mappings, see the section Levelization of Classifcation Variables on page 89. \n",
       "If the procedure permits a classifcation variable as a response (dependent variable or target), the response does not need to be specifed in the CLASS statement. \n",
       "You can specify options either as individual variable options, by enclosing the options in parentheses after the variable name, or as global-options, by placing them after a slash (/). Global-options are applied to all variables that are specifed in the CLASS statement. If you specify more than one CLASS statement, the global-options that are specifed in any one CLASS statement apply to all CLASS statements. However, individual CLASS variable options override the global-options. \n",
       "Table 4.1 summarizes the values you can use for either an option or a global-option. The options are described in detail in the list that follows Table 4.1.\n",
       "\n",
       "**Source 45** (Rank: N/A)\n",
       " the vertex that has the \n",
       ".k.1/\n",
       "lowest function value and .j is defned as the vertex that has the highest function value in the simplex. The default value is r = 1E8 for the NMSIMP technique and r = 0 otherwise. \n",
       "XSIZE=r \n",
       "specifes the XSIZE parameter of the relative parameter termination criterion. The value of r must be \n",
       "greater than or equal to 0; the default is r D 0. For more information, see the XCONV= option. \n",
       "\n",
       "\n",
       "Details for SAS Econometrics Procedures \n",
       "Levelization of Classifcation Variables \n",
       "This section applies to the following procedures: CNTSELECT, CPANEL, CQLIM, CSPATIALREG, and SEVSELECT. \n",
       "A classifcation variable enters the statistical analysis or model not through its values but through its levels. The process of associating values of a variable with levels is called levelization. \n",
       "During the process of levelization, observations that share the same value are assigned to the same level. The manner in which values are grouped can be affected by the inclusion of formats. The sort order of the levels can be determined by specifying the ORDER= option in the procedure statement. In procedures in this book, you can also control the sorting order separately for each variable in the CLASS statement. \n",
       "Consider the data on nine observations in Table 4.7. The variable A is integer-valued, and the variable X is a continuous variable that has a missing value for the fourth observation. The fourth and ffth columns of Table 4.7 apply two different formats to the variable X. \n",
       "Table 4.7 Example Data for Levelization \n",
       "Obs  A  x  FORMAT  FORMAT  \n",
       "x 3.0  x 3.1  \n",
       "1  2  1.09  1  1.1  \n",
       "2  2  1.13  1  1.1  \n",
       "3  2  1.27  1  1.3  \n",
       "4  3  .  .  .  \n",
       "5  3  2.26  2  2.3  \n",
       "6  3  2.48  2  2.5  \n",
       "7  4  3.34  3  3.3  \n",
       "8  4  3.34  3  3.3  \n",
       "9  4  3.14  3  3.1  \n",
       "\n",
       "By default, levelization of the variables groups the observations by the formatted value of the variable, except for numerical variables for which no explicit format is provided. Numerical variables for which no explicit format is provided are sorted by their internal value. The levelization of the four columns in Table 4.7 leads to the level assignment in Table 4.8. \n",
       "Table 4.8 Values and Levels \n",
       "A  X  FORMAT x 3.0  FORMAT x 3.1  \n",
       "Obs  Value Level  Value Level  Value Level  Value Level  \n",
       "1  2  1  1.09  1  1  1  1.1  1  \n",
       "\n",
       "Table 4.8 continued \n",
       "A  X  FORMAT x 3.0  FORMAT x 3.1  \n",
       "Obs  Value Level  Value Level  Value Level  Value Level  \n",
       "2  2  1  1.13  2  1  1  1.1  1  \n",
       "3  2  1  1.27  3  1  1  1.3  2  \n",
       "4  3  2  .  .  .  .  .  .  \n",
       "5  3  2  2.26  4  2  2  2.3  3  \n",
       "6  3  2  2.48  5  2  2  2.5  4  \n",
       "7  4  3  3.34  7  3  3  3.3  6  \n",
       "8  4  3  3.34  7  3  3  3.3  6  \n",
       "9  4  3  3.14  6  3  3  3.1  5  \n",
       "\n",
       "The sort order for the levels of CLASS variables can be specifed in the ORDER= option in the CLASS statement. \n",
       "When ORDER=FORMATTED (which is the default) is in effect for numeric variables for which you have supplied no explicit format, the levels are ordered by their internal values. To order numeric classifcation levels that have no explicit format by their BEST12. formatted values, you can specify the BEST12. format explicitly for the CLASS variables. \n",
       "Table 4.9 shows how values of the ORDER= option are interpreted. \n",
       "Table 4.9 Interpretation of Values of ORDER= Option \n",
       "Value of ORDER=  Levels Sorted By  \n",
       "FORMATTED  External formatted value, except for numeric variables that have no explicit format, which are sorted by their unformatted (internal) value. The sort order is machine-dependent.  \n",
       "FREQ  Descending frequency count (levels that have the most observations come frst in the order)  \n",
       "INTERNAL  Unformatted value. The sort order is machine-dependent.  \n",
       "\n",
       "For more information about sort order, see the chapter about the SORT procedure in the Base SAS Procedures Guide and the discussion of BY-group processing in the Grouping Data section of SAS Programmers Guide: Essentials. \n",
       "When the MISSING option is specifed in\n",
       "\n",
       "**Source 10** (Rank: N/A)\n",
       " of the log posterior at the posterior mode (Gelman et al. 2004, Appendix B; Schervish 1995, Section 7.4). That is why a normal proposal distribution often works well in practice. The proposal distribution for random walk Metropolis in SAS Econometrics procedures is always the normal distribution. It is generated in the following fashion: \n",
       "\u0012prop \u0018 N.\u0012cur curCOVcur\n",
       ";c / \n",
       "where \u0012prop is the proposed parameter vector, \u0012cur is the previous value of the parameters in the Markov chain, ccur is a scale parameter that is tuned in the tuning phase of the algorithm, and COVcur is an approximation of the posterior covariance that is also tuned in the tuning phase of the algorithm. When there are multiple parameter blocksthat is, in a Metropolis within Gibbs schemeeach block is tuned separately, each with its own ccur and COVcur. In particular for a scalar block, COVcur is also a scalaran approximation to the posterior variance of the only parameter in that block. \n",
       "The tuning phase consists of nstage tuning stages, each ntune iterations long. Between tuning stages, two sorts of adjustments to the proposal distribution are made: adjustments to the scale parameter ccur and adjustments to the proposal covariance matrix COVcur. \n",
       "Scale Tuning \n",
       "The acceptance rate of a Metropolis chain is closely related to its sampling effciency. For a random walk Metropolis algorithm, a high acceptance rate means that most new samples occur right around the current parameter value. Their frequent acceptance means that the Markov chain is moving rather slowly and not fully exploring the parameter space. A low acceptance rate means that the proposed samples are often rejected; hence the chain is not moving much. An effcient random walk Metropolis sampler has an acceptance rate that is neither too high nor too low. The scale parameter ccur in the proposal distribution effectively controls this acceptance probability. Roberts, Gelman, and Gilks (1997) show that if both the target density and the proposal density are normal, the optimal acceptance probability for the Markov chain should be around 0.45 in a one-dimensional problem and should asymptotically approach 0.234 in higher-dimensional problems. The corresponding optimal scale is 2.38. \n",
       "Because of the nature of stochastic simulations, it is impossible to fne-tune a set of variables so that the Metropolis chain has exactly the target acceptance rate that you want. In addition, Roberts and Rosenthal (2001) empirically demonstrate that an acceptance rate between 0.15 and 0.5 is at least 80% effcient, so there is really no need to fne-tune the algorithms to reach an acceptance probability that is within a small tolerance of the optimal values. SAS Econometrics procedures tune the scale parameter so that the observed acceptance rate falls within some tolerance of the target acceptance rate. Let pobs denote the observed acceptance rate in a given tuning stage, let idenote the target acceptance rate, and let \u000fdenote the tolerance. \n",
       "obs obs\n",
       "Then if p <i. \u000f, the value of ccur is decreased. On the other hand, if p >iC \u000f, then the value of ccur is increased. Typically \u000f D 0:075is a good choice, i D 0:45is a good choice for blocks of parameters larger than three or four dimensions, and i D 0:234is a good choice for scalar blocks. \n",
       "When ccur is adjusted between tuning stages, it is done using the following scheme:1 \n",
       "ccur \u0001 .1.i=2/ \n",
       "new D\n",
       "c .1 obs=2/ \n",
       ".p \n",
       "where ccur is the scale from the previous tuning stage; cnew is the new scale for the next stage or any further sampling; pobs is the observed acceptance rate in the previous tuning stage; and iis the target acceptance rate. A good choice for the initial value of ccur is 2.38 because it is optimal for a normal model with a normal proposal. \n",
       "\n",
       "Covariance Tuning \n",
       "When the proposal scale parameter is adjusted between stages, the proposal covariance matrix is also adjusted. To do this, SAS Econometrics procedures take a weighted average of the previously used covariance matrix and the observed covariance matrix in the last tuning stage. The formula to update the covariance matrix is \n",
       "COVnew D COVobs C .1. w/COVcur \n",
       "w \n",
       "1 Roberts, Gelman, and Gilks (1997) and Roberts and Rosenthal (2001) demonstrate that the relationship between acceptance \n",
       "\u0010 p \u0011 probability and scale in a random walk Metropolis scheme is p D 2 . Ic=2 , where cis the scale; p is the acceptance rate;  \n",
       "is the cumulative distribution function of a standard normal distribution; and I \u0011 Ef.f0.x/=f.x//2, where f.x/is the density function of samples. This relationship determines the updating scheme, with I replaced by the identity matrix to simplify calculation. \n",
       "where w is the tuning weight, between 0 and 1; COVcur is the covariance matrix that was used in the previous tuning stage; COVobs is the covariance matrix that was observed in the previous tuning stage; and COVnew is the new covariance matrix that will be used for the next tuning stage or any further sampling. Larger weights cause the sampler to adapt the proposal covariance matrix faster during the tuning process, but if this value is too large, the sampler might never settle into a good choice. Typically, a good tuning weight is w D 0:75. If the initial value of COVcur is very different from the posterior covariance matrix, many \n",
       "tuning stages might be\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "=== END OF CONTEXT ===\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Analyze the context data and token usage that the LLM receives\n",
    "import tiktoken\n",
    "from IPython.display import Markdown, display\n",
    "\n",
    "def analyze_context_and_tokens(result, search_engine):\n",
    "    \"\"\"Analyze the context data and count tokens for different message components.\"\"\"\n",
    "    \n",
    "    # Get the token encoder\n",
    "    encoding = tiktoken.encoding_for_model(\"gpt-4\")\n",
    "    \n",
    "    print(\"=== CONTEXT DATA ANALYSIS ===\\n\")\n",
    "    \n",
    "    # Show context data statistics\n",
    "    if \"entities\" in result.context_data:\n",
    "        entities_df = result.context_data[\"entities\"]\n",
    "        print(f\"📊 Entities in context: {len(entities_df)} entities\")\n",
    "        print(f\"   - Columns: {list(entities_df.columns)}\")\n",
    "        print(f\"   - Sample entity: {entities_df.iloc[0]['entity'] if len(entities_df) > 0 else 'None'}\")\n",
    "    \n",
    "    if \"relationships\" in result.context_data:\n",
    "        relationships_df = result.context_data[\"relationships\"]\n",
    "        print(f\"📊 Relationships in context: {len(relationships_df)} relationships\")\n",
    "        print(f\"   - Columns: {list(relationships_df.columns)}\")\n",
    "        print(f\"   - Sample relationship: {relationships_df.iloc[0]['source']} -> {relationships_df.iloc[0]['target'] if len(relationships_df) > 0 else 'None'}\")\n",
    "    \n",
    "    if \"reports\" in result.context_data:\n",
    "        reports_df = result.context_data[\"reports\"]\n",
    "        print(f\"📊 Community reports in context: {len(reports_df)} reports\")\n",
    "    \n",
    "    if \"sources\" in result.context_data:\n",
    "        sources_df = result.context_data[\"sources\"]\n",
    "        print(f\"📊 Text sources in context: {len(sources_df)} text units\")\n",
    "    \n",
    "    print(\"\\n=== TOKEN ANALYSIS ===\\n\")\n",
    "    \n",
    "    # Reconstruct the context that was sent to the LLM\n",
    "    # This is an approximation of what the LocalSearch builds\n",
    "    \n",
    "    # 1. System prompt\n",
    "    with open(\"/home/chuaxu/projects/graphrag/ragsas/prompts/local_search_system_prompt.txt\", \"r\") as f:\n",
    "        system_prompt = f.read()\n",
    "\n",
    "    # 2. Context data formatting (full version of what LocalSearch does)\n",
    "    context_parts = []\n",
    "    \n",
    "    # Add entities context (FULL - no truncation)\n",
    "    if \"entities\" in result.context_data and len(result.context_data[\"entities\"]) > 0:\n",
    "        entities_context = \"## Relevant Entities:\\n\\n\"\n",
    "        for _, entity in result.context_data[\"entities\"].iterrows():  # Show ALL entities\n",
    "            desc = entity.get('description', 'No description')\n",
    "            rank = entity.get('rank', 'N/A')\n",
    "            entities_context += f\"**{entity['entity']}** (Rank: {rank})\\n\"\n",
    "            entities_context += f\"Description: {desc}\\n\\n\"\n",
    "        context_parts.append(entities_context)\n",
    "    \n",
    "    # Add relationships context (FULL - no truncation)\n",
    "    if \"relationships\" in result.context_data and len(result.context_data[\"relationships\"]) > 0:\n",
    "        relationships_context = \"## Relevant Relationships:\\n\\n\"\n",
    "        for _, rel in result.context_data[\"relationships\"].iterrows():  # Show ALL relationships\n",
    "            desc = rel.get('description', 'No description')\n",
    "            weight = rel.get('weight', 'N/A')\n",
    "            relationships_context += f\"**{rel['source']} → {rel['target']}** (Weight: {weight})\\n\"\n",
    "            relationships_context += f\"Description: {desc}\\n\\n\"\n",
    "        context_parts.append(relationships_context)\n",
    "    \n",
    "    # Add community reports context (FULL - no truncation)\n",
    "    if \"reports\" in result.context_data and len(result.context_data[\"reports\"]) > 0:\n",
    "        reports_context = \"## Relevant Community Reports:\\n\\n\"\n",
    "        for _, report in result.context_data[\"reports\"].iterrows():  # Show ALL reports\n",
    "            title = report.get('title', 'Untitled Report')\n",
    "            content = report.get('content', report.get('summary', 'No content'))\n",
    "            rank = report.get('rank', 'N/A')\n",
    "            reports_context += f\"**{title}** (Rank: {rank})\\n\"\n",
    "            reports_context += f\"{content}\\n\\n\"\n",
    "        context_parts.append(reports_context)\n",
    "    \n",
    "    # Add sources context (FULL - no truncation)\n",
    "    if \"sources\" in result.context_data and len(result.context_data[\"sources\"]) > 0:\n",
    "        sources_context = \"## Relevant Text Sources:\\n\\n\"\n",
    "        for _, source in result.context_data[\"sources\"].iterrows():  # Show ALL sources\n",
    "            text_content = source.get('text', source.get('content', 'No content'))\n",
    "            source_id = source.get('id', 'Unknown')\n",
    "            rank = source.get('rank', 'N/A')\n",
    "            sources_context += f\"**Source {source_id}** (Rank: {rank})\\n\"\n",
    "            sources_context += f\"{text_content}\\n\\n\"\n",
    "        context_parts.append(sources_context)\n",
    "    \n",
    "    # 3. User question\n",
    "    user_question = question  # From the previous cell\n",
    "    \n",
    "    # Combine all context\n",
    "    full_context = \"\\n\".join(context_parts)\n",
    "    \n",
    "    # 4. Calculate tokens for each component\n",
    "    system_prompt_tokens = len(encoding.encode(system_prompt))\n",
    "    context_tokens = len(encoding.encode(full_context))\n",
    "    user_question_tokens = len(encoding.encode(user_question))\n",
    "    response_tokens = len(encoding.encode(result.response))\n",
    "    \n",
    "    total_input_tokens = system_prompt_tokens + context_tokens + user_question_tokens\n",
    "    total_tokens = total_input_tokens + response_tokens\n",
    "    \n",
    "    print(f\"🔢 Token Breakdown:\")\n",
    "    print(f\"   - System prompt: {system_prompt_tokens:,} tokens\")\n",
    "    print(f\"   - Context data: {context_tokens:,} tokens\")\n",
    "    print(f\"   - User question: {user_question_tokens:,} tokens\")\n",
    "    print(f\"   - Total INPUT: {total_input_tokens:,} tokens\")\n",
    "    print(f\"   - Response: {response_tokens:,} tokens\")\n",
    "    print(f\"   - TOTAL MESSAGE: {total_tokens:,} tokens\")\n",
    "    \n",
    "    # Show model limits\n",
    "    model_limit = 128_000  # GPT-4o limit\n",
    "    print(f\"\\n📏 Model Capacity:\")\n",
    "    print(f\"   - Model limit: {model_limit:,} tokens\")\n",
    "    print(f\"   - Used: {total_tokens:,} tokens ({total_tokens/model_limit*100:.1f}%)\")\n",
    "    print(f\"   - Remaining: {model_limit - total_tokens:,} tokens\")\n",
    "    \n",
    "    if total_tokens > model_limit:\n",
    "        print(\"   ⚠️  WARNING: Token count exceeds model limit!\")\n",
    "    elif total_tokens > model_limit * 0.9:\n",
    "        print(\"   ⚠️  WARNING: Token count is near model limit!\")\n",
    "    else:\n",
    "        print(\"   ✅ Token count is within safe limits\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"=== FULL CONTEXT SENT TO LLM ===\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Display the complete context in a nice formatted way\n",
    "    print(f\"\\n� SYSTEM PROMPT (Skipped)\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    print(f\"\\n🔵 USER QUESTION:\")\n",
    "    print(\"-\" * 40)\n",
    "    print(user_question)\n",
    "    \n",
    "    print(f\"\\n🔵 CONTEXT DATA ({context_tokens:,} tokens):\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    # Display full context as Markdown for better formatting\n",
    "    display(Markdown(full_context))\n",
    "    \n",
    "    print(\"=\"*80)\n",
    "    print(\"=== END OF CONTEXT ===\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    return {\n",
    "        \"system_prompt_tokens\": system_prompt_tokens,\n",
    "        \"context_tokens\": context_tokens,\n",
    "        \"user_question_tokens\": user_question_tokens,\n",
    "        \"response_tokens\": response_tokens,\n",
    "        \"total_tokens\": total_tokens,\n",
    "        \"context_data\": result.context_data,\n",
    "        \"full_context\": full_context\n",
    "    }\n",
    "\n",
    "# Run the analysis\n",
    "token_analysis = analyze_context_and_tokens(result, search_engine)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Understanding Relationship Weights in GraphRAG\n",
    "\n",
    "In GraphRAG, the **weight** property on relationships represents the **strength** or **importance** of the connection between two entities. Here's what it means:\n",
    "\n",
    "## What is Weight?\n",
    "\n",
    "**Weight** is a numerical value that indicates:\n",
    "- **Frequency**: How often two entities appear together in the source documents\n",
    "- **Co-occurrence strength**: The statistical significance of their relationship\n",
    "- **Semantic closeness**: How tightly connected the entities are in the knowledge graph\n",
    "\n",
    "## How is Weight Calculated?\n",
    "\n",
    "The weight is typically derived from:\n",
    "1. **Text co-occurrence**: How many times the entities appear in the same text units/chunks\n",
    "2. **Window proximity**: How close the entities appear to each other in the text\n",
    "3. **Relationship strength**: The confidence level of the extracted relationship\n",
    "4. **Document frequency**: Across how many documents the relationship appears\n",
    "\n",
    "## Weight Values\n",
    "\n",
    "- **Higher weights** (e.g., 8.0, 10.0): Strong, frequently occurring relationships\n",
    "- **Lower weights** (e.g., 1.0, 2.0): Weaker or less frequent relationships\n",
    "- **Weight = 1.0**: Often the default/minimum weight for detected relationships\n",
    "\n",
    "## Usage in GraphRAG\n",
    "\n",
    "Weights are used for:\n",
    "- **Ranking relationships**: More important relationships get higher priority in search results\n",
    "- **Graph visualization**: Thicker edges represent stronger relationships (as seen in the yfiles visualization)\n",
    "- **Context selection**: Higher-weight relationships are more likely to be included in LLM context\n",
    "- **Graph algorithms**: Centrality and community detection algorithms use weights to identify key entities\n",
    "\n",
    "Let's examine the weights in our current result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== RELATIONSHIP WEIGHT ANALYSIS ===\n",
      "\n",
      "📊 Weight Statistics:\n",
      "   - Total relationships: 1\n",
      "   - Relationships with weights: 1\n",
      "   - Weight range: 1.00 to 1.00\n",
      "   - Average weight: 1.00\n",
      "   - Median weight: 1.00\n",
      "\n",
      "🔝 Top 5 Strongest Relationships (by weight):\n",
      "   SAS → SAS ECONOMETRICS PROCEDURES (Weight: 1.00)\n",
      "      Description: SAS Econometrics Procedures are part of the SAS software suite for econometric analysis...\n",
      "\n",
      "🔻 Bottom 5 Weakest Relationships (by weight):\n",
      "   SAS → SAS ECONOMETRICS PROCEDURES (Weight: 1.00)\n",
      "      Description: SAS Econometrics Procedures are part of the SAS software suite for econometric analysis...\n",
      "\n",
      "📈 Weight Distribution:\n",
      "   - Weight 0-1: 0 relationships (0.0%)\n",
      "   - Weight 1-2: 1 relationships (100.0%)\n",
      "   - Weight 2-5: 0 relationships (0.0%)\n",
      "   - Weight 5-10: 0 relationships (0.0%)\n",
      "   - Weight 10+: 0 relationships (0.0%)\n"
     ]
    }
   ],
   "source": [
    "# Analyze relationship weights in our current search result\n",
    "print(\"=== RELATIONSHIP WEIGHT ANALYSIS ===\\n\")\n",
    "\n",
    "if \"relationships\" in result.context_data:\n",
    "    relationships_df = result.context_data[\"relationships\"]\n",
    "    \n",
    "    if 'weight' in relationships_df.columns:\n",
    "        # Convert weights to numeric, handling any string values\n",
    "        relationships_df['weight_numeric'] = pd.to_numeric(relationships_df['weight'], errors='coerce')\n",
    "        weights = relationships_df['weight_numeric'].dropna()\n",
    "        \n",
    "        if len(weights) > 0:\n",
    "            print(f\"📊 Weight Statistics:\")\n",
    "            print(f\"   - Total relationships: {len(relationships_df)}\")\n",
    "            print(f\"   - Relationships with weights: {len(weights)}\")\n",
    "            print(f\"   - Weight range: {weights.min():.2f} to {weights.max():.2f}\")\n",
    "            print(f\"   - Average weight: {weights.mean():.2f}\")\n",
    "            print(f\"   - Median weight: {weights.median():.2f}\")\n",
    "            \n",
    "            print(f\"\\n🔝 Top 5 Strongest Relationships (by weight):\")\n",
    "            top_relationships = relationships_df.nlargest(5, 'weight_numeric')[['source', 'target', 'weight_numeric', 'description']]\n",
    "            for idx, row in top_relationships.iterrows():\n",
    "                weight_val = row['weight_numeric']\n",
    "                if pd.notna(weight_val):\n",
    "                    print(f\"   {row['source']} → {row['target']} (Weight: {weight_val:.2f})\")\n",
    "                else:\n",
    "                    print(f\"   {row['source']} → {row['target']} (Weight: N/A)\")\n",
    "                desc = str(row['description'])[:100] if pd.notna(row['description']) else \"No description\"\n",
    "                print(f\"      Description: {desc}...\")\n",
    "                print()\n",
    "            \n",
    "            print(f\"🔻 Bottom 5 Weakest Relationships (by weight):\")\n",
    "            bottom_relationships = relationships_df.nsmallest(5, 'weight_numeric')[['source', 'target', 'weight_numeric', 'description']]\n",
    "            for idx, row in bottom_relationships.iterrows():\n",
    "                weight_val = row['weight_numeric']\n",
    "                if pd.notna(weight_val):\n",
    "                    print(f\"   {row['source']} → {row['target']} (Weight: {weight_val:.2f})\")\n",
    "                else:\n",
    "                    print(f\"   {row['source']} → {row['target']} (Weight: N/A)\")\n",
    "                desc = str(row['description'])[:100] if pd.notna(row['description']) else \"No description\"\n",
    "                print(f\"      Description: {desc}...\")\n",
    "                print()\n",
    "            \n",
    "            # Weight distribution\n",
    "            print(f\"📈 Weight Distribution:\")\n",
    "            weight_bins = [0, 1, 2, 5, 10, float('inf')]\n",
    "            weight_labels = ['0-1', '1-2', '2-5', '5-10', '10+']\n",
    "            \n",
    "            for i, (low, high) in enumerate(zip(weight_bins[:-1], weight_bins[1:])):\n",
    "                if high == float('inf'):\n",
    "                    count = len(weights[weights >= low])\n",
    "                    label = weight_labels[i]\n",
    "                else:\n",
    "                    count = len(weights[(weights >= low) & (weights < high)])\n",
    "                    label = weight_labels[i]\n",
    "                \n",
    "                percentage = count / len(weights) * 100 if len(weights) > 0 else 0\n",
    "                print(f\"   - Weight {label}: {count} relationships ({percentage:.1f}%)\")\n",
    "        else:\n",
    "            print(\"❌ No valid numeric weights found in relationships data\")\n",
    "            print(f\"Sample weight values: {relationships_df['weight'].head().tolist()}\")\n",
    "    else:\n",
    "        print(\"❌ No 'weight' column found in relationships data\")\n",
    "        print(f\"Available columns: {list(relationships_df.columns)}\")\n",
    "else:\n",
    "    print(\"❌ No relationships found in context data\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.11.13)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
