{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualizing the knowledge graph with `yfiles-jupyter-graphs`\n",
    "\n",
    "This notebook is a partial copy of [local_search.ipynb](../../local_search.ipynb) that shows how to use `yfiles-jupyter-graphs` to add interactive graph visualizations of the parquet files  and how to visualize the result context of `graphrag` queries (see at the end of this notebook)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright (c) 2024 Microsoft Corporation.\n",
    "# Licensed under the MIT License."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "import tiktoken\n",
    "\n",
    "from graphrag.config.enums import ModelType\n",
    "from graphrag.config.models.language_model_config import LanguageModelConfig\n",
    "from graphrag.language_model.manager import ModelManager\n",
    "from graphrag.query.context_builder.entity_extraction import EntityVectorStoreKey\n",
    "from graphrag.query.indexer_adapters import (\n",
    "    read_indexer_covariates,\n",
    "    read_indexer_entities,\n",
    "    read_indexer_relationships,\n",
    "    read_indexer_reports,\n",
    "    read_indexer_text_units,\n",
    ")\n",
    "from graphrag.query.structured_search.local_search.mixed_context import (\n",
    "    LocalSearchMixedContext,\n",
    ")\n",
    "from graphrag.query.structured_search.local_search.search import LocalSearch\n",
    "from graphrag.vector_stores.lancedb import LanceDBVectorStore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Local Search Example\n",
    "\n",
    "Local search method generates answers by combining relevant data from the AI-extracted knowledge-graph with text chunks of the raw documents. This method is suitable for questions that require an understanding of specific entities mentioned in the documents (e.g. What are the healing properties of chamomile?)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load text units and graph data tables as context for local search\n",
    "\n",
    "- In this test we first load indexing outputs from parquet files to dataframes, then convert these dataframes into collections of data objects aligning with the knowledge model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load tables to dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_DIR = \"/home/chuaxu/projects/graphrag/ragsas/output\"\n",
    "LANCEDB_URI = f\"{INPUT_DIR}/lancedb\"\n",
    "\n",
    "COMMUNITY_REPORT_TABLE = \"community_reports\"\n",
    "COMMUNITY_TABLE = \"communities\"\n",
    "ENTITY_TABLE = \"entities\"\n",
    "RELATIONSHIP_TABLE = \"relationships\"\n",
    "COVARIATE_TABLE = \"covariates\"\n",
    "TEXT_UNIT_TABLE = \"text_units\"\n",
    "COMMUNITY_LEVEL = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read nodes table to get community and degree data\n",
    "entity_df = pd.read_parquet(f\"{INPUT_DIR}/{ENTITY_TABLE}.parquet\")\n",
    "community_df = pd.read_parquet(f\"{INPUT_DIR}/{COMMUNITY_TABLE}.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read relationships"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "relationship_df = pd.read_parquet(f\"{INPUT_DIR}/{RELATIONSHIP_TABLE}.parquet\")\n",
    "relationships = read_indexer_relationships(relationship_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualizing nodes and relationships with `yfiles-jupyter-graphs`\n",
    "\n",
    "`yfiles-jupyter-graphs` is a graph visualization extension that provides interactive and customizable visualizations for structured node and relationship data.\n",
    "\n",
    "In this case, we use it to provide an interactive visualization for the knowledge graph of the [local_search.ipynb](../../local_search.ipynb) sample by passing node and relationship lists converted from the given parquet files. The requirements for the input data is an `id` attribute for the nodes and `start`/`end` properties for the relationships that correspond to the node ids. Additional attributes can be added in the `properties` of each node/relationship dict:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install yfiles_jupyter_graphs --quiet\n",
    "from yfiles_jupyter_graphs import GraphWidget\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# converts the entities dataframe to a list of dicts for yfiles-jupyter-graphs\n",
    "def convert_entities_to_dicts(df):\n",
    "    \"\"\"Convert the entities dataframe to a list of dicts for yfiles-jupyter-graphs.\"\"\"\n",
    "    def clean_value(value):\n",
    "        \"\"\"Clean a value to make it JSON serializable.\"\"\"\n",
    "        # Handle arrays first (before checking for NaN)\n",
    "        if isinstance(value, (np.ndarray, list)):\n",
    "            # Convert arrays to strings or take first element if single value\n",
    "            if len(value) == 0:\n",
    "                return None\n",
    "            elif len(value) == 1:\n",
    "                return str(value[0])\n",
    "            else:\n",
    "                return str(value)\n",
    "        # Now check for NaN on scalar values\n",
    "        elif pd.isna(value):\n",
    "            return None\n",
    "        elif isinstance(value, (np.integer, np.floating)):\n",
    "            # Convert numpy numbers to Python numbers\n",
    "            if np.isnan(value) or np.isinf(value):\n",
    "                return None\n",
    "            return value.item()\n",
    "        elif isinstance(value, float):\n",
    "            # Handle Python floats that might be NaN or inf\n",
    "            if pd.isna(value) or np.isinf(value):\n",
    "                return None\n",
    "            return value\n",
    "        else:\n",
    "            return value\n",
    "    \n",
    "    nodes_dict = {}\n",
    "    for _, row in df.iterrows():\n",
    "        # Create a dictionary for each row and collect unique nodes\n",
    "        node_id = row[\"title\"]\n",
    "        if node_id not in nodes_dict:\n",
    "            # Clean all properties to make them JSON serializable\n",
    "            cleaned_properties = {k: clean_value(v) for k, v in row.to_dict().items()}\n",
    "            nodes_dict[node_id] = {\n",
    "                \"id\": node_id,\n",
    "                \"properties\": cleaned_properties,\n",
    "            }\n",
    "    return list(nodes_dict.values())\n",
    "\n",
    "\n",
    "# converts the relationships dataframe to a list of dicts for yfiles-jupyter-graphs\n",
    "def convert_relationships_to_dicts(df):\n",
    "    \"\"\"Convert the relationships dataframe to a list of dicts for yfiles-jupyter-graphs.\"\"\"\n",
    "    def clean_value(value):\n",
    "        \"\"\"Clean a value to make it JSON serializable.\"\"\"\n",
    "        # Handle arrays first (before checking for NaN)\n",
    "        if isinstance(value, (np.ndarray, list)):\n",
    "            # Convert arrays to strings or take first element if single value\n",
    "            if len(value) == 0:\n",
    "                return None\n",
    "            elif len(value) == 1:\n",
    "                return str(value[0])\n",
    "            else:\n",
    "                return str(value)\n",
    "        # Now check for NaN on scalar values\n",
    "        elif pd.isna(value):\n",
    "            return None\n",
    "        elif isinstance(value, (np.integer, np.floating)):\n",
    "            # Convert numpy numbers to Python numbers\n",
    "            if np.isnan(value) or np.isinf(value):\n",
    "                return None\n",
    "            return value.item()\n",
    "        elif isinstance(value, float):\n",
    "            # Handle Python floats that might be NaN or inf\n",
    "            if pd.isna(value) or np.isinf(value):\n",
    "                return None\n",
    "            return value\n",
    "        else:\n",
    "            return value\n",
    "    \n",
    "    relationships = []\n",
    "    for _, row in df.iterrows():\n",
    "        # Create a dictionary for each row\n",
    "        cleaned_properties = {k: clean_value(v) for k, v in row.to_dict().items()}\n",
    "        relationships.append({\n",
    "            \"start\": row[\"source\"],\n",
    "            \"end\": row[\"target\"],\n",
    "            \"properties\": cleaned_properties,\n",
    "        })\n",
    "    return relationships\n",
    "\n",
    "\n",
    "w = GraphWidget()\n",
    "w.directed = True\n",
    "w.nodes = convert_entities_to_dicts(entity_df)\n",
    "w.edges = convert_relationships_to_dicts(relationship_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure data-driven visualization\n",
    "\n",
    "The additional properties can be used to configure the visualization for different use cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show title on the node\n",
    "w.node_label_mapping = \"title\"\n",
    "\n",
    "\n",
    "# map community to a color\n",
    "def community_to_color(community):\n",
    "    \"\"\"Map a community to a color.\"\"\"\n",
    "    colors = [\n",
    "        \"crimson\",\n",
    "        \"darkorange\",\n",
    "        \"indigo\",\n",
    "        \"cornflowerblue\",\n",
    "        \"cyan\",\n",
    "        \"teal\",\n",
    "        \"green\",\n",
    "    ]\n",
    "    return (\n",
    "        colors[int(community) % len(colors)] if community is not None else \"lightgray\"\n",
    "    )\n",
    "\n",
    "\n",
    "def edge_to_source_community(edge):\n",
    "    \"\"\"Get the community of the source node of an edge.\"\"\"\n",
    "    source_node = next(\n",
    "        (entry for entry in w.nodes if entry[\"properties\"][\"title\"] == edge[\"start\"]),\n",
    "        None,\n",
    "    )\n",
    "    if source_node is None:\n",
    "        return None\n",
    "    # Handle missing community property gracefully\n",
    "    source_node_community = source_node[\"properties\"].get(\"community\", None)\n",
    "    return source_node_community if source_node_community is not None else None\n",
    "\n",
    "\n",
    "w.node_color_mapping = lambda node: community_to_color(node[\"properties\"].get(\"community\", None))\n",
    "w.edge_color_mapping = lambda edge: community_to_color(edge_to_source_community(edge))\n",
    "# map size data to a reasonable factor\n",
    "w.node_scale_factor_mapping = lambda node: 0.5 + node[\"properties\"].get(\"size\", 0) * 1.5 / 20\n",
    "# use weight for edge thickness\n",
    "w.edge_thickness_factor_mapping = \"weight\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Automatic layouts\n",
    "\n",
    "The widget provides different automatic layouts that serve different purposes: `Circular`, `Hierarchic`, `Organic (interactiv or static)`, `Orthogonal`, `Radial`, `Tree`, `Geo-spatial`.\n",
    "\n",
    "For the knowledge graph, this sample uses the `Circular` layout, though `Hierarchic` or `Organic` are also suitable choices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the circular layout for this visualization. For larger graphs, the default organic layout is often preferrable.\n",
    "w.circular_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Display the graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13287011e0764a7d8db98b0a19ce32f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "GraphWidget(layout=Layout(height='800px', width='100%'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualizing the result context of `graphrag` queries\n",
    "\n",
    "The result context of `graphrag` queries allow to inspect the context graph of the request. This data can similarly be visualized as graph with `yfiles-jupyter-graphs`.\n",
    "\n",
    "## Making the request\n",
    "\n",
    "The following cell recreates the sample queries from [local_search.ipynb](../../local_search.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Covariate file not found, proceeding without covariates\n"
     ]
    }
   ],
   "source": [
    "# setup (see also ../../local_search.ipynb)\n",
    "entities = read_indexer_entities(entity_df, community_df, COMMUNITY_LEVEL)\n",
    "\n",
    "description_embedding_store = LanceDBVectorStore(\n",
    "    collection_name=\"default-entity-description\",\n",
    ")\n",
    "description_embedding_store.connect(db_uri=LANCEDB_URI)\n",
    "\n",
    "# Comment out covariates for now if file doesn't exist\n",
    "try:\n",
    "    covariate_df = pd.read_parquet(f\"{INPUT_DIR}/{COVARIATE_TABLE}.parquet\")\n",
    "    claims = read_indexer_covariates(covariate_df)\n",
    "    covariates = {\"claims\": claims}\n",
    "except FileNotFoundError:\n",
    "    print(\"Covariate file not found, proceeding without covariates\")\n",
    "    covariates = {}\n",
    "\n",
    "report_df = pd.read_parquet(f\"{INPUT_DIR}/{COMMUNITY_REPORT_TABLE}.parquet\")\n",
    "reports = read_indexer_reports(report_df, community_df, COMMUNITY_LEVEL)\n",
    "text_unit_df = pd.read_parquet(f\"{INPUT_DIR}/{TEXT_UNIT_TABLE}.parquet\")\n",
    "text_units = read_indexer_text_units(text_unit_df)\n",
    "\n",
    "# Load configuration from settings.yaml\n",
    "from graphrag.config.load_config import load_config\n",
    "from pathlib import Path\n",
    "\n",
    "config_path = Path(\"/home/chuaxu/projects/graphrag/ragsas\")\n",
    "config = load_config(config_path)\n",
    "\n",
    "# Get model configurations from the loaded config\n",
    "chat_model_config = config.get_language_model_config(\"default_chat_model\")\n",
    "embedding_model_config = config.get_language_model_config(\"default_embedding_model\")\n",
    "\n",
    "chat_model = ModelManager().get_or_create_chat_model(\n",
    "    name=\"local_search\",\n",
    "    model_type=chat_model_config.type,\n",
    "    config=chat_model_config,\n",
    ")\n",
    "\n",
    "token_encoder = tiktoken.encoding_for_model(chat_model_config.model)\n",
    "\n",
    "text_embedder = ModelManager().get_or_create_embedding_model(\n",
    "    name=\"local_search_embedding\",\n",
    "    model_type=embedding_model_config.type,\n",
    "    config=embedding_model_config,\n",
    ")\n",
    "\n",
    "context_builder = LocalSearchMixedContext(\n",
    "    community_reports=reports,\n",
    "    text_units=text_units,\n",
    "    entities=entities,\n",
    "    relationships=relationships,\n",
    "    covariates=covariates,\n",
    "    entity_text_embeddings=description_embedding_store,\n",
    "    embedding_vectorstore_key=EntityVectorStoreKey.ID,  # if the vectorstore uses entity title as ids, set this to EntityVectorStoreKey.TITLE\n",
    "    text_embedder=text_embedder,\n",
    "    token_encoder=token_encoder,\n",
    ")\n",
    "\n",
    "local_context_params = {\n",
    "    \"text_unit_prop\": 0.5,\n",
    "    \"community_prop\": 0.1,\n",
    "    \"conversation_history_max_turns\": 5,\n",
    "    \"conversation_history_user_turns_only\": True,\n",
    "    \"top_k_mapped_entities\": 10,\n",
    "    \"top_k_relationships\": 10,\n",
    "    \"include_entity_rank\": True,\n",
    "    \"include_relationship_weight\": True,\n",
    "    \"include_community_rank\": False,\n",
    "    \"return_candidate_context\": False,\n",
    "    \"embedding_vectorstore_key\": EntityVectorStoreKey.ID,  # set this to EntityVectorStoreKey.TITLE if the vectorstore uses entity title as ids\n",
    "    \"max_tokens\": 80_000,  # change this based on the token limit you have on your model (if you are using a model with 8k limit, a good setting could be 5000)\n",
    "}\n",
    "\n",
    "model_params = {\n",
    "    \"max_tokens\": 16_000,  # change this based on the token limit you have on your model (if you are using a model with 8k limit, a good setting could be 1000=1500, the model supports at most 16384 completion tokens)\n",
    "    \"temperature\": 0.0,\n",
    "}\n",
    "\n",
    "search_engine = LocalSearch(\n",
    "    model=chat_model,\n",
    "    context_builder=context_builder,\n",
    "    token_encoder=token_encoder,\n",
    "    model_params=model_params,\n",
    "    context_builder_params=local_context_params,\n",
    "    response_type=\"multiple paragraphs\",  # free form text describing the response type and format, can be anything, e.g. prioritized list, single paragraph, multiple paragraphs, multiple-page report\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run local search on sample queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'm sorry, but I don't have any information about Agent Mercer in the provided data tables. If you have any other questions or need information on a different topic, feel free to ask!\n"
     ]
    }
   ],
   "source": [
    "result = await search_engine.search(\"Tell me about Agent Mercer\")\n",
    "print(result.response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-15 16:58:27.0577 - WARNING - graphrag.query.structured_search.local_search.mixed_context - Reached token limit - reverting to previous context state\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "# Impact of Pricing Strategies on Business Revenue\n",
       "\n",
       "Pricing strategies are crucial for businesses aiming to optimize revenue and enhance customer engagement. The Online Media Company, specializing in music subscription services, provides a compelling case study of how personalized pricing strategies can significantly impact business revenue. By leveraging advanced analytical tools such as SAS's DEEPPRICE procedure, the company tailors its pricing plans to align with individual user profiles, thereby maximizing profitability [Data: Reports (14); Entities (1729, 1762, 1745, 1754); Relationships (1968, 1992)].\n",
       "\n",
       "## Personalized Pricing Plans\n",
       "\n",
       "The Online Media Company employs personalized pricing plans to cater to the unique consumption patterns and willingness to pay of each subscriber. This approach involves analyzing user behavior and characteristics to set user-specific prices, which are adjusted based on the price elasticity of demand. The DEEPPRICE procedure plays a pivotal role in this process by estimating demand curves and accounting for heterogeneous price effects based on user characteristics. This enables the company to offer pricing plans that reflect the individual needs and behaviors of its users, ultimately driving revenue growth [Data: Entities (1729, 1762, 1745); Relationships (1992); Sources (438, 413)].\n",
       "\n",
       "## Policy Evaluation and Comparison\n",
       "\n",
       "The company conducts policy evaluation and comparison to identify the most effective pricing strategies. For instance, policies such as S1, which offers discounts to all users, and S2, which targets price-sensitive users, are compared based on their revenue generation capabilities. Output 16.1.7 highlights that policy S1 generates the most revenue, indicating the effectiveness of personalized pricing strategies in maximizing business profitability. Additionally, the company considers offering personalized discounts during the discount season, further enhancing its ability to attract and retain subscribers [Data: Entities (1784, 1778, 1740, 1741); Relationships (1967, 1980); Sources (438)].\n",
       "\n",
       "## Analytical Tools and Data Integration\n",
       "\n",
       "The integration of SAS analytical tools, such as DEEPPRICE and MYLIB, into the company's operations underscores the importance of data-driven decision-making in optimizing pricing strategies. The PRICING_SAMPLE dataset, comprising 10,000 simulated observations, is utilized to analyze personalized discount policies, providing valuable insights into customer behavior and business outcomes. This data-driven approach ensures that the company remains responsive to the evolving needs and preferences of its subscribers, allowing it to maintain a competitive edge in the rapidly changing online media landscape [Data: Reports (14); Entities (1733); Relationships (1911); Sources (413, 436)].\n",
       "\n",
       "In conclusion, the strategic use of personalized pricing plans and advanced analytical tools enables businesses like the Online Media Company to optimize their pricing strategies, thereby enhancing revenue and customer engagement. By tailoring prices to individual user profiles and conducting thorough policy evaluations, the company effectively balances competitive pricing with the goal of maximizing profitability."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import Markdown, display\n",
    "\n",
    "question = \"How do different pricing strategies impact the business revenue?\"\n",
    "# question = \"Which published studies in our knowledge base used both panel data methods and cointegration analysis on emerging market economies?\"\n",
    "result = await search_engine.search(question)\n",
    "\n",
    "# Display as formatted Markdown instead of plain text\n",
    "display(Markdown(result.response))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspecting the context data used to generate the response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>entity</th>\n",
       "      <th>description</th>\n",
       "      <th>number of relationships</th>\n",
       "      <th>in_context</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1776</td>\n",
       "      <td>POLICY S1</td>\n",
       "      <td>An optimal pricing policy that sets user-speci...</td>\n",
       "      <td>2</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1729</td>\n",
       "      <td>ONLINE MEDIA COMPANY</td>\n",
       "      <td>The ONLINE MEDIA COMPANY is a dynamic organiza...</td>\n",
       "      <td>12</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1774</td>\n",
       "      <td>POLICY OPTIMIZATION</td>\n",
       "      <td>The goal of maximizing the expected utility of...</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1745</td>\n",
       "      <td>DEEPPRICE PROCEDURE</td>\n",
       "      <td>The DEEPPRICE PROCEDURE is a comprehensive met...</td>\n",
       "      <td>8</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1785</td>\n",
       "      <td>OUTPUT 16.1.8</td>\n",
       "      <td>Output showing policy comparison based on reve...</td>\n",
       "      <td>4</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     id                entity  \\\n",
       "0  1776             POLICY S1   \n",
       "1  1729  ONLINE MEDIA COMPANY   \n",
       "2  1774   POLICY OPTIMIZATION   \n",
       "3  1745   DEEPPRICE PROCEDURE   \n",
       "4  1785         OUTPUT 16.1.8   \n",
       "\n",
       "                                         description number of relationships  \\\n",
       "0  An optimal pricing policy that sets user-speci...                       2   \n",
       "1  The ONLINE MEDIA COMPANY is a dynamic organiza...                      12   \n",
       "2  The goal of maximizing the expected utility of...                       1   \n",
       "3  The DEEPPRICE PROCEDURE is a comprehensive met...                       8   \n",
       "4  Output showing policy comparison based on reve...                       4   \n",
       "\n",
       "   in_context  \n",
       "0        True  \n",
       "1        True  \n",
       "2        True  \n",
       "3        True  \n",
       "4        True  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.context_data[\"entities\"].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>source</th>\n",
       "      <th>target</th>\n",
       "      <th>description</th>\n",
       "      <th>weight</th>\n",
       "      <th>links</th>\n",
       "      <th>in_context</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1968</td>\n",
       "      <td>ONLINE MEDIA COMPANY</td>\n",
       "      <td>POLICY OPTIMIZATION</td>\n",
       "      <td>The online media company aims to optimize pric...</td>\n",
       "      <td>8.0</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1967</td>\n",
       "      <td>ONLINE MEDIA COMPANY</td>\n",
       "      <td>POLICY EVALUATION AND COMPARISON</td>\n",
       "      <td>The online media company is conducting policy ...</td>\n",
       "      <td>8.0</td>\n",
       "      <td>2</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1923</td>\n",
       "      <td>DEEPPRICE PROCEDURE</td>\n",
       "      <td>POLICY EVALUATION AND COMPARISON</td>\n",
       "      <td>The DEEPPRICE procedure is related to the poli...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1992</td>\n",
       "      <td>PROC DEEPPRICE</td>\n",
       "      <td>ONLINE MEDIA COMPANY</td>\n",
       "      <td>The online media company uses PROC DEEPPRICE t...</td>\n",
       "      <td>8.0</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1924</td>\n",
       "      <td>DEEPPRICE PROCEDURE</td>\n",
       "      <td>SAS CLOUD ANALYTIC SERVICES</td>\n",
       "      <td>The DEEPPRICE procedure requires SAS Cloud Ana...</td>\n",
       "      <td>8.0</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     id                source                            target  \\\n",
       "0  1968  ONLINE MEDIA COMPANY               POLICY OPTIMIZATION   \n",
       "1  1967  ONLINE MEDIA COMPANY  POLICY EVALUATION AND COMPARISON   \n",
       "2  1923   DEEPPRICE PROCEDURE  POLICY EVALUATION AND COMPARISON   \n",
       "3  1992        PROC DEEPPRICE              ONLINE MEDIA COMPANY   \n",
       "4  1924   DEEPPRICE PROCEDURE       SAS CLOUD ANALYTIC SERVICES   \n",
       "\n",
       "                                         description weight links  in_context  \n",
       "0  The online media company aims to optimize pric...    8.0     1        True  \n",
       "1  The online media company is conducting policy ...    8.0     2        True  \n",
       "2  The DEEPPRICE procedure is related to the poli...    1.0     2        True  \n",
       "3  The online media company uses PROC DEEPPRICE t...    8.0     1        True  \n",
       "4  The DEEPPRICE procedure requires SAS Cloud Ana...    8.0     1        True  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.context_data[\"relationships\"].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing the result context as graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "deb058d7b2e6431b8b339292abd72dab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "GraphWidget(layout=Layout(height='700px', width='100%'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Helper function to visualize the result context with `yfiles-jupyter-graphs`.\n",
    "\n",
    "The dataframes are converted into supported nodes and relationships lists and then passed to yfiles-jupyter-graphs.\n",
    "Additionally, some values are mapped to visualization properties.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def show_graph(result):\n",
    "    \"\"\"Visualize the result context with yfiles-jupyter-graphs.\"\"\"\n",
    "    from yfiles_jupyter_graphs import GraphWidget\n",
    "\n",
    "    if (\n",
    "        \"entities\" not in result.context_data\n",
    "        or \"relationships\" not in result.context_data\n",
    "    ):\n",
    "        msg = \"The passed results do not contain 'entities' or 'relationships'\"\n",
    "        raise ValueError(msg)\n",
    "\n",
    "    # converts the entities dataframe to a list of dicts for yfiles-jupyter-graphs\n",
    "    def convert_entities_to_dicts(df):\n",
    "        \"\"\"Convert the entities dataframe to a list of dicts for yfiles-jupyter-graphs.\"\"\"\n",
    "        nodes_dict = {}\n",
    "        for _, row in df.iterrows():\n",
    "            # Create a dictionary for each row and collect unique nodes\n",
    "            node_id = row[\"entity\"]\n",
    "            if node_id not in nodes_dict:\n",
    "                nodes_dict[node_id] = {\n",
    "                    \"id\": node_id,\n",
    "                    \"properties\": row.to_dict(),\n",
    "                }\n",
    "        return list(nodes_dict.values())\n",
    "\n",
    "    # converts the relationships dataframe to a list of dicts for yfiles-jupyter-graphs\n",
    "    def convert_relationships_to_dicts(df):\n",
    "        \"\"\"Convert the relationships dataframe to a list of dicts for yfiles-jupyter-graphs.\"\"\"\n",
    "        relationships = []\n",
    "        for _, row in df.iterrows():\n",
    "            # Create a dictionary for each row\n",
    "            relationships.append({\n",
    "                \"start\": row[\"source\"],\n",
    "                \"end\": row[\"target\"],\n",
    "                \"properties\": row.to_dict(),\n",
    "            })\n",
    "        return relationships\n",
    "\n",
    "    w = GraphWidget()\n",
    "    # use the converted data to visualize the graph\n",
    "    w.nodes = convert_entities_to_dicts(result.context_data[\"entities\"])\n",
    "    w.edges = convert_relationships_to_dicts(result.context_data[\"relationships\"])\n",
    "    w.directed = True\n",
    "    # show title on the node\n",
    "    w.node_label_mapping = \"entity\"\n",
    "    # use weight for edge thickness\n",
    "    w.edge_thickness_factor_mapping = \"weight\"\n",
    "    display(w)\n",
    "\n",
    "\n",
    "show_graph(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== CONTEXT DATA ANALYSIS ===\n",
      "\n",
      "📊 Entities in context: 20 entities\n",
      "   - Columns: ['id', 'entity', 'description', 'number of relationships', 'in_context']\n",
      "   - Sample entity: POLICY S1\n",
      "📊 Relationships in context: 26 relationships\n",
      "   - Columns: ['id', 'source', 'target', 'description', 'weight', 'links', 'in_context']\n",
      "   - Sample relationship: ONLINE MEDIA COMPANY -> POLICY OPTIMIZATION\n",
      "📊 Community reports in context: 1 reports\n",
      "📊 Text sources in context: 3 text units\n",
      "\n",
      "=== TOKEN ANALYSIS ===\n",
      "\n",
      "🔢 Token Breakdown:\n",
      "   - System prompt: 604 tokens\n",
      "   - Context data: 7,447 tokens\n",
      "   - User question: 10 tokens\n",
      "   - Total INPUT: 8,061 tokens\n",
      "   - Response: 592 tokens\n",
      "   - TOTAL MESSAGE: 8,653 tokens\n",
      "\n",
      "📏 Model Capacity:\n",
      "   - Model limit: 128,000 tokens\n",
      "   - Used: 8,653 tokens (6.8%)\n",
      "   - Remaining: 119,347 tokens\n",
      "   ✅ Token count is within safe limits\n",
      "\n",
      "================================================================================\n",
      "=== FULL CONTEXT SENT TO LLM ===\n",
      "================================================================================\n",
      "\n",
      "� SYSTEM PROMPT (Skipped)\n",
      "----------------------------------------\n",
      "\n",
      "🔵 USER QUESTION:\n",
      "----------------------------------------\n",
      "How do different pricing strategies impact the business revenue?\n",
      "\n",
      "🔵 CONTEXT DATA (7,447 tokens):\n",
      "----------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Relevant Entities:\n",
       "\n",
       "**POLICY S1** (Rank: N/A)\n",
       "Description: An optimal pricing policy that sets user-specific prices to maximize revenue\n",
       "\n",
       "**ONLINE MEDIA COMPANY** (Rank: N/A)\n",
       "Description: The ONLINE MEDIA COMPANY is a dynamic organization specializing in music subscription services, leveraging advanced pricing strategies to enhance user engagement and maximize revenue. This company offers a comprehensive music subscription service, providing users with access to a vast library of music tailored to their preferences. To attract and retain subscribers, the company employs personalized discount policies and targeted discounts, ensuring that users receive offers that are most relevant to their listening habits and preferences.\n",
       "\n",
       "A key component of the company's strategy is its innovative use of personalized pricing plans. By analyzing user behavior and characteristics, the company optimizes its pricing policies to align with the price elasticity of demand. This approach allows the company to adjust prices based on individual user profiles, ensuring that each subscriber receives a pricing plan that reflects their unique consumption patterns and willingness to pay.\n",
       "\n",
       "The company utilizes a sophisticated pricing tool known as PROC DEEPPRICE, which is instrumental in setting user-specific prices. This tool enables the company to achieve maximum revenue by tailoring prices to the specific needs and behaviors of its users. Through this method, the company can effectively balance the need for competitive pricing with the goal of maximizing profitability.\n",
       "\n",
       "In addition to its focus on pricing, the ONLINE MEDIA COMPANY is committed to understanding and analyzing user behavior. By collecting and interpreting data on how users interact with its services, the company can refine its offerings and improve the overall user experience. This data-driven approach ensures that the company remains responsive to the evolving needs and preferences of its subscribers, allowing it to maintain a competitive edge in the rapidly changing online media landscape.\n",
       "\n",
       "Overall, the ONLINE MEDIA COMPANY stands out as a leader in the online media industry, combining cutting-edge technology with strategic pricing and user analysis to deliver a superior music subscription service. Its commitment to personalized pricing and targeted discounts not only enhances user satisfaction but also drives revenue growth, positioning the company for continued success in the digital marketplace.\n",
       "\n",
       "**POLICY OPTIMIZATION** (Rank: N/A)\n",
       "Description: The goal of maximizing the expected utility of a pricing policy by setting user-specific prices\n",
       "\n",
       "**DEEPPRICE PROCEDURE** (Rank: N/A)\n",
       "Description: The DEEPPRICE PROCEDURE is a comprehensive method detailed in a chapter of a document that explores its various applications. This procedure is primarily utilized for creating tables and analyzing data, specifically tailored for personalized pricing plans. It serves as a robust tool in estimating parameters, which is crucial for understanding and implementing effective pricing strategies. Additionally, the DEEPPRICE PROCEDURE is instrumental in performing policy evaluation and comparison, particularly in scenarios where the treatment variable is not endogenous. This aspect of the procedure allows for a more accurate assessment of different pricing policies, ensuring that the most effective strategies are identified and implemented. Overall, the DEEPPRICE PROCEDURE is a versatile and essential method for businesses looking to optimize their pricing plans through detailed data analysis and policy evaluation.\n",
       "\n",
       "**OUTPUT 16.1.8** (Rank: N/A)\n",
       "Description: Output showing policy comparison based on revenue, indicating that personalized policies s1, s1d, s2, and s4 generate more revenue than the base policy\n",
       "\n",
       "**PROC DEEPPRICE** (Rank: N/A)\n",
       "Description: PROC DEEPPRICE is a versatile procedure designed for deep learning and policy evaluation, particularly in the context of analyzing price and demand data. It is equipped to handle continuous outcome variables by utilizing the identity function for G in the outcome model, which simplifies the process of modeling these types of variables. This procedure is instrumental in estimating demand curves, allowing users to specify the correct functional form to accurately capture the relationship between price and demand. Additionally, PROC DEEPPRICE is adept at estimating optimal revenue per user by accounting for heterogeneous price effects based on user characteristics, thereby enabling more personalized and effective pricing strategies.\n",
       "\n",
       "The procedure offers several options to enhance its functionality and adaptability. Users can specify the minibatch size, which is crucial for managing computational resources and optimizing the learning process in deep learning applications. Furthermore, PROC DEEPPRICE includes options for random seed generation, ensuring reproducibility and consistency in results across different runs. It also provides mechanisms for handling missing values, which is essential for maintaining the integrity and accuracy of the data analysis.\n",
       "\n",
       "In the context of price effect estimation, PROC DEEPPRICE saves the estimation details, facilitating a comprehensive analysis of how price changes impact demand. This feature is particularly useful for businesses and researchers aiming to understand and predict consumer behavior in response to pricing strategies. By integrating these capabilities, PROC DEEPPRICE serves as a powerful tool for those seeking to leverage data-driven insights to optimize pricing and maximize revenue.\n",
       "\n",
       "Overall, PROC DEEPPRICE stands out as a robust procedure that combines deep learning techniques with advanced policy evaluation methods to deliver precise and actionable insights into price and demand dynamics. Its ability to handle continuous outcome variables, estimate demand curves, and account for heterogeneous price effects makes it an invaluable asset for analysts and decision-makers in various industries.\n",
       "\n",
       "**POLICY S3** (Rank: N/A)\n",
       "Description: A pricing policy offering a discount to all users\n",
       "\n",
       "**PRICING_SAMPLE** (Rank: N/A)\n",
       "Description: The \"PRICING_SAMPLE\" is a dataset comprising 10,000 simulated observations that capture various customer characteristics and online behaviors. This dataset is specifically designed to facilitate the analysis of personalized discount policies, allowing researchers and analysts to explore how tailored pricing strategies can impact consumer behavior and business outcomes. The \"PRICING_SAMPLE\" serves as a valuable resource for understanding the dynamics of customer interactions and preferences in an online shopping environment.\n",
       "\n",
       "Additionally, the \"PRICING_SAMPLE\" is utilized within analytical sessions associated with the \"mylib\" libref, indicating its integration into broader data analysis frameworks and libraries. This integration suggests that the dataset is not only a standalone resource but also part of a larger ecosystem of data tools and libraries, enhancing its utility for comprehensive analysis and research purposes.\n",
       "\n",
       "Overall, the \"PRICING_SAMPLE\" provides a robust foundation for examining the implications of personalized pricing strategies, offering insights into customer behavior that can inform business decisions and marketing strategies. Its role within the \"mylib\" libref further underscores its importance in data-driven analysis, making it a critical component for those seeking to leverage data for strategic advantage in the realm of personalized discounts and customer engagement.\n",
       "\n",
       "**OUTPUT 16.1.7** (Rank: N/A)\n",
       "Description: Output showing policy evaluation based on revenue, highlighting the optimal personalized policy s1\n",
       "\n",
       "**TRADING STRATEGIES** (Rank: N/A)\n",
       "Description: Various strategies compared based on forecasted market states, such as buying in a bull market and selling in a bear market\n",
       "\n",
       "**DEEPPRICE** (Rank: N/A)\n",
       "Description: DEEPPRICE is a sophisticated procedure designed to facilitate various tasks within the realm of data analysis. It serves as a tool for model information storage, ensuring that complex data structures and their corresponding parameters are efficiently organized and accessible for further analysis. Additionally, DEEPPRICE plays a crucial role in parameter learning, which involves the process of adjusting model parameters to improve the accuracy and reliability of data predictions and insights.\n",
       "\n",
       "Beyond its foundational capabilities in data management and learning, DEEPPRICE is also employed to estimate patterns in user characteristics and pricing policies. This aspect of the procedure is particularly valuable for businesses and organizations seeking to understand consumer behavior and optimize their pricing strategies. By analyzing user data, DEEPPRICE can identify trends and correlations that inform more effective pricing decisions, ultimately enhancing the organization's ability to meet market demands and maximize profitability.\n",
       "\n",
       "In summary, DEEPPRICE is a versatile and powerful procedure that combines model information storage, parameter learning, and pattern estimation to provide comprehensive data analysis solutions. Its application in understanding user characteristics and pricing policies underscores its importance in strategic decision-making processes, making it an indispensable tool for entities aiming to leverage data for competitive advantage.\n",
       "\n",
       "**MARCH** (Rank: N/A)\n",
       "Description: March is a month mentioned in the context of varying price sensitivity\n",
       "\n",
       "**APRIL** (Rank: N/A)\n",
       "Description: April is a month mentioned in the context of varying price sensitivity\n",
       "\n",
       "**POLICY S2** (Rank: N/A)\n",
       "Description: A pricing policy offering discounts to users with income less than 1 and who visit the website less than five days a week\n",
       "\n",
       "**S1** (Rank: N/A)\n",
       "Description: Policy of offering a discount to everyone\n",
       "\n",
       "**S1DSTAR** (Rank: N/A)\n",
       "Description: S1DSTAR is an optimized discount policy that generates less revenue than S1 and S1STAR\n",
       "\n",
       "**MAY** (Rank: N/A)\n",
       "Description: May is a month mentioned in the context of varying price sensitivity\n",
       "\n",
       "**S1D** (Rank: N/A)\n",
       "Description: S1D is an optimized discount policy that generates less revenue than S1 and S1STAR\n",
       "\n",
       "**S2** (Rank: N/A)\n",
       "Description: S2 is a personalized policy designed to offer discounts specifically to price-sensitive users, thereby improving upon the base policy price. The optimal strategy employed by S2 involves offering discounts exclusively to individuals who exhibit positive Individual Treatment Effect (ITE) values. This approach ensures that discounts are targeted effectively, maximizing the benefit for both the users and the entity implementing the policy. By focusing on users who are most likely to respond positively to price adjustments, S2 enhances the overall efficiency and effectiveness of the discount offering process.\n",
       "\n",
       "**EXAMPLE 15.2** (Rank: N/A)\n",
       "Description: Example 15.2 illustrates a personalized discount policy for an online media company\n",
       "\n",
       "\n",
       "## Relevant Relationships:\n",
       "\n",
       "**ONLINE MEDIA COMPANY → POLICY OPTIMIZATION** (Weight: 8.0)\n",
       "Description: The online media company aims to optimize pricing policies to maximize revenue\n",
       "\n",
       "**ONLINE MEDIA COMPANY → POLICY EVALUATION AND COMPARISON** (Weight: 8.0)\n",
       "Description: The online media company is conducting policy evaluation and comparison to optimize pricing strategies\n",
       "\n",
       "**DEEPPRICE PROCEDURE → POLICY EVALUATION AND COMPARISON** (Weight: 1.0)\n",
       "Description: The DEEPPRICE procedure is related to the policy evaluation and comparison discussed in the document\n",
       "\n",
       "**PROC DEEPPRICE → ONLINE MEDIA COMPANY** (Weight: 8.0)\n",
       "Description: The online media company uses PROC DEEPPRICE to optimize pricing strategies\n",
       "\n",
       "**DEEPPRICE PROCEDURE → SAS CLOUD ANALYTIC SERVICES** (Weight: 8.0)\n",
       "Description: The DEEPPRICE procedure requires SAS Cloud Analytic Services to run\n",
       "\n",
       "**DEEPPRICE PROCEDURE → SAS VIYA** (Weight: 7.0)\n",
       "Description: The DEEPPRICE procedure is supported by SAS Viya's Deep Learning Programming Guide\n",
       "\n",
       "**DEEPPRICE PROCEDURE → ODS** (Weight: 1.0)\n",
       "Description: The DEEPPRICE procedure uses the Output Delivery System to create tables\n",
       "\n",
       "**DEEPPRICE PROCEDURE → DEEPCAUSAL PROCEDURE** (Weight: 1.0)\n",
       "Description: Both DEEPPRICE and DEEPCAUSAL procedures use deep neural networks for causal inference\n",
       "\n",
       "**ONLINE MEDIA COMPANY → US** (Weight: 1.0)\n",
       "Description: Some users of the online media company access its website from the US\n",
       "\n",
       "**ONLINE MEDIA COMPANY → PRICING_SAMPLE** (Weight: 7.0)\n",
       "Description: The PRICING_SAMPLE data set is used to analyze the personalized discount policy of the online media company\n",
       "\n",
       "**ONLINE MEDIA COMPANY → PRICING_SAMPLE.CSV** (Weight: 1.0)\n",
       "Description: The online media company uses the pricing_sample.csv dataset for personalized customer segmentation\n",
       "\n",
       "**ONLINE MEDIA COMPANY → OUTPUT 16.1.2** (Weight: 7.0)\n",
       "Description: Output 16.1.2 is used by the online media company to analyze user behavior\n",
       "\n",
       "**ONLINE MEDIA COMPANY → OUTPUT 16.1.3** (Weight: 7.0)\n",
       "Description: Output 16.1.3 is used by the online media company to analyze user behavior\n",
       "\n",
       "**MICROSOFT → ONLINE MEDIA COMPANY** (Weight: 5.0)\n",
       "Description: Microsoft provided data for the online media company to offer personalized pricing plans\n",
       "\n",
       "**EXAMPLE 15.2 → ONLINE MEDIA COMPANY** (Weight: 1.0)\n",
       "Description: Example 15.2 discusses the personalized discount policy of an online media company\n",
       "\n",
       "**ONLINE MEDIA COMPANY → MUSIC SUBSCRIPTION SERVICE** (Weight: 8.0)\n",
       "Description: The online media company offers the music subscription service to its customers\n",
       "\n",
       "**DEEPPRICE PROCEDURE → DNN** (Weight: 8.0)\n",
       "Description: The DEEPPRICE procedure uses DNNs for causal inference in a two-step semiparametric framework\n",
       "\n",
       "**ONLINE MEDIA COMPANY → DISCOUNT SEASON** (Weight: 7.0)\n",
       "Description: The online media company considers offering personalized discounts during the discount season\n",
       "\n",
       "**OUTPUT 16.1.8 → DGP** (Weight: 7.0)\n",
       "Description: Output 16.1.8 compares policies using true DGP revenues\n",
       "\n",
       "**DEEPPRICE PROCEDURE → CAUSAL INFERENCE** (Weight: 9.0)\n",
       "Description: The DEEPPRICE procedure is designed to perform causal inference using deep neural networks\n",
       "\n",
       "**DEEPPRICE PROCEDURE → OBSERVATIONAL STUDY** (Weight: 1.0)\n",
       "Description: The DEEPPRICE procedure treats experiments as observational studies to estimate causal effects\n",
       "\n",
       "**OUTPUT 16.1.8 → POLICY S3** (Weight: 6.0)\n",
       "Description: Policy s3 is worse than price according to Output 16.1.8\n",
       "\n",
       "**OUTPUT 16.1.7 → POLICY S1** (Weight: 8.0)\n",
       "Description: Policy s1 is highlighted in Output 16.1.7 as generating the most revenue\n",
       "\n",
       "**OUTPUT 16.1.8 → POLICY S5** (Weight: 6.0)\n",
       "Description: Policy s5 is the worst compared to price according to Output 16.1.8\n",
       "\n",
       "**OUTPUT 16.1.8 → POLICY S0** (Weight: 6.0)\n",
       "Description: Policy s0 is equivalent to price according to Output 16.1.8\n",
       "\n",
       "**POLICY S1 → POLICY S1D** (Weight: 9.0)\n",
       "Description: Policy s1d is a modified version of policy s1 that ensures prices do not exceed the original price\n",
       "\n",
       "\n",
       "## Relevant Community Reports:\n",
       "\n",
       "**SAS Analytical Tools and Online Media Company** (Rank: N/A)\n",
       "# SAS Analytical Tools and Online Media Company\n",
       "\n",
       "This community is centered around the use of SAS analytical tools, particularly the LIBNAME statement, DEEPPRICE, and MYLIB, in conjunction with an Online Media Company that leverages these tools for advanced data analysis and pricing strategies. The entities are interconnected through various data management and analysis processes, highlighting the integration of SAS capabilities in optimizing business operations and decision-making.\n",
       "\n",
       "## LIBNAME Statement's Role in Data Management\n",
       "\n",
       "The LIBNAME statement in SAS is a fundamental command used to assign library references to data sources, facilitating efficient data management and access within the SAS environment. It is integral to the SAS programming language, allowing users to define a libref, which acts as a shortcut or alias for a directory or data source. This capability is crucial for organizing and accessing data, particularly when dealing with large datasets that require distributed processing through SAS's Cloud Analytic Services (CAS) engine [Data: Entities (1681); Relationships (1858)].\n",
       "\n",
       "## DEEPPRICE's Analytical Capabilities\n",
       "\n",
       "DEEPPRICE is a sophisticated procedure designed to facilitate various tasks within data analysis, including model information storage, parameter learning, and pattern estimation. It is particularly valuable for businesses seeking to understand consumer behavior and optimize pricing strategies. By analyzing user data, DEEPPRICE can identify trends and correlations that inform more effective pricing decisions, enhancing the organization's ability to meet market demands and maximize profitability [Data: Entities (1754); Relationships (1927, 1934, 1936, 1937, 1938, 1939)].\n",
       "\n",
       "## MYLIB as a Central Data Repository\n",
       "\n",
       "MYLIB is a versatile library reference used within the SAS environment, specifically designed to facilitate data storage and management across various analytical and modeling processes. It connects to a CAS session, enabling efficient handling of data tables distributed across machine nodes. MYLIB serves as a central repository for storing a wide array of datasets, including those used in statistical analysis, modeling, and data processing tasks [Data: Entities (288); Relationships (248, 1958, 1964, 1965, 1966, 2003, 2397, 249, 2287, 2289, 3386, 492, 493, 494, 2266, 2267, 2268, 2269, 2270, 2271, 2272, 2273, 2274, 2275, 2276, 3178)].\n",
       "\n",
       "## Online Media Company's Pricing Strategies\n",
       "\n",
       "The Online Media Company is a dynamic organization specializing in music subscription services, leveraging advanced pricing strategies to enhance user engagement and maximize revenue. The company employs personalized discount policies and targeted discounts, ensuring that users receive offers most relevant to their listening habits and preferences. This approach allows the company to adjust prices based on individual user profiles, optimizing pricing policies to align with the price elasticity of demand [Data: Entities (1729); Relationships (1911, 1968, 1969)].\n",
       "\n",
       "## Integration of SAS Tools in Business Operations\n",
       "\n",
       "The integration of SAS tools such as DEEPPRICE and MYLIB into business operations is evident in the Online Media Company's use of these tools for data analysis and pricing strategy optimization. The company utilizes the PRICING_SAMPLE dataset to analyze personalized discount policies, demonstrating the practical application of SAS capabilities in real-world business scenarios. This integration underscores the importance of advanced analytical tools in driving data-driven decision-making and enhancing business performance [Data: Entities (1733); Relationships (1911, 1970, 1971)].\n",
       "\n",
       "\n",
       "## Relevant Text Sources:\n",
       "\n",
       "**Source 438** (Rank: N/A)\n",
       "The estimated price elasticities can then be plotted against various characteristics of users. The following statements plot the relationship between elasticity and income, which is shown in Output 16.1.2: \n",
       "proc sort data=odetails ; \n",
       "by income; \n",
       "run; \n",
       "proc sgplot data=odetails ; \n",
       "series x=income y=elasticity /legendlabel='Sales elasticity prediction'; \n",
       "xaxis label='Income' values=(0 to 6 by 1); \n",
       "yaxis label='Song sales elasticity'; \n",
       "run; \n",
       "Example 16.1: Personalized Customer Segmentation for an Online Media Company F 991 \n",
       "\n",
       "Output 16.1.2 shows a remarkable heterogeneity among users with respect to income for their response to a price increase. Users whose income is less than 1 (51% of all users) are more sensitive to a price increase; if price goes up by 1%, their number of songs purchased falls by 1% to 5%, compared to around 0.15% for higher-income users. \n",
       "The following statements plot the relationship between elasticity, income, and days_visited, which is shown in Output 16.1.3: \n",
       "proc sort data=odetails ; \n",
       "by days_visited income; run; proc sgpanel data=odetails; \n",
       "panelby days_visited/layout=panel columns=4 rows=2 uniscale=column; series x=income y=elasticity; colaxis label='Income' valueshint values=(0 1 2 3 4 5 6); rowaxis label='Song sales elasticity' grid; \n",
       "run; \n",
       "\n",
       "Output 16.1.3 further shows that low-income users who visited the website less often in the past (less than fve days a week) are even more price-sensitive than low-income users who visited the website more often. The discussion next turns to policy evaluation and comparison. \n",
       "In policy optimization, the goal is to maximize the expected utility of a policy. For the defnition of the expected utility function and how it is estimated, as well as details such as the defnition of a policy rule, see the section Policy Evaluation and Comparison on page 984. In this example, the policy decisions are to choose user-specifc prices (pricei) in order to maximize the revenue, which is defned as \n",
       "Revenue.pricei;xi/D ...xi/C ..xi/\u0003 pricei/\u0003 pricei \n",
       "The optimal price for each user is therefore derived as price\u0003 D. ..xi / . Let s1 be the optimal policy that \n",
       "i 2..xi / \n",
       "sets each users price as price\u0003 . This optimal policy is highly personalized, because it sets user-specifc prices \n",
       "i \n",
       "during the discount season, but those prices can exceed the original prices. It is therefore more meaningful to consider an optimized discount personalized policy, s1d, that is equal to s1 if s1 does not exceed the original price, and equal to 1 otherwise. \n",
       "Although the optimal pricing policy s1 guarantees the maximum revenue and is highly personalized, the online media company might want to consider a price discount that targets a group of users or all users. Whether the companys revenue will rise or fall depends on the price elasticity of songs demand. Microeconomic theory predicts that lowering prices will increase revenue if demand is price-elastic (elasticity < 1) and decrease revenue if demand is price-inelastic (elasticity > 1). \n",
       "Output 16.1.2 and Output 16.1.3 show that price elasticity is heterogeneous among users; the most price-sensitive users are those whose income is less than 1 and who spend on average less than fve days a week on the website. This suggests a policy that offers this type of user a price discount. Let s2 be this policy. For comparison, also consider policies s3 (offer everyone a discount) and s0 (offer no one a discount). \n",
       "To select the discount rates, revenues were computed for policies s2 and s3 and for each of the six discount values 5%, 10%, 15%, 20%, 25%, and 30%. For policy s2, a 5% discount did not improve the base policy price, and although a 10%, 15%, 20%, 25%, or 30% discount generated more revenue than the base policy, there was no signifcant difference among the fve discount rates. A 10% discount was therefore selected for policy s2. Using a similar analysis, a 5% discount was selected for policy s3. \n",
       "Policies s2 and s3 offer the same discount to all users or groups of users. Versions of these policies (s4 and s5) that randomly select with equal probability a discount rate from the set (5%, 10%, 15%, 20%, 25%, and 30%) are also considered. \n",
       "The following DATA step creates policies s0, s1, s1d, s2, s3, s4, and s5 in the data table odetails, which also contains the variable elasticity that was previously computed: \n",
       "data mylib.discountPolicy; \n",
       "array prob[6] _temporary_ (6*0.1666667); \n",
       "call streaminit(54321); \n",
       "set odetails; \n",
       "s0 =1; \n",
       "s1 = -_alpha_/(2*_beta_); \n",
       "ifs1>1thens1d =1; \n",
       "else s1d = s1; \n",
       "if (income < 1 and days_visited <5) then s2 = 0.9; \n",
       "else s2=1; \n",
       "s3 = 0.95; \n",
       "if (income < 1 and days_visited <5) then s4\n",
       "\n",
       "**Source 413** (Rank: N/A)\n",
       "% level. \n",
       "\n",
       "For more information about the estimates of parameters of interest, policy evaluation, and policy comparison, see the sections Full-Population Average Effect Parameters on page 928 and Subpopulation Average Effect Parameters on page 930. \n",
       "Example 15.2: Personalized Discount Policy for an Online Media Company \n",
       "This example illustrates how a music subscription service from an online media company can offer targeted discounts through a personalized pricing plan based on many features that it observes about its customers to encourage them to buy more songs or beco me members. The main goal is to construct a policy that raises demand enough to boost overall revenue despite decreasing the price for some customers. \n",
       "The data set is provided by the Microsoft research project ALICE and is available at https:// \n",
       "msalicedatapublic.z5.web.core.windows.net/datasets/Pricing/pricing_sample. \n",
       "csv. The data set has 10,000 simulated observations that represent customers personal characteristics, such \n",
       "as age and log income, and online behavior history, such as previous purchase and previous online time per \n",
       "week. The treatment variable, t, is a binary variable that indicates whether or not a discount is applied. This \n",
       "variable is generated according to the values that the variable price takes in the data set. The value of t is 0 if \n",
       "the value of price is 1, indicating that no discount is given, and the value of t is 1 if the value of price is less \n",
       "than 1, indicating that a discount is applied. The outcome variable, revenue, is calculated by multiplying the \n",
       "number of songs purchased during the discount season by the price paid for the songs. Table 15.3 shows \n",
       "the names of the variables that are used in the model, their types, and their defnitions. The type can be T \n",
       "(treatment), Y (outcome), x.1/ (a covariate in the propensity score model), and/or x.2/ (a covariate in the \n",
       "outcome model). \n",
       "Table 15.3 Model Variables \n",
       "Name Type Details \n",
       "x.1/, x.2/\n",
       "account_age Users account age x.1/, x.2/\n",
       "age Users age x.1/, x.2/\n",
       "avg_hours Average number of hours user was online per week in the past x.1/, x.2/\n",
       "days_visited Average number of days user visited website per week in the past x.1/, x.2/\n",
       "friend_count Number of friends user connected to in account x.1/, x.2/\n",
       "has_membership Whether user has membership x.1/, x.2/\n",
       "is_US Whether user accesses website from US x.1/, x.2/\n",
       "songs_purchased Average number of songs user purchased per week in the past x.1/, x.2/\n",
       "income Users income t T Whether a discount is applied revenue Y Number of songs purchased during discount season times price paid \n",
       "Assuming that you have downloaded the data set pricing_sample in your session that is associated with the mylib libref, the following statements create an ID variable that has a unique value for each observation, the treatment variable (t), and the outcome variable (revenue) in the data table new_pricing_sample: \n",
       "data mylib.new_pricing_sample; \n",
       "set mylib.pricing_sample; id = put(_threadid_,8.) || '_' || Put(_n_,8.); * ID variable; if price<1 then t=1; else t=0; * treatment variable; revenue=price*demand; * outcome variable; \n",
       "run; \n",
       "The frst step in policy evaluation and policy optimization is to estimate the effect of the treatment and to save to a specifed output data table the details of the estimation, including ..\u0001/, ..\u0001/, p.\u0001/, the residual, and the infuence functions for each unit. You can do this by using the following statements: \n",
       "/*---Estimate the treatment effect and save the estimation details ---*/ \n",
       "proc deepcausal data=mylib.new_pricing_sample; id id; psmodel t = account_age age avg_hours days_visited friends_count \n",
       "has_membership is_US songs_purchased income / dnn=(nodes=(32 32 32 32) train=(optimizer=(miniBatchSize=500 regL1=0.0001 maxEpochs=32000 algorithm=adam) nthreads=20 seed=12345 recordseed=67890)); \n",
       "model revenue = account_age age avg_hours days_visited friends_count has_membership is_US songs_purchased income / dnn=(nodes=(32 32 32 32) train=(optimizer=(miniBatchSize=500 regL1=0.001 maxEpochs=32000 algorithm=adam) nthreads=20 seed=12345 recordseed=67890)); \n",
       "infer out=mylib.oest outdetails=mylib.odetails; run; \n",
       "For this example, the same covariates are used in both the propensity score mode and the outcome model. The model estimation details are saved in the output data table odetails. \n",
       "The estimation results are shown in Output 15.2.1. The estimate of the average treatment effect, ATE, is negative and statistically signifcant, suggesting that the discount, on average, causes revenue that is generated by the whole population to decrease. However, the effect of the discount on the revenue among the customers who received a discount, which is measured by the parameter ATT (average treatment effect on the treated), is considerably different from the ATE estimate. It might suggest that identifying the characteristics of customers to whom the discount matters would help construct the optimum policy so that it uses the fewest resources and earns the most proft. \n",
       "\n",
       "In policy optimization, the goal is to maximize the expected utility of a policy. For the defnition of the expected utility\n",
       "\n",
       "**Source 436** (Rank: N/A)\n",
       " variable Z \n",
       ".2/ .2/\n",
       "_epsy_ value of the residual in the outcome model, \" i D yi . ...O x /C ..O x /ti/when\n",
       "ii .2/ .2/\n",
       "there is no instrumental variable, and \" i D yi . ...O x /C ..O x /tOi/when there is \n",
       "ii .1/ .1/\n",
       "the instrumental variable Z, where tOi D \u0011O0.x /C \u0011O1.x /zi\n",
       "ii \n",
       "_eta0_ value of \u0011O0.x.1//for each observation. This variable is available only when there is \n",
       "i \n",
       "the instrumental variable Z. \n",
       "_eta1_ value of \u0011O1.x.1//for each observation. This variable is available only when there is \n",
       "i \n",
       "the instrumental variable Z. \n",
       "_alpha_ value of ..O x.2//for each observation \n",
       "i \n",
       "_beta_ value of ..x.2//for each observation \n",
       "i \n",
       "_alphatilde_ value of ..OQ x/for each observation. This variable is available only when there is the instrumental variable Z. \n",
       "_betatilde_ value of ..OQ x/for each observation. This variable is available only when there is the instrumental variable Z. .2/ .2/\n",
       "_if_alpha_ value of ..yi;xi;ti;..O x /;..O x /;.xi/;t\u0003/for each observation when there is \n",
       "ii OQ .1/ .1/\n",
       "no instrumental variable, and value of ..yi;xi;ti;zi;.OQi.xi/;.i.xi/;\u0011O0.x /;\u0011O1.x /;Z;t\u0003/\n",
       "ii \n",
       "for each observation when there is the instrumental variable Z \n",
       ".2/ .2/\n",
       "_if_beta_ value of ..yi;xi;ti;..O x /;..O x /;.xi/;t\u0003/for each observation when there is \n",
       "ii OQ .1/ .1/\n",
       "no instrumental variable, and value of ..yi;xi;ti;zi;.OQi.xi/;.i.xi/;\u0011O0.x /;\u0011O1.x /;Z;t\u0003/\n",
       "ii \n",
       "for each observation when there is the instrumental variable Z \n",
       ".2/ .2/\n",
       "_policy_policy-var_ value of \u0019.s/.yi;xi;ti;..O xi /;..O xi /;.xi/;t\u0003/, where s is the value of a policy variable that you specify in the POLICY= option in the INFER statement. The estimate is displayed for each policy variable. .2/ .2/\n",
       "_policy_comparison_policy-var_base-var_ value of \u0019.sc;sb/.yi;xi;ti;..O xi /;..O xi /;.xi/;t\u0003/, where sc is the value of a policy variable, POLICY-VAR, that you specify in the COMPARE= suboption of the POLICYCOMPARISON= option in the INFER statement and sb is the value of a policy variable, BASE-VAR, that you specify in the BASE= suboption of the POLICYCOMPARISON= option in the INFER statement. The estimate is displayed for each combination of sc and sb. \n",
       "\n",
       "\n",
       "ODS Table Names \n",
       "The DEEPPRICE procedure assigns a name to each table that it creates. You can use these names to refer to the tables when you use the Output Delivery System (ODS) to select tables and create output data tables. \n",
       "These names are listed in Table 16.2. \n",
       "Table 16.2 ODS Tables Produced in the DEEPPRICE Procedure \n",
       "ODS Table Name  Description  Option  \n",
       "NObs  Number of observations  Default  \n",
       "ParameterEstimates  Parameter estimates  Default  \n",
       "PolicyComparison  Policy comparison  POLICYCOMPARISON=  \n",
       "PolicyEvaluation  Policy evaluation  POLICY=  \n",
       "\n",
       "\n",
       "\n",
       "Examples: DEEPPRICE Procedure \n",
       "Example 16.1: Personalized Customer Segmentation for an Online Media Company \n",
       "This example illustrates how a music subscription service from an online media company can offer targeted \n",
       "discounts through a personalized pricing plan based on many features that the company observes about its \n",
       "customers to encourage them to buy more songs. \n",
       "The data set is provided by the Microsoft research project ALICE and is available at https://econmldata. \n",
       "azurewebsites.net/datasets/Pricing/pricing_sample.csv. The data set has 10,000 simu­\n",
       "lated observations that represent users personal characteristics, such as age, log income, and account age, \n",
       "as well as users online behavior history, including previous purchases and previous online time per week. \n",
       "The treatment variable, price, is the price that the customer was exposed to during the discount season. A \n",
       "value of 1 indicates that no discount is given, and a value less than 1 indicates that a discount is applied. The \n",
       "outcome variable, demand, is the number of songs that the customer purchased during the discount season. \n",
       "Table 16.3 shows the names of the variables that are used in the model, their types, and their defnitions. The \n",
       "type can be T (treatment), Y (outcome), x.1/ (a covariate in the treatment model), and/or x.2/ (a covariate in \n",
       "the outcome model). \n",
       "Table 16.3 Model Variables \n",
       "\n",
       "Name  Type  Details  \n",
       "account_age age avg_hours days_visited friend_count  x.1/, x.2/ x.1/, x.2/ x.1/, x.2/ x.1/, x.2/ x.1/, x.2/  User\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "=== END OF CONTEXT ===\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Analyze the context data and token usage that the LLM receives\n",
    "import tiktoken\n",
    "from IPython.display import Markdown, display\n",
    "\n",
    "def analyze_context_and_tokens(result, search_engine):\n",
    "    \"\"\"Analyze the context data and count tokens for different message components.\"\"\"\n",
    "    \n",
    "    # Get the token encoder\n",
    "    encoding = tiktoken.encoding_for_model(\"gpt-4\")\n",
    "    \n",
    "    print(\"=== CONTEXT DATA ANALYSIS ===\\n\")\n",
    "    \n",
    "    # Show context data statistics\n",
    "    if \"entities\" in result.context_data:\n",
    "        entities_df = result.context_data[\"entities\"]\n",
    "        print(f\"📊 Entities in context: {len(entities_df)} entities\")\n",
    "        print(f\"   - Columns: {list(entities_df.columns)}\")\n",
    "        print(f\"   - Sample entity: {entities_df.iloc[0]['entity'] if len(entities_df) > 0 else 'None'}\")\n",
    "    \n",
    "    if \"relationships\" in result.context_data:\n",
    "        relationships_df = result.context_data[\"relationships\"]\n",
    "        print(f\"📊 Relationships in context: {len(relationships_df)} relationships\")\n",
    "        print(f\"   - Columns: {list(relationships_df.columns)}\")\n",
    "        print(f\"   - Sample relationship: {relationships_df.iloc[0]['source']} -> {relationships_df.iloc[0]['target'] if len(relationships_df) > 0 else 'None'}\")\n",
    "    \n",
    "    if \"reports\" in result.context_data:\n",
    "        reports_df = result.context_data[\"reports\"]\n",
    "        print(f\"📊 Community reports in context: {len(reports_df)} reports\")\n",
    "    \n",
    "    if \"sources\" in result.context_data:\n",
    "        sources_df = result.context_data[\"sources\"]\n",
    "        print(f\"📊 Text sources in context: {len(sources_df)} text units\")\n",
    "    \n",
    "    print(\"\\n=== TOKEN ANALYSIS ===\\n\")\n",
    "    \n",
    "    # Reconstruct the context that was sent to the LLM\n",
    "    # This is an approximation of what the LocalSearch builds\n",
    "    \n",
    "    # 1. System prompt\n",
    "    with open(\"/home/chuaxu/projects/graphrag/ragsas/prompts/local_search_system_prompt.txt\", \"r\") as f:\n",
    "        system_prompt = f.read()\n",
    "\n",
    "    # 2. Context data formatting (full version of what LocalSearch does)\n",
    "    context_parts = []\n",
    "    \n",
    "    # Add entities context (FULL - no truncation)\n",
    "    if \"entities\" in result.context_data and len(result.context_data[\"entities\"]) > 0:\n",
    "        entities_context = \"## Relevant Entities:\\n\\n\"\n",
    "        for _, entity in result.context_data[\"entities\"].iterrows():  # Show ALL entities\n",
    "            desc = entity.get('description', 'No description')\n",
    "            rank = entity.get('rank', 'N/A')\n",
    "            entities_context += f\"**{entity['entity']}** (Rank: {rank})\\n\"\n",
    "            entities_context += f\"Description: {desc}\\n\\n\"\n",
    "        context_parts.append(entities_context)\n",
    "    \n",
    "    # Add relationships context (FULL - no truncation)\n",
    "    if \"relationships\" in result.context_data and len(result.context_data[\"relationships\"]) > 0:\n",
    "        relationships_context = \"## Relevant Relationships:\\n\\n\"\n",
    "        for _, rel in result.context_data[\"relationships\"].iterrows():  # Show ALL relationships\n",
    "            desc = rel.get('description', 'No description')\n",
    "            weight = rel.get('weight', 'N/A')\n",
    "            relationships_context += f\"**{rel['source']} → {rel['target']}** (Weight: {weight})\\n\"\n",
    "            relationships_context += f\"Description: {desc}\\n\\n\"\n",
    "        context_parts.append(relationships_context)\n",
    "    \n",
    "    # Add community reports context (FULL - no truncation)\n",
    "    if \"reports\" in result.context_data and len(result.context_data[\"reports\"]) > 0:\n",
    "        reports_context = \"## Relevant Community Reports:\\n\\n\"\n",
    "        for _, report in result.context_data[\"reports\"].iterrows():  # Show ALL reports\n",
    "            title = report.get('title', 'Untitled Report')\n",
    "            content = report.get('content', report.get('summary', 'No content'))\n",
    "            rank = report.get('rank', 'N/A')\n",
    "            reports_context += f\"**{title}** (Rank: {rank})\\n\"\n",
    "            reports_context += f\"{content}\\n\\n\"\n",
    "        context_parts.append(reports_context)\n",
    "    \n",
    "    # Add sources context (FULL - no truncation)\n",
    "    if \"sources\" in result.context_data and len(result.context_data[\"sources\"]) > 0:\n",
    "        sources_context = \"## Relevant Text Sources:\\n\\n\"\n",
    "        for _, source in result.context_data[\"sources\"].iterrows():  # Show ALL sources\n",
    "            text_content = source.get('text', source.get('content', 'No content'))\n",
    "            source_id = source.get('id', 'Unknown')\n",
    "            rank = source.get('rank', 'N/A')\n",
    "            sources_context += f\"**Source {source_id}** (Rank: {rank})\\n\"\n",
    "            sources_context += f\"{text_content}\\n\\n\"\n",
    "        context_parts.append(sources_context)\n",
    "    \n",
    "    # 3. User question\n",
    "    user_question = question  # From the previous cell\n",
    "    \n",
    "    # Combine all context\n",
    "    full_context = \"\\n\".join(context_parts)\n",
    "    \n",
    "    # 4. Calculate tokens for each component\n",
    "    system_prompt_tokens = len(encoding.encode(system_prompt))\n",
    "    context_tokens = len(encoding.encode(full_context))\n",
    "    user_question_tokens = len(encoding.encode(user_question))\n",
    "    response_tokens = len(encoding.encode(result.response))\n",
    "    \n",
    "    total_input_tokens = system_prompt_tokens + context_tokens + user_question_tokens\n",
    "    total_tokens = total_input_tokens + response_tokens\n",
    "    \n",
    "    print(f\"🔢 Token Breakdown:\")\n",
    "    print(f\"   - System prompt: {system_prompt_tokens:,} tokens\")\n",
    "    print(f\"   - Context data: {context_tokens:,} tokens\")\n",
    "    print(f\"   - User question: {user_question_tokens:,} tokens\")\n",
    "    print(f\"   - Total INPUT: {total_input_tokens:,} tokens\")\n",
    "    print(f\"   - Response: {response_tokens:,} tokens\")\n",
    "    print(f\"   - TOTAL MESSAGE: {total_tokens:,} tokens\")\n",
    "    \n",
    "    # Show model limits\n",
    "    model_limit = 128_000  # GPT-4o limit\n",
    "    print(f\"\\n📏 Model Capacity:\")\n",
    "    print(f\"   - Model limit: {model_limit:,} tokens\")\n",
    "    print(f\"   - Used: {total_tokens:,} tokens ({total_tokens/model_limit*100:.1f}%)\")\n",
    "    print(f\"   - Remaining: {model_limit - total_tokens:,} tokens\")\n",
    "    \n",
    "    if total_tokens > model_limit:\n",
    "        print(\"   ⚠️  WARNING: Token count exceeds model limit!\")\n",
    "    elif total_tokens > model_limit * 0.9:\n",
    "        print(\"   ⚠️  WARNING: Token count is near model limit!\")\n",
    "    else:\n",
    "        print(\"   ✅ Token count is within safe limits\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"=== FULL CONTEXT SENT TO LLM ===\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Display the complete context in a nice formatted way\n",
    "    print(f\"\\n� SYSTEM PROMPT (Skipped)\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    print(f\"\\n🔵 USER QUESTION:\")\n",
    "    print(\"-\" * 40)\n",
    "    print(user_question)\n",
    "    \n",
    "    print(f\"\\n🔵 CONTEXT DATA ({context_tokens:,} tokens):\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    # Display full context as Markdown for better formatting\n",
    "    display(Markdown(full_context))\n",
    "    \n",
    "    print(\"=\"*80)\n",
    "    print(\"=== END OF CONTEXT ===\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    return {\n",
    "        \"system_prompt_tokens\": system_prompt_tokens,\n",
    "        \"context_tokens\": context_tokens,\n",
    "        \"user_question_tokens\": user_question_tokens,\n",
    "        \"response_tokens\": response_tokens,\n",
    "        \"total_tokens\": total_tokens,\n",
    "        \"context_data\": result.context_data,\n",
    "        \"full_context\": full_context\n",
    "    }\n",
    "\n",
    "# Run the analysis\n",
    "token_analysis = analyze_context_and_tokens(result, search_engine)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Understanding Relationship Weights in GraphRAG\n",
    "\n",
    "In GraphRAG, the **weight** property on relationships represents the **strength** or **importance** of the connection between two entities. Here's what it means:\n",
    "\n",
    "## What is Weight?\n",
    "\n",
    "**Weight** is a numerical value that indicates:\n",
    "- **Frequency**: How often two entities appear together in the source documents\n",
    "- **Co-occurrence strength**: The statistical significance of their relationship\n",
    "- **Semantic closeness**: How tightly connected the entities are in the knowledge graph\n",
    "\n",
    "## How is Weight Calculated?\n",
    "\n",
    "The weight is typically derived from:\n",
    "1. **Text co-occurrence**: How many times the entities appear in the same text units/chunks\n",
    "2. **Window proximity**: How close the entities appear to each other in the text\n",
    "3. **Relationship strength**: The confidence level of the extracted relationship\n",
    "4. **Document frequency**: Across how many documents the relationship appears\n",
    "\n",
    "## Weight Values\n",
    "\n",
    "- **Higher weights** (e.g., 8.0, 10.0): Strong, frequently occurring relationships\n",
    "- **Lower weights** (e.g., 1.0, 2.0): Weaker or less frequent relationships\n",
    "- **Weight = 1.0**: Often the default/minimum weight for detected relationships\n",
    "\n",
    "## Usage in GraphRAG\n",
    "\n",
    "Weights are used for:\n",
    "- **Ranking relationships**: More important relationships get higher priority in search results\n",
    "- **Graph visualization**: Thicker edges represent stronger relationships (as seen in the yfiles visualization)\n",
    "- **Context selection**: Higher-weight relationships are more likely to be included in LLM context\n",
    "- **Graph algorithms**: Centrality and community detection algorithms use weights to identify key entities\n",
    "\n",
    "Let's examine the weights in our current result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== RELATIONSHIP WEIGHT ANALYSIS ===\n",
      "\n",
      "📊 Weight Statistics:\n",
      "   - Total relationships: 26\n",
      "   - Relationships with weights: 26\n",
      "   - Weight range: 1.00 to 9.00\n",
      "   - Average weight: 5.62\n",
      "   - Median weight: 7.00\n",
      "\n",
      "🔝 Top 5 Strongest Relationships (by weight):\n",
      "   DEEPPRICE PROCEDURE → CAUSAL INFERENCE (Weight: 9.00)\n",
      "      Description: The DEEPPRICE procedure is designed to perform causal inference using deep neural networks...\n",
      "\n",
      "   POLICY S1 → POLICY S1D (Weight: 9.00)\n",
      "      Description: Policy s1d is a modified version of policy s1 that ensures prices do not exceed the original price...\n",
      "\n",
      "   ONLINE MEDIA COMPANY → POLICY OPTIMIZATION (Weight: 8.00)\n",
      "      Description: The online media company aims to optimize pricing policies to maximize revenue...\n",
      "\n",
      "   ONLINE MEDIA COMPANY → POLICY EVALUATION AND COMPARISON (Weight: 8.00)\n",
      "      Description: The online media company is conducting policy evaluation and comparison to optimize pricing strategi...\n",
      "\n",
      "   PROC DEEPPRICE → ONLINE MEDIA COMPANY (Weight: 8.00)\n",
      "      Description: The online media company uses PROC DEEPPRICE to optimize pricing strategies...\n",
      "\n",
      "🔻 Bottom 5 Weakest Relationships (by weight):\n",
      "   DEEPPRICE PROCEDURE → POLICY EVALUATION AND COMPARISON (Weight: 1.00)\n",
      "      Description: The DEEPPRICE procedure is related to the policy evaluation and comparison discussed in the document...\n",
      "\n",
      "   DEEPPRICE PROCEDURE → ODS (Weight: 1.00)\n",
      "      Description: The DEEPPRICE procedure uses the Output Delivery System to create tables...\n",
      "\n",
      "   DEEPPRICE PROCEDURE → DEEPCAUSAL PROCEDURE (Weight: 1.00)\n",
      "      Description: Both DEEPPRICE and DEEPCAUSAL procedures use deep neural networks for causal inference...\n",
      "\n",
      "   ONLINE MEDIA COMPANY → US (Weight: 1.00)\n",
      "      Description: Some users of the online media company access its website from the US...\n",
      "\n",
      "   ONLINE MEDIA COMPANY → PRICING_SAMPLE.CSV (Weight: 1.00)\n",
      "      Description: The online media company uses the pricing_sample.csv dataset for personalized customer segmentation...\n",
      "\n",
      "📈 Weight Distribution:\n",
      "   - Weight 0-1: 0 relationships (0.0%)\n",
      "   - Weight 1-2: 7 relationships (26.9%)\n",
      "   - Weight 2-5: 0 relationships (0.0%)\n",
      "   - Weight 5-10: 19 relationships (73.1%)\n",
      "   - Weight 10+: 0 relationships (0.0%)\n"
     ]
    }
   ],
   "source": [
    "# Analyze relationship weights in our current search result\n",
    "print(\"=== RELATIONSHIP WEIGHT ANALYSIS ===\\n\")\n",
    "\n",
    "if \"relationships\" in result.context_data:\n",
    "    relationships_df = result.context_data[\"relationships\"]\n",
    "    \n",
    "    if 'weight' in relationships_df.columns:\n",
    "        # Convert weights to numeric, handling any string values\n",
    "        relationships_df['weight_numeric'] = pd.to_numeric(relationships_df['weight'], errors='coerce')\n",
    "        weights = relationships_df['weight_numeric'].dropna()\n",
    "        \n",
    "        if len(weights) > 0:\n",
    "            print(f\"📊 Weight Statistics:\")\n",
    "            print(f\"   - Total relationships: {len(relationships_df)}\")\n",
    "            print(f\"   - Relationships with weights: {len(weights)}\")\n",
    "            print(f\"   - Weight range: {weights.min():.2f} to {weights.max():.2f}\")\n",
    "            print(f\"   - Average weight: {weights.mean():.2f}\")\n",
    "            print(f\"   - Median weight: {weights.median():.2f}\")\n",
    "            \n",
    "            print(f\"\\n🔝 Top 5 Strongest Relationships (by weight):\")\n",
    "            top_relationships = relationships_df.nlargest(5, 'weight_numeric')[['source', 'target', 'weight_numeric', 'description']]\n",
    "            for idx, row in top_relationships.iterrows():\n",
    "                weight_val = row['weight_numeric']\n",
    "                if pd.notna(weight_val):\n",
    "                    print(f\"   {row['source']} → {row['target']} (Weight: {weight_val:.2f})\")\n",
    "                else:\n",
    "                    print(f\"   {row['source']} → {row['target']} (Weight: N/A)\")\n",
    "                desc = str(row['description'])[:100] if pd.notna(row['description']) else \"No description\"\n",
    "                print(f\"      Description: {desc}...\")\n",
    "                print()\n",
    "            \n",
    "            print(f\"🔻 Bottom 5 Weakest Relationships (by weight):\")\n",
    "            bottom_relationships = relationships_df.nsmallest(5, 'weight_numeric')[['source', 'target', 'weight_numeric', 'description']]\n",
    "            for idx, row in bottom_relationships.iterrows():\n",
    "                weight_val = row['weight_numeric']\n",
    "                if pd.notna(weight_val):\n",
    "                    print(f\"   {row['source']} → {row['target']} (Weight: {weight_val:.2f})\")\n",
    "                else:\n",
    "                    print(f\"   {row['source']} → {row['target']} (Weight: N/A)\")\n",
    "                desc = str(row['description'])[:100] if pd.notna(row['description']) else \"No description\"\n",
    "                print(f\"      Description: {desc}...\")\n",
    "                print()\n",
    "            \n",
    "            # Weight distribution\n",
    "            print(f\"📈 Weight Distribution:\")\n",
    "            weight_bins = [0, 1, 2, 5, 10, float('inf')]\n",
    "            weight_labels = ['0-1', '1-2', '2-5', '5-10', '10+']\n",
    "            \n",
    "            for i, (low, high) in enumerate(zip(weight_bins[:-1], weight_bins[1:])):\n",
    "                if high == float('inf'):\n",
    "                    count = len(weights[weights >= low])\n",
    "                    label = weight_labels[i]\n",
    "                else:\n",
    "                    count = len(weights[(weights >= low) & (weights < high)])\n",
    "                    label = weight_labels[i]\n",
    "                \n",
    "                percentage = count / len(weights) * 100 if len(weights) > 0 else 0\n",
    "                print(f\"   - Weight {label}: {count} relationships ({percentage:.1f}%)\")\n",
    "        else:\n",
    "            print(\"❌ No valid numeric weights found in relationships data\")\n",
    "            print(f\"Sample weight values: {relationships_df['weight'].head().tolist()}\")\n",
    "    else:\n",
    "        print(\"❌ No 'weight' column found in relationships data\")\n",
    "        print(f\"Available columns: {list(relationships_df.columns)}\")\n",
    "else:\n",
    "    print(\"❌ No relationships found in context data\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.11.13)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
