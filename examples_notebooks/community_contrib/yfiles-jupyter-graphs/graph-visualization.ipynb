{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualizing the knowledge graph with `yfiles-jupyter-graphs`\n",
    "\n",
    "This notebook is a partial copy of [local_search.ipynb](../../local_search.ipynb) that shows how to use `yfiles-jupyter-graphs` to add interactive graph visualizations of the parquet files  and how to visualize the result context of `graphrag` queries (see at the end of this notebook)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright (c) 2024 Microsoft Corporation.\n",
    "# Licensed under the MIT License."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "import tiktoken\n",
    "\n",
    "from graphrag.config.enums import ModelType\n",
    "from graphrag.config.models.language_model_config import LanguageModelConfig\n",
    "from graphrag.language_model.manager import ModelManager\n",
    "from graphrag.query.context_builder.entity_extraction import EntityVectorStoreKey\n",
    "from graphrag.query.indexer_adapters import (\n",
    "    read_indexer_covariates,\n",
    "    read_indexer_entities,\n",
    "    read_indexer_relationships,\n",
    "    read_indexer_reports,\n",
    "    read_indexer_text_units,\n",
    ")\n",
    "from graphrag.query.structured_search.local_search.mixed_context import (\n",
    "    LocalSearchMixedContext,\n",
    ")\n",
    "from graphrag.query.structured_search.local_search.search import LocalSearch\n",
    "from graphrag.vector_stores.lancedb import LanceDBVectorStore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Local Search Example\n",
    "\n",
    "Local search method generates answers by combining relevant data from the AI-extracted knowledge-graph with text chunks of the raw documents. This method is suitable for questions that require an understanding of specific entities mentioned in the documents (e.g. What are the healing properties of chamomile?)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load text units and graph data tables as context for local search\n",
    "\n",
    "- In this test we first load indexing outputs from parquet files to dataframes, then convert these dataframes into collections of data objects aligning with the knowledge model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load tables to dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_DIR = \"/home/chuaxu/projects/graphrag/ragsas/output\"\n",
    "LANCEDB_URI = f\"{INPUT_DIR}/lancedb\"\n",
    "\n",
    "COMMUNITY_REPORT_TABLE = \"community_reports\"\n",
    "COMMUNITY_TABLE = \"communities\"\n",
    "ENTITY_TABLE = \"entities\"\n",
    "RELATIONSHIP_TABLE = \"relationships\"\n",
    "COVARIATE_TABLE = \"covariates\"\n",
    "TEXT_UNIT_TABLE = \"text_units\"\n",
    "COMMUNITY_LEVEL = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read nodes table to get community and degree data\n",
    "entity_df = pd.read_parquet(f\"{INPUT_DIR}/{ENTITY_TABLE}.parquet\")\n",
    "community_df = pd.read_parquet(f\"{INPUT_DIR}/{COMMUNITY_TABLE}.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read relationships"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "relationship_df = pd.read_parquet(f\"{INPUT_DIR}/{RELATIONSHIP_TABLE}.parquet\")\n",
    "relationships = read_indexer_relationships(relationship_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualizing nodes and relationships with `yfiles-jupyter-graphs`\n",
    "\n",
    "`yfiles-jupyter-graphs` is a graph visualization extension that provides interactive and customizable visualizations for structured node and relationship data.\n",
    "\n",
    "In this case, we use it to provide an interactive visualization for the knowledge graph of the [local_search.ipynb](../../local_search.ipynb) sample by passing node and relationship lists converted from the given parquet files. The requirements for the input data is an `id` attribute for the nodes and `start`/`end` properties for the relationships that correspond to the node ids. Additional attributes can be added in the `properties` of each node/relationship dict:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install yfiles_jupyter_graphs --quiet\n",
    "from yfiles_jupyter_graphs import GraphWidget\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# converts the entities dataframe to a list of dicts for yfiles-jupyter-graphs\n",
    "def convert_entities_to_dicts(df):\n",
    "    \"\"\"Convert the entities dataframe to a list of dicts for yfiles-jupyter-graphs.\"\"\"\n",
    "    def clean_value(value):\n",
    "        \"\"\"Clean a value to make it JSON serializable.\"\"\"\n",
    "        # Handle arrays first (before checking for NaN)\n",
    "        if isinstance(value, (np.ndarray, list)):\n",
    "            # Convert arrays to strings or take first element if single value\n",
    "            if len(value) == 0:\n",
    "                return None\n",
    "            elif len(value) == 1:\n",
    "                return str(value[0])\n",
    "            else:\n",
    "                return str(value)\n",
    "        # Now check for NaN on scalar values\n",
    "        elif pd.isna(value):\n",
    "            return None\n",
    "        elif isinstance(value, (np.integer, np.floating)):\n",
    "            # Convert numpy numbers to Python numbers\n",
    "            if np.isnan(value) or np.isinf(value):\n",
    "                return None\n",
    "            return value.item()\n",
    "        elif isinstance(value, float):\n",
    "            # Handle Python floats that might be NaN or inf\n",
    "            if pd.isna(value) or np.isinf(value):\n",
    "                return None\n",
    "            return value\n",
    "        else:\n",
    "            return value\n",
    "    \n",
    "    nodes_dict = {}\n",
    "    for _, row in df.iterrows():\n",
    "        # Create a dictionary for each row and collect unique nodes\n",
    "        node_id = row[\"title\"]\n",
    "        if node_id not in nodes_dict:\n",
    "            # Clean all properties to make them JSON serializable\n",
    "            cleaned_properties = {k: clean_value(v) for k, v in row.to_dict().items()}\n",
    "            nodes_dict[node_id] = {\n",
    "                \"id\": node_id,\n",
    "                \"properties\": cleaned_properties,\n",
    "            }\n",
    "    return list(nodes_dict.values())\n",
    "\n",
    "\n",
    "# converts the relationships dataframe to a list of dicts for yfiles-jupyter-graphs\n",
    "def convert_relationships_to_dicts(df):\n",
    "    \"\"\"Convert the relationships dataframe to a list of dicts for yfiles-jupyter-graphs.\"\"\"\n",
    "    def clean_value(value):\n",
    "        \"\"\"Clean a value to make it JSON serializable.\"\"\"\n",
    "        # Handle arrays first (before checking for NaN)\n",
    "        if isinstance(value, (np.ndarray, list)):\n",
    "            # Convert arrays to strings or take first element if single value\n",
    "            if len(value) == 0:\n",
    "                return None\n",
    "            elif len(value) == 1:\n",
    "                return str(value[0])\n",
    "            else:\n",
    "                return str(value)\n",
    "        # Now check for NaN on scalar values\n",
    "        elif pd.isna(value):\n",
    "            return None\n",
    "        elif isinstance(value, (np.integer, np.floating)):\n",
    "            # Convert numpy numbers to Python numbers\n",
    "            if np.isnan(value) or np.isinf(value):\n",
    "                return None\n",
    "            return value.item()\n",
    "        elif isinstance(value, float):\n",
    "            # Handle Python floats that might be NaN or inf\n",
    "            if pd.isna(value) or np.isinf(value):\n",
    "                return None\n",
    "            return value\n",
    "        else:\n",
    "            return value\n",
    "    \n",
    "    relationships = []\n",
    "    for _, row in df.iterrows():\n",
    "        # Create a dictionary for each row\n",
    "        cleaned_properties = {k: clean_value(v) for k, v in row.to_dict().items()}\n",
    "        relationships.append({\n",
    "            \"start\": row[\"source\"],\n",
    "            \"end\": row[\"target\"],\n",
    "            \"properties\": cleaned_properties,\n",
    "        })\n",
    "    return relationships\n",
    "\n",
    "\n",
    "w = GraphWidget()\n",
    "w.directed = True\n",
    "w.nodes = convert_entities_to_dicts(entity_df)\n",
    "w.edges = convert_relationships_to_dicts(relationship_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure data-driven visualization\n",
    "\n",
    "The additional properties can be used to configure the visualization for different use cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show title on the node\n",
    "w.node_label_mapping = \"title\"\n",
    "\n",
    "\n",
    "# map community to a color\n",
    "def community_to_color(community):\n",
    "    \"\"\"Map a community to a color.\"\"\"\n",
    "    colors = [\n",
    "        \"crimson\",\n",
    "        \"darkorange\",\n",
    "        \"indigo\",\n",
    "        \"cornflowerblue\",\n",
    "        \"cyan\",\n",
    "        \"teal\",\n",
    "        \"green\",\n",
    "    ]\n",
    "    return (\n",
    "        colors[int(community) % len(colors)] if community is not None else \"lightgray\"\n",
    "    )\n",
    "\n",
    "\n",
    "def edge_to_source_community(edge):\n",
    "    \"\"\"Get the community of the source node of an edge.\"\"\"\n",
    "    source_node = next(\n",
    "        (entry for entry in w.nodes if entry[\"properties\"][\"title\"] == edge[\"start\"]),\n",
    "        None,\n",
    "    )\n",
    "    if source_node is None:\n",
    "        return None\n",
    "    # Handle missing community property gracefully\n",
    "    source_node_community = source_node[\"properties\"].get(\"community\", None)\n",
    "    return source_node_community if source_node_community is not None else None\n",
    "\n",
    "\n",
    "w.node_color_mapping = lambda node: community_to_color(node[\"properties\"].get(\"community\", None))\n",
    "w.edge_color_mapping = lambda edge: community_to_color(edge_to_source_community(edge))\n",
    "# map size data to a reasonable factor\n",
    "w.node_scale_factor_mapping = lambda node: 0.5 + node[\"properties\"].get(\"size\", 0) * 1.5 / 20\n",
    "# use weight for edge thickness\n",
    "w.edge_thickness_factor_mapping = \"weight\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Automatic layouts\n",
    "\n",
    "The widget provides different automatic layouts that serve different purposes: `Circular`, `Hierarchic`, `Organic (interactiv or static)`, `Orthogonal`, `Radial`, `Tree`, `Geo-spatial`.\n",
    "\n",
    "For the knowledge graph, this sample uses the `Circular` layout, though `Hierarchic` or `Organic` are also suitable choices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the circular layout for this visualization. For larger graphs, the default organic layout is often preferrable.\n",
    "w.circular_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Display the graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "43fee90abdf947b5941d87492c395006",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "GraphWidget(layout=Layout(height='800px', width='100%'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualizing the result context of `graphrag` queries\n",
    "\n",
    "The result context of `graphrag` queries allow to inspect the context graph of the request. This data can similarly be visualized as graph with `yfiles-jupyter-graphs`.\n",
    "\n",
    "## Making the request\n",
    "\n",
    "The following cell recreates the sample queries from [local_search.ipynb](../../local_search.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Covariate file not found, proceeding without covariates\n"
     ]
    }
   ],
   "source": [
    "# setup (see also ../../local_search.ipynb)\n",
    "entities = read_indexer_entities(entity_df, community_df, COMMUNITY_LEVEL)\n",
    "\n",
    "description_embedding_store = LanceDBVectorStore(\n",
    "    collection_name=\"default-entity-description\",\n",
    ")\n",
    "description_embedding_store.connect(db_uri=LANCEDB_URI)\n",
    "\n",
    "# Comment out covariates for now if file doesn't exist\n",
    "try:\n",
    "    covariate_df = pd.read_parquet(f\"{INPUT_DIR}/{COVARIATE_TABLE}.parquet\")\n",
    "    claims = read_indexer_covariates(covariate_df)\n",
    "    covariates = {\"claims\": claims}\n",
    "except FileNotFoundError:\n",
    "    print(\"Covariate file not found, proceeding without covariates\")\n",
    "    covariates = {}\n",
    "\n",
    "report_df = pd.read_parquet(f\"{INPUT_DIR}/{COMMUNITY_REPORT_TABLE}.parquet\")\n",
    "reports = read_indexer_reports(report_df, community_df, COMMUNITY_LEVEL)\n",
    "text_unit_df = pd.read_parquet(f\"{INPUT_DIR}/{TEXT_UNIT_TABLE}.parquet\")\n",
    "text_units = read_indexer_text_units(text_unit_df)\n",
    "\n",
    "# Load configuration from settings.yaml\n",
    "from graphrag.config.load_config import load_config\n",
    "from pathlib import Path\n",
    "\n",
    "config_path = Path(\"/home/chuaxu/projects/graphrag/ragsas\")\n",
    "config = load_config(config_path)\n",
    "\n",
    "# Get model configurations from the loaded config\n",
    "chat_model_config = config.get_language_model_config(\"default_chat_model\")\n",
    "embedding_model_config = config.get_language_model_config(\"default_embedding_model\")\n",
    "\n",
    "chat_model = ModelManager().get_or_create_chat_model(\n",
    "    name=\"local_search\",\n",
    "    model_type=chat_model_config.type,\n",
    "    config=chat_model_config,\n",
    ")\n",
    "\n",
    "token_encoder = tiktoken.encoding_for_model(chat_model_config.model)\n",
    "\n",
    "text_embedder = ModelManager().get_or_create_embedding_model(\n",
    "    name=\"local_search_embedding\",\n",
    "    model_type=embedding_model_config.type,\n",
    "    config=embedding_model_config,\n",
    ")\n",
    "\n",
    "context_builder = LocalSearchMixedContext(\n",
    "    community_reports=reports,\n",
    "    text_units=text_units,\n",
    "    entities=entities,\n",
    "    relationships=relationships,\n",
    "    covariates=covariates,\n",
    "    entity_text_embeddings=description_embedding_store,\n",
    "    embedding_vectorstore_key=EntityVectorStoreKey.ID,  # if the vectorstore uses entity title as ids, set this to EntityVectorStoreKey.TITLE\n",
    "    text_embedder=text_embedder,\n",
    "    token_encoder=token_encoder,\n",
    ")\n",
    "\n",
    "local_context_params = {\n",
    "    \"text_unit_prop\": 0.5,\n",
    "    \"community_prop\": 0.1,\n",
    "    \"conversation_history_max_turns\": 5,\n",
    "    \"conversation_history_user_turns_only\": True,\n",
    "    \"top_k_mapped_entities\": 3,\n",
    "    \"top_k_relationships\": 3,\n",
    "    \"include_entity_rank\": True,\n",
    "    \"include_relationship_weight\": True,\n",
    "    \"include_community_rank\": False,\n",
    "    \"return_candidate_context\": False,\n",
    "    \"embedding_vectorstore_key\": EntityVectorStoreKey.ID,  # set this to EntityVectorStoreKey.TITLE if the vectorstore uses entity title as ids\n",
    "    \"max_tokens\": 80_000,  # change this based on the token limit you have on your model (if you are using a model with 8k limit, a good setting could be 5000)\n",
    "}\n",
    "\n",
    "model_params = {\n",
    "    \"max_tokens\": 16_000,  # change this based on the token limit you have on your model (if you are using a model with 8k limit, a good setting could be 1000=1500, the model supports at most 16384 completion tokens)\n",
    "    \"temperature\": 0.0,\n",
    "}\n",
    "\n",
    "search_engine = LocalSearch(\n",
    "    model=chat_model,\n",
    "    context_builder=context_builder,\n",
    "    token_encoder=token_encoder,\n",
    "    model_params=model_params,\n",
    "    context_builder_params=local_context_params,\n",
    "    response_type=\"multiple paragraphs\",  # free form text describing the response type and format, can be anything, e.g. prioritized list, single paragraph, multiple paragraphs, multiple-page report\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run local search on sample queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'm sorry, but there is no information available about Agent Mercer in the provided data tables. If you have any other questions or need information on a different topic, feel free to ask!\n"
     ]
    }
   ],
   "source": [
    "result = await search_engine.search(\"Tell me about Agent Mercer\")\n",
    "print(result.response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "### Introduction to SAS Econometrics for Pricing Strategy Analysis\n",
       "\n",
       "SAS Econometrics is a comprehensive software suite developed by SAS Institute Inc., designed to facilitate advanced econometric analysis. It provides a wide array of tools and procedures tailored for statistical and econometric modeling, making it an invaluable resource for analyzing the impact of pricing strategies on business revenue [Data: Entities (2); Reports (163)].\n",
       "\n",
       "### Key Procedures for Pricing Strategy Analysis\n",
       "\n",
       "One of the standout features of SAS Econometrics is its support for Bayesian analysis and advanced econometric methods, which are crucial for understanding complex pricing dynamics. The suite includes specialized procedures such as PROC DEEPPRICE, which is particularly designed for deep learning and policy evaluation in the context of analyzing price and demand data. This procedure is instrumental in estimating demand curves and optimal revenue per user by accounting for heterogeneous price effects based on user characteristics [Data: Entities (1762); Relationships (1940)].\n",
       "\n",
       "PROC DEEPPRICE allows businesses to specify the correct functional form to accurately capture the relationship between price and demand. It also provides options for handling missing values and ensuring reproducibility, which are essential for maintaining the integrity and accuracy of the data analysis. By integrating these capabilities, PROC DEEPPRICE serves as a powerful tool for those seeking to leverage data-driven insights to optimize pricing and maximize revenue [Data: Entities (1762)].\n",
       "\n",
       "### Additional Tools and Techniques\n",
       "\n",
       "In addition to PROC DEEPPRICE, SAS Econometrics offers other procedures such as PROC CNTSELECT and PROC SEVSELECT, which are used for count data modeling and severity modeling, respectively. These procedures are essential for accurately capturing the underlying patterns in data and addressing issues such as overdispersion and excess zeros, which are common in real-world datasets [Data: Entities (290); Relationships (2131, 2137)].\n",
       "\n",
       "SAS Econometrics operates in a cloud environment, providing users with the flexibility and scalability needed for large-scale data analysis. This cloud-based approach ensures that users can access the software's powerful features from anywhere, facilitating collaboration and efficiency in data-driven decision-making [Data: Entities (2)].\n",
       "\n",
       "### Conclusion\n",
       "\n",
       "Overall, SAS Econometrics provides a robust and versatile toolset that empowers users to conduct sophisticated econometric analyses, develop predictive models, and perform Bayesian inference with ease and precision. Its comprehensive suite of procedures and cloud-based accessibility make it an invaluable resource for professionals in the field of econometrics, particularly when analyzing the impact of different pricing strategies on business revenue [Data: Entities (2); Reports (163)]."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import Markdown, display\n",
    "\n",
    "question = \"How can we use SAS Econometric products to help analyze the impact of different pricing strategies on business revenue?\"\n",
    "# question = \"Which published studies in our knowledge base used both panel data methods and cointegration analysis on emerging market economies?\"\n",
    "result = await search_engine.search(question)\n",
    "\n",
    "# Display as formatted Markdown instead of plain text\n",
    "display(Markdown(result.response))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspecting the context data used to generate the response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>entity</th>\n",
       "      <th>description</th>\n",
       "      <th>number of relationships</th>\n",
       "      <th>in_context</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>290</td>\n",
       "      <td>SAS ECONOMETRICS PROCEDURES</td>\n",
       "      <td>SAS Econometrics Procedures are a comprehensiv...</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>SAS ECONOMETRICS</td>\n",
       "      <td>SAS Econometrics is a comprehensive software s...</td>\n",
       "      <td>31</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1102</td>\n",
       "      <td>SAS/ETS SOFTWARE</td>\n",
       "      <td></td>\n",
       "      <td>2</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>SAS/ETS</td>\n",
       "      <td>SAS/ETS is a comprehensive software suite with...</td>\n",
       "      <td>14</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1762</td>\n",
       "      <td>PROC DEEPPRICE</td>\n",
       "      <td>PROC DEEPPRICE is a versatile procedure design...</td>\n",
       "      <td>24</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     id                       entity  \\\n",
       "0   290  SAS ECONOMETRICS PROCEDURES   \n",
       "1     2             SAS ECONOMETRICS   \n",
       "2  1102             SAS/ETS SOFTWARE   \n",
       "3     4                      SAS/ETS   \n",
       "4  1762               PROC DEEPPRICE   \n",
       "\n",
       "                                         description number of relationships  \\\n",
       "0  SAS Econometrics Procedures are a comprehensiv...                       1   \n",
       "1  SAS Econometrics is a comprehensive software s...                      31   \n",
       "2                                                                          2   \n",
       "3  SAS/ETS is a comprehensive software suite with...                      14   \n",
       "4  PROC DEEPPRICE is a versatile procedure design...                      24   \n",
       "\n",
       "   in_context  \n",
       "0        True  \n",
       "1        True  \n",
       "2        True  \n",
       "3        True  \n",
       "4        True  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.context_data[\"entities\"].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>source</th>\n",
       "      <th>target</th>\n",
       "      <th>description</th>\n",
       "      <th>weight</th>\n",
       "      <th>links</th>\n",
       "      <th>in_context</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4</td>\n",
       "      <td>SAS ECONOMETRICS</td>\n",
       "      <td>SAS/ETS</td>\n",
       "      <td>SAS Econometrics and SAS/ETS are software suit...</td>\n",
       "      <td>16.0</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>SAS INSTITUTE INC.</td>\n",
       "      <td>SAS ECONOMETRICS</td>\n",
       "      <td>SAS Institute Inc. developed and provides the ...</td>\n",
       "      <td>9.0</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3484</td>\n",
       "      <td>SAS</td>\n",
       "      <td>SAS ECONOMETRICS</td>\n",
       "      <td>SAS Econometrics is a product within the SAS s...</td>\n",
       "      <td>9.0</td>\n",
       "      <td>2</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>247</td>\n",
       "      <td>SAS</td>\n",
       "      <td>SAS ECONOMETRICS PROCEDURES</td>\n",
       "      <td>SAS Econometrics Procedures are part of the SA...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2143</td>\n",
       "      <td>SAS ECONOMETRICS</td>\n",
       "      <td>PROC CCDM</td>\n",
       "      <td>PROC CCDM is a procedure within SAS Econometri...</td>\n",
       "      <td>8.0</td>\n",
       "      <td>2</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     id              source                       target  \\\n",
       "0     4    SAS ECONOMETRICS                      SAS/ETS   \n",
       "1     0  SAS INSTITUTE INC.             SAS ECONOMETRICS   \n",
       "2  3484                 SAS             SAS ECONOMETRICS   \n",
       "3   247                 SAS  SAS ECONOMETRICS PROCEDURES   \n",
       "4  2143    SAS ECONOMETRICS                    PROC CCDM   \n",
       "\n",
       "                                         description weight links  in_context  \n",
       "0  SAS Econometrics and SAS/ETS are software suit...   16.0     1        True  \n",
       "1  SAS Institute Inc. developed and provides the ...    9.0     1        True  \n",
       "2  SAS Econometrics is a product within the SAS s...    9.0     2        True  \n",
       "3  SAS Econometrics Procedures are part of the SA...    1.0     2        True  \n",
       "4  PROC CCDM is a procedure within SAS Econometri...    8.0     2        True  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.context_data[\"relationships\"].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing the result context as graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f38860732d7d4879a1d6ec4b7fb37b58",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "GraphWidget(layout=Layout(height='500px', width='100%'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Helper function to visualize the result context with `yfiles-jupyter-graphs`.\n",
    "\n",
    "The dataframes are converted into supported nodes and relationships lists and then passed to yfiles-jupyter-graphs.\n",
    "Additionally, some values are mapped to visualization properties.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def show_graph(result):\n",
    "    \"\"\"Visualize the result context with yfiles-jupyter-graphs.\"\"\"\n",
    "    from yfiles_jupyter_graphs import GraphWidget\n",
    "\n",
    "    if (\n",
    "        \"entities\" not in result.context_data\n",
    "        or \"relationships\" not in result.context_data\n",
    "    ):\n",
    "        msg = \"The passed results do not contain 'entities' or 'relationships'\"\n",
    "        raise ValueError(msg)\n",
    "\n",
    "    # converts the entities dataframe to a list of dicts for yfiles-jupyter-graphs\n",
    "    def convert_entities_to_dicts(df):\n",
    "        \"\"\"Convert the entities dataframe to a list of dicts for yfiles-jupyter-graphs.\"\"\"\n",
    "        nodes_dict = {}\n",
    "        for _, row in df.iterrows():\n",
    "            # Create a dictionary for each row and collect unique nodes\n",
    "            node_id = row[\"entity\"]\n",
    "            if node_id not in nodes_dict:\n",
    "                nodes_dict[node_id] = {\n",
    "                    \"id\": node_id,\n",
    "                    \"properties\": row.to_dict(),\n",
    "                }\n",
    "        return list(nodes_dict.values())\n",
    "\n",
    "    # converts the relationships dataframe to a list of dicts for yfiles-jupyter-graphs\n",
    "    def convert_relationships_to_dicts(df):\n",
    "        \"\"\"Convert the relationships dataframe to a list of dicts for yfiles-jupyter-graphs.\"\"\"\n",
    "        relationships = []\n",
    "        for _, row in df.iterrows():\n",
    "            # Create a dictionary for each row\n",
    "            relationships.append({\n",
    "                \"start\": row[\"source\"],\n",
    "                \"end\": row[\"target\"],\n",
    "                \"properties\": row.to_dict(),\n",
    "            })\n",
    "        return relationships\n",
    "\n",
    "    w = GraphWidget()\n",
    "    # use the converted data to visualize the graph\n",
    "    w.nodes = convert_entities_to_dicts(result.context_data[\"entities\"])\n",
    "    w.edges = convert_relationships_to_dicts(result.context_data[\"relationships\"])\n",
    "    w.directed = True\n",
    "    # show title on the node\n",
    "    w.node_label_mapping = \"entity\"\n",
    "    # use weight for edge thickness\n",
    "    w.edge_thickness_factor_mapping = \"weight\"\n",
    "    display(w)\n",
    "\n",
    "\n",
    "show_graph(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== CONTEXT DATA ANALYSIS ===\n",
      "\n",
      "📊 Entities in context: 6 entities\n",
      "   - Columns: ['id', 'entity', 'description', 'number of relationships', 'in_context']\n",
      "   - Sample entity: SAS ECONOMETRICS PROCEDURES\n",
      "📊 Relationships in context: 20 relationships\n",
      "   - Columns: ['id', 'source', 'target', 'description', 'weight', 'links', 'in_context']\n",
      "   - Sample relationship: SAS ECONOMETRICS -> SAS/ETS\n",
      "📊 Community reports in context: 1 reports\n",
      "📊 Text sources in context: 3 text units\n",
      "\n",
      "=== TOKEN ANALYSIS ===\n",
      "\n",
      "🔢 Token Breakdown:\n",
      "   - System prompt: 604 tokens\n",
      "   - Context data: 7,568 tokens\n",
      "   - User question: 21 tokens\n",
      "   - Total INPUT: 8,193 tokens\n",
      "   - Response: 516 tokens\n",
      "   - TOTAL MESSAGE: 8,709 tokens\n",
      "\n",
      "📏 Model Capacity:\n",
      "   - Model limit: 128,000 tokens\n",
      "   - Used: 8,709 tokens (6.8%)\n",
      "   - Remaining: 119,291 tokens\n",
      "   ✅ Token count is within safe limits\n",
      "\n",
      "================================================================================\n",
      "=== FULL CONTEXT SENT TO LLM ===\n",
      "================================================================================\n",
      "\n",
      "� SYSTEM PROMPT (Skipped)\n",
      "----------------------------------------\n",
      "\n",
      "🔵 USER QUESTION:\n",
      "----------------------------------------\n",
      "How can we use SAS Econometric products to help analyze the impact of different pricing strategies on business revenue?\n",
      "\n",
      "🔵 CONTEXT DATA (7,568 tokens):\n",
      "----------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Relevant Entities:\n",
       "\n",
       "**SAS ECONOMETRICS PROCEDURES** (Rank: N/A)\n",
       "Description: SAS Econometrics Procedures are a comprehensive suite of statistical analysis tools designed for conducting a wide range of econometric analyses. These procedures are particularly useful for the levelization of classification variables, which is a critical step in preparing data for further econometric modeling. The suite includes a variety of statistical methods tailored to address different aspects of econometric analysis, ensuring that users have the flexibility and precision needed for their specific research or business needs.\n",
       "\n",
       "Among the key procedures included in SAS Econometrics are CNTSELECT, CPANEL, CQLIM, CSPATIALREG, and SEVSELECT. Each of these methods serves a distinct purpose within the econometric analysis framework. CNTSELECT is typically used for selecting the best model from a set of candidate models, ensuring that the chosen model is the most appropriate for the data at hand. CPANEL is designed for panel data analysis, allowing users to handle data that involves observations over multiple time periods for the same entities, which is common in economic and financial studies.\n",
       "\n",
       "CQLIM is another critical procedure, used for limited dependent variable models, which are essential when dealing with outcomes that are categorical or otherwise constrained. CSPATIALREG is employed for spatial regression analysis, enabling users to account for spatial dependencies in their data, which is particularly important in fields such as regional economics and real estate. Lastly, SEVSELECT is used for selection models, helping to correct for selection bias in econometric models, which can otherwise lead to inaccurate estimates and conclusions.\n",
       "\n",
       "Overall, SAS Econometrics Procedures provide a robust and versatile toolkit for econometricians and analysts, facilitating the execution of complex statistical analyses with precision and efficiency. These procedures are integral to the SAS software suite, known for its powerful data analysis capabilities, and are widely used in academia, government, and industry for their reliability and comprehensive approach to econometric challenges.\n",
       "\n",
       "**SAS ECONOMETRICS** (Rank: N/A)\n",
       "Description: SAS Econometrics is a comprehensive software suite developed by SAS Institute Inc., designed to facilitate advanced econometric analysis. This suite is part of the broader SAS software ecosystem and provides a wide array of tools and procedures tailored for statistical and econometric modeling. It is particularly noted for its capabilities in time series analysis, econometric modeling, and Bayesian inference.\n",
       "\n",
       "The suite includes a variety of specialized procedures such as PROC UCM for time series analysis, PROC SEVSELECT and PROC CNTSELECT for econometric analysis, and the ECM procedure for developing economic capital models. These procedures are instrumental in implementing modeling and simulation steps in economic capital modeling. SAS Econometrics also offers the CCDM and CCOPULA procedures, which are used for modeling compound distributions and copulas, respectively.\n",
       "\n",
       "One of the standout features of SAS Econometrics is its support for Bayesian analysis, which is facilitated through procedures like CQLIM, CNTSELECT, and SMC. These procedures employ advanced algorithms such as the random walk Metropolis (RWM) algorithm, which is enhanced with self-tuning capabilities to optimize Bayesian inference processes. The suite also includes tools for predictive modeling and various econometric methods, making it a versatile choice for econometricians and data analysts.\n",
       "\n",
       "SAS Econometrics operates in a cloud environment, providing users with the flexibility and scalability needed for large-scale data analysis. This cloud-based approach ensures that users can access the software's powerful features from anywhere, facilitating collaboration and efficiency in data-driven decision-making.\n",
       "\n",
       "Overall, SAS Econometrics is a robust and versatile toolset that empowers users to conduct sophisticated econometric analyses, develop predictive models, and perform Bayesian inference with ease and precision. Its comprehensive suite of procedures and cloud-based accessibility make it an invaluable resource for professionals in the field of econometrics.\n",
       "\n",
       "**SAS/ETS SOFTWARE** (Rank: N/A)\n",
       "Description: \n",
       "\n",
       "**SAS/ETS** (Rank: N/A)\n",
       "Description: SAS/ETS is a comprehensive software suite within the SAS software ecosystem, specifically designed to provide robust tools for econometric and time series analysis. This suite includes a variety of procedures that cater to different analytical needs, making it a versatile choice for professionals in the field of econometrics and time series data analysis.\n",
       "\n",
       "One of the key features of SAS/ETS is its inclusion of the UCM (Unobserved Components Model) procedure, which is instrumental in time series analysis. This procedure allows users to decompose time series data into components such as trend, seasonal, and irregular components, facilitating a deeper understanding of the underlying patterns in the data.\n",
       "\n",
       "In addition to UCM, SAS/ETS offers a wide array of other procedures that enhance its functionality. These include ARIMA (AutoRegressive Integrated Moving Average), which is widely used for forecasting and understanding time series data; ESM (Exponential Smoothing Models), which provides a framework for smoothing time series data; VARMAX (Vector Autoregressive Moving-Average with Exogenous Inputs), which is useful for multivariate time series analysis; STATESPACE, which is used for state space modeling; and PANEL, which is designed for panel data analysis.\n",
       "\n",
       "SAS/ETS also includes procedures specifically for count-data modeling, such as COUNTREG and HPCOUNTREG. These procedures are essential for estimating count regression models, which are used when the data being analyzed are counts or non-negative integers. This capability is particularly useful in fields such as biostatistics, epidemiology, and social sciences, where count data is prevalent.\n",
       "\n",
       "Overall, SAS/ETS stands out as a powerful and flexible toolset for econometric and time series analysis, offering a wide range of procedures that cater to both basic and advanced analytical needs. Its integration within the broader SAS software suite ensures that users have access to a comprehensive set of tools for data analysis, making it a valuable resource for analysts and researchers alike.\n",
       "\n",
       "**PROC DEEPPRICE** (Rank: N/A)\n",
       "Description: PROC DEEPPRICE is a versatile procedure designed for deep learning and policy evaluation, particularly in the context of analyzing price and demand data. It is equipped to handle continuous outcome variables by utilizing the identity function for G in the outcome model, which simplifies the process of modeling these types of variables. This procedure is instrumental in estimating demand curves, allowing users to specify the correct functional form to accurately capture the relationship between price and demand. Additionally, PROC DEEPPRICE is adept at estimating optimal revenue per user by accounting for heterogeneous price effects based on user characteristics, thereby enabling more personalized and effective pricing strategies.\n",
       "\n",
       "The procedure offers several options to enhance its functionality and adaptability. Users can specify the minibatch size, which is crucial for managing computational resources and optimizing the learning process in deep learning applications. Furthermore, PROC DEEPPRICE includes options for random seed generation, ensuring reproducibility and consistency in results across different runs. It also provides mechanisms for handling missing values, which is essential for maintaining the integrity and accuracy of the data analysis.\n",
       "\n",
       "In the context of price effect estimation, PROC DEEPPRICE saves the estimation details, facilitating a comprehensive analysis of how price changes impact demand. This feature is particularly useful for businesses and researchers aiming to understand and predict consumer behavior in response to pricing strategies. By integrating these capabilities, PROC DEEPPRICE serves as a powerful tool for those seeking to leverage data-driven insights to optimize pricing and maximize revenue.\n",
       "\n",
       "Overall, PROC DEEPPRICE stands out as a robust procedure that combines deep learning techniques with advanced policy evaluation methods to deliver precise and actionable insights into price and demand dynamics. Its ability to handle continuous outcome variables, estimate demand curves, and account for heterogeneous price effects makes it an invaluable asset for analysts and decision-makers in various industries.\n",
       "\n",
       "**SAS INSTITUTE INC.** (Rank: N/A)\n",
       "Description: SAS Institute Inc. is a prominent software company headquartered in Cary, North Carolina, USA. Renowned for its expertise in analytics software, SAS Institute Inc. has established itself as a leader in the development of advanced statistical and econometric tools. The company is particularly well-known for its comprehensive statistical software suite, which is widely used across various industries for data analysis and decision-making processes.\n",
       "\n",
       "In addition to its statistical software, SAS Institute Inc. offers a range of analytics services that cater to diverse business needs. Among its notable offerings is SAS Econometrics, a sophisticated toolset designed to provide predictive modeling capabilities and econometrics procedures. This suite of tools is instrumental for businesses and researchers who require robust analytical solutions to forecast trends, analyze economic data, and make informed decisions based on quantitative insights.\n",
       "\n",
       "SAS Institute Inc.'s commitment to innovation and excellence in analytics software has made it a trusted partner for organizations seeking to leverage data for strategic advantage. By continuously enhancing its software capabilities and expanding its service offerings, SAS Institute Inc. remains at the forefront of the analytics industry, empowering users to harness the power of data for improved outcomes.\n",
       "\n",
       "\n",
       "## Relevant Relationships:\n",
       "\n",
       "**SAS ECONOMETRICS → SAS/ETS** (Weight: 16.0)\n",
       "Description: SAS Econometrics and SAS/ETS are software suites designed to facilitate econometric and time series analysis. Both suites include the UCM Procedure, which is a powerful tool for analyzing unobserved components in time series data. SAS Econometrics offers users access to the comprehensive capabilities of SAS/ETS software, enabling them to perform advanced econometric analysis. These suites are integral for professionals and researchers who require robust tools for modeling, forecasting, and analyzing economic and financial data. By leveraging the features of SAS/ETS, SAS Econometrics enhances the user's ability to conduct sophisticated analyses, making it a valuable resource in the field of econometrics.\n",
       "\n",
       "**SAS INSTITUTE INC. → SAS ECONOMETRICS** (Weight: 9.0)\n",
       "Description: SAS Institute Inc. developed and provides the SAS Econometrics suite\n",
       "\n",
       "**SAS → SAS ECONOMETRICS** (Weight: 9.0)\n",
       "Description: SAS Econometrics is a product within the SAS software suite\n",
       "\n",
       "**SAS → SAS ECONOMETRICS PROCEDURES** (Weight: 1.0)\n",
       "Description: SAS Econometrics Procedures are part of the SAS software suite for econometric analysis\n",
       "\n",
       "**SAS ECONOMETRICS → PROC CCDM** (Weight: 8.0)\n",
       "Description: PROC CCDM is a procedure within SAS Econometrics used for estimating compound distribution models\n",
       "\n",
       "**PROC CCDM → SAS ECONOMETRICS** (Weight: 23.0)\n",
       "Description: PROC CCDM is a specialized procedure integrated within the SAS Econometrics software suite, designed to facilitate advanced statistical analysis. As a component of SAS Econometrics, PROC CCDM plays a crucial role in the simulation of counts and severity, making it an essential tool for econometricians and data analysts who require precise modeling capabilities. The procedure is utilized in conjunction with other features of SAS Econometrics to enhance the accuracy and reliability of statistical simulations, particularly in scenarios where understanding the distribution and impact of various economic factors is critical. By leveraging PROC CCDM, users can effectively analyze complex datasets, enabling them to derive meaningful insights and make informed decisions based on robust statistical evidence. Overall, PROC CCDM, as part of SAS Econometrics, provides a comprehensive framework for conducting sophisticated econometric analyses, ensuring that users have access to the tools necessary for tackling intricate statistical challenges.\n",
       "\n",
       "**PROC CCDM → SAS/ETS** (Weight: 5.0)\n",
       "Description: PROC CCDM is used in conjunction with SAS/ETS for simulating counts and severity\n",
       "\n",
       "**SAS ECONOMETRICS → UCM PROCEDURE** (Weight: 9.0)\n",
       "Description: The UCM Procedure is a part of the SAS Econometrics software suite\n",
       "\n",
       "**UCM PROCEDURE → SAS ECONOMETRICS** (Weight: 8.0)\n",
       "Description: The UCM procedure is part of SAS Econometrics for time series analysis\n",
       "\n",
       "**SAS ECONOMETRICS → PROC UCM** (Weight: 9.0)\n",
       "Description: PROC UCM is a procedure available in the SAS Econometrics software suite\n",
       "\n",
       "**SAS/ETS → UCM PROCEDURE** (Weight: 1.0)\n",
       "Description: The UCM Procedure is a part of the SAS/ETS software suite\n",
       "\n",
       "**UCM PROCEDURE → SAS/ETS** (Weight: 8.0)\n",
       "Description: The UCM procedure is also part of SAS/ETS for time series analysis\n",
       "\n",
       "**PROC DEEPPRICE → MODEL** (Weight: 1.0)\n",
       "Description: PROC DEEPPRICE includes a MODEL statement for specifying outcome models and DNN fitting\n",
       "\n",
       "**SAS/ETS → PROC UCM** (Weight: 1.0)\n",
       "Description: PROC UCM is a procedure available in the SAS/ETS software suite\n",
       "\n",
       "**MODEL → SAS/ETS** (Weight: 7.0)\n",
       "Description: The MODEL procedure is part of the SAS/ETS software suite\n",
       "\n",
       "**SAS ECONOMETRICS → PROC SEVSELECT** (Weight: 8.0)\n",
       "Description: PROC SEVSELECT is a procedure within SAS Econometrics for severity modeling\n",
       "\n",
       "**PROC SEVSELECT → SAS ECONOMETRICS** (Weight: 1.0)\n",
       "Description: PROC SEVSELECT is a procedure within SAS Econometrics\n",
       "\n",
       "**PROC DEEPPRICE → MYLIB** (Weight: 7.0)\n",
       "Description: PROC DEEPPRICE uses data from MYLIB to estimate price effects and save results\n",
       "\n",
       "**PROC CSSM → SAS/ETS** (Weight: 7.0)\n",
       "Description: PROC CSSM complements several SAS/ETS procedures by offering solutions for more general problems or detailed analysis\n",
       "\n",
       "**PROC CNTSELECT → SAS ECONOMETRICS** (Weight: 9.0)\n",
       "Description: PROC CNTSELECT is a specialized procedure within SAS Econometrics designed for count data modeling and statistical analysis. As part of the SAS Econometrics suite, PROC CNTSELECT provides users with robust tools for handling count data, which is data that represents the number of occurrences of an event. This procedure is particularly useful in fields such as economics, healthcare, and social sciences, where count data is prevalent and requires precise modeling techniques.\n",
       "\n",
       "SAS Econometrics, the broader framework within which PROC CNTSELECT operates, is a comprehensive software package that offers a wide range of econometric and statistical tools. It is widely used by researchers and analysts to perform complex data analysis, model economic phenomena, and make informed decisions based on empirical data. The inclusion of PROC CNTSELECT in SAS Econometrics enhances the suite's capabilities by allowing users to effectively model count data, which can often be challenging due to its discrete nature and potential for overdispersion.\n",
       "\n",
       "PROC CNTSELECT is equipped with advanced statistical methods that enable users to select the most appropriate model for their count data. It supports various modeling techniques, including Poisson regression, negative binomial regression, and zero-inflated models, among others. These techniques are essential for accurately capturing the underlying patterns in count data and addressing issues such as overdispersion and excess zeros, which are common in real-world datasets.\n",
       "\n",
       "The procedure is designed to be user-friendly, with a syntax that is consistent with other SAS procedures, making it accessible to both novice and experienced users. It provides comprehensive output that includes parameter estimates, goodness-of-fit statistics, and diagnostic measures, allowing users to thoroughly evaluate the performance of their models. Additionally, PROC CNTSELECT offers options for model selection and validation, ensuring that users can identify the best-fitting model for their data.\n",
       "\n",
       "Overall, PROC CNTSELECT is a valuable tool within SAS Econometrics for anyone dealing with count data. Its integration into the SAS Econometrics suite underscores its importance in the realm of statistical modeling, providing users with the necessary resources to tackle complex count data challenges and derive meaningful insights from their analyses.\n",
       "\n",
       "\n",
       "## Relevant Community Reports:\n",
       "\n",
       "**SAS Econometrics and SAS Institute Inc.** (Rank: N/A)\n",
       "# SAS Econometrics and SAS Institute Inc.\n",
       "\n",
       "The community is centered around SAS Econometrics, a comprehensive software suite developed by SAS Institute Inc., which is headquartered in Cary, NC. This suite is integral to advanced econometric analysis and is supported by various data tables and technical support. Key figures such as Schervish and Gilks contribute to the theoretical underpinnings of the software's capabilities, particularly in Bayesian statistics and MCMC methods.\n",
       "\n",
       "## SAS Econometrics as a central tool for econometric analysis\n",
       "\n",
       "SAS Econometrics is a pivotal software suite developed by SAS Institute Inc., designed to facilitate advanced econometric analysis. It offers a wide array of tools tailored for statistical and econometric modeling, including time series analysis, econometric modeling, and Bayesian inference. The suite's cloud-based environment provides flexibility and scalability, making it a valuable resource for professionals in the field of econometrics [Data: Entities (2, 0); Relationships (0, 2)].\n",
       "\n",
       "## Role of SAS Institute Inc. in analytics software development\n",
       "\n",
       "SAS Institute Inc., headquartered in Cary, NC, is a leader in analytics software development, renowned for its expertise in creating advanced statistical and econometric tools. The company's commitment to innovation and excellence has established it as a trusted partner for organizations seeking to leverage data for strategic advantage. SAS Econometrics is one of its notable offerings, providing predictive modeling capabilities and econometrics procedures essential for data-driven decision-making [Data: Entities (0, 1); Relationships (0)].\n",
       "\n",
       "## Schervish's contributions to statistical methods\n",
       "\n",
       "Schervish is a prominent figure in the field of statistics, known for his contributions to probability and statistical methods. His work on large-sample theory, particularly concerning the posterior distribution of parameters, has been influential in shaping contemporary statistical thought. Schervish's expertise is utilized in SAS Econometrics for tuning random walk Metropolis algorithms, enhancing the software's capabilities in Bayesian analysis [Data: Entities (23); Relationships (55)].\n",
       "\n",
       "## Operational risk management with SAS Econometrics\n",
       "\n",
       "SAS Econometrics plays a crucial role in operational risk management through its integration with data tables such as OpRiskLossCounts, OpRiskLosses, and OpRiskMatchedLosses. These tables provide structured formats for capturing and analyzing loss data, enabling organizations to develop effective risk mitigation strategies. The comprehensive view of operational risk exposure offered by these tables facilitates informed decision-making and strategic planning [Data: Entities (1881, 1882, 1883); Relationships (2132, 2133, 2134)].\n",
       "\n",
       "## Technical support for SAS Econometrics users\n",
       "\n",
       "SAS Technical Support provides assistance to users of SAS Econometrics, addressing technical questions and ensuring the effective use of the software's procedures. This support is crucial for users to fully leverage the suite's capabilities in econometric analysis and operational risk management, enhancing the overall user experience and efficiency in data-driven decision-making [Data: Entities (7); Relationships (7)].\n",
       "\n",
       "## Gilks's impact on Bayesian statistics and MCMC methods\n",
       "\n",
       "Gilks is an esteemed author recognized for his contributions to statistical methods, particularly in MCMC methods and Bayesian statistics. His work has advanced the understanding and application of these techniques, which are pivotal in various scientific domains. Gilks's research on acceptance rates and scale parameters in Metropolis algorithms has improved the efficiency and accuracy of MCMC simulations, benefiting users of SAS Econometrics [Data: Entities (53); Relationships (43, 44)].\n",
       "\n",
       "\n",
       "## Relevant Text Sources:\n",
       "\n",
       "**Source 30** (Rank: N/A)\n",
       " and associated cloud services in SAS Viya. This section describes how to create a CAS session and set up a CAS engine libref that you can use to connect to the CAS session. It assumes that you have a CAS server already available; contact your system administrator if you need help starting and terminating a server. This CAS server is identifed by specifying the host on which it runs and the port on which it listens for communications. To simplify your interactions with this CAS server, the host information and port information for the server are stored as SAS option values that are retrieved automatically whenever this CAS server needs to be accessed. You can examine the host and port values for the server at your site by using the following statements: \n",
       "proc options option=(CASHOST CASPORT); run; \n",
       "In addition to starting a CAS server, your system administrator might also have created a CAS session and a CAS engine libref for your use. You can defne your own sessions and CAS engine librefs that connect to the CAS server as shown in the following statements: \n",
       "cas mysess; libname mylib cas sessref=mysess; \n",
       "The CAS statement creates the CAS session named mysess, and the LIBNAME statement creates the mylib CAS engine libref that you use to connect to this session. It is not necessary to explicitly name the CASHOST and CASPORT of the CAS server in the CAS statement, because these values are retrieved from the corresponding SAS option values. \n",
       "If you have created the mysess session, you can terminate it by using the TERMINATE option in the CAS statement as follows: \n",
       "cas mysess terminate; \n",
       "For more information about the CAS statement and the LIBNAME statement, see SAS Cloud Analytic Services: Users Guide. For general information about CAS and CAS sessions, see SAS Cloud Analytic Services: Fundamentals. \n",
       "\n",
       "Loading a SAS Data Set onto a CAS Server \n",
       "Procedures in this book require the input data to reside on a CAS server. To work with a SAS data set, you must frst load the data set onto the CAS server. Data loaded on the CAS server are called data tables. This section lists three methods of loading a SAS data set onto a CAS server. In this section, mylib is the name of the caslib that is connected to the mysess CAS session. \n",
       "\u000f You can use a single DATA step to create a data table on the CAS server as follows: \n",
       "data mylib.Sample; input y x @@; datalines; \n",
       ".461 .472 .573.61 4.62 5.68 6.69 7 ; \n",
       "Note that DATA step operations might not work as intended when you perform them on the CAS server instead of the SAS client. \n",
       "\u000f You can create a SAS data set frst, and when it contains exactly what you want, you can use another DATA step to load it onto the CAS server as follows: \n",
       "data Sample; input y x @@; datalines; \n",
       ".461 .472 .573.61 4.62 5.68 6.69 7.788 ; data mylib.Sample; \n",
       "set Sample; run; \n",
       "\u000f You can use the CASUTIL procedure as follows: \n",
       "proc casutil sessref=mysess; load data=Sample casout=\"Sample\"; quit; \n",
       "The CASUTIL procedure can load data onto a CAS server more effciently than the DATA step. For more information about the CASUTIL procedure, see SAS Cloud Analytic Services: Users Guide. \n",
       "The mylib caslib stores the Sample data table, which can be distributed across many machine nodes. You must use a caslib reference in procedures in this book to enable the SAS client machine to communicate with the CAS session. For example, the following SEVSELECT procedure statements use a data table that resides in the mylib caslib: \n",
       "proc sevselect data = mylib.Sample; ...statements...; run; \n",
       "You can delete your data table by using the DELETE procedure as follows: \n",
       "proc delete data = mylib.Sample; run; \n",
       "The Sample data table is accessible only in the mysess session. When you terminate the mysess session, the Sample data table is no longer accessible from the CAS server. If you want your Sample data table to be available to other CAS sessions, then you must promote your data table. For more information about data tables, see SAS Cloud Analytic Services: Users Guide. \n",
       "\n",
       "\n",
       "Syntax Common to SAS Econometrics Procedures \n",
       "CLASS Statement \n",
       "CLASS variable < (options) >:::< variable < (options) >>< / global-options > ; \n",
       "This section applies to the following procedures: CNTSELECT, CPANEL, CQLIM, CSPATIALREG, and SEVSELECT. \n",
       "The CLASS statement names the classifcation variables to be used as explanatory variables in the analysis. These variables enter the analysis not through their values, but through levels to which the unique values are mapped. For more information about these mappings, see the section Levelization of Classifcation Variables on page 89. \n",
       "If the procedure permits a classifcation variable as a response (dependent variable or target), the response does not need to be specifed in the CLASS statement. \n",
       "You can specify options either as individual variable options, by enclosing the options in parentheses after the variable name, or as global-options, by placing them after a slash (/). Global-options are applied to all variables that are specifed in the CLASS statement. If you specify more than one CLASS statement, the global-options that are specifed in any one CLASS statement apply to all CLASS statements. However, individual CLASS variable options override the global-options. \n",
       "Table 4.1 summarizes the values you can use for either an option or a global-option. The options are described in detail in the list that follows Table 4.1.\n",
       "\n",
       "**Source 45** (Rank: N/A)\n",
       " the vertex that has the \n",
       ".k.1/\n",
       "lowest function value and .j is defned as the vertex that has the highest function value in the simplex. The default value is r = 1E8 for the NMSIMP technique and r = 0 otherwise. \n",
       "XSIZE=r \n",
       "specifes the XSIZE parameter of the relative parameter termination criterion. The value of r must be \n",
       "greater than or equal to 0; the default is r D 0. For more information, see the XCONV= option. \n",
       "\n",
       "\n",
       "Details for SAS Econometrics Procedures \n",
       "Levelization of Classifcation Variables \n",
       "This section applies to the following procedures: CNTSELECT, CPANEL, CQLIM, CSPATIALREG, and SEVSELECT. \n",
       "A classifcation variable enters the statistical analysis or model not through its values but through its levels. The process of associating values of a variable with levels is called levelization. \n",
       "During the process of levelization, observations that share the same value are assigned to the same level. The manner in which values are grouped can be affected by the inclusion of formats. The sort order of the levels can be determined by specifying the ORDER= option in the procedure statement. In procedures in this book, you can also control the sorting order separately for each variable in the CLASS statement. \n",
       "Consider the data on nine observations in Table 4.7. The variable A is integer-valued, and the variable X is a continuous variable that has a missing value for the fourth observation. The fourth and ffth columns of Table 4.7 apply two different formats to the variable X. \n",
       "Table 4.7 Example Data for Levelization \n",
       "Obs  A  x  FORMAT  FORMAT  \n",
       "x 3.0  x 3.1  \n",
       "1  2  1.09  1  1.1  \n",
       "2  2  1.13  1  1.1  \n",
       "3  2  1.27  1  1.3  \n",
       "4  3  .  .  .  \n",
       "5  3  2.26  2  2.3  \n",
       "6  3  2.48  2  2.5  \n",
       "7  4  3.34  3  3.3  \n",
       "8  4  3.34  3  3.3  \n",
       "9  4  3.14  3  3.1  \n",
       "\n",
       "By default, levelization of the variables groups the observations by the formatted value of the variable, except for numerical variables for which no explicit format is provided. Numerical variables for which no explicit format is provided are sorted by their internal value. The levelization of the four columns in Table 4.7 leads to the level assignment in Table 4.8. \n",
       "Table 4.8 Values and Levels \n",
       "A  X  FORMAT x 3.0  FORMAT x 3.1  \n",
       "Obs  Value Level  Value Level  Value Level  Value Level  \n",
       "1  2  1  1.09  1  1  1  1.1  1  \n",
       "\n",
       "Table 4.8 continued \n",
       "A  X  FORMAT x 3.0  FORMAT x 3.1  \n",
       "Obs  Value Level  Value Level  Value Level  Value Level  \n",
       "2  2  1  1.13  2  1  1  1.1  1  \n",
       "3  2  1  1.27  3  1  1  1.3  2  \n",
       "4  3  2  .  .  .  .  .  .  \n",
       "5  3  2  2.26  4  2  2  2.3  3  \n",
       "6  3  2  2.48  5  2  2  2.5  4  \n",
       "7  4  3  3.34  7  3  3  3.3  6  \n",
       "8  4  3  3.34  7  3  3  3.3  6  \n",
       "9  4  3  3.14  6  3  3  3.1  5  \n",
       "\n",
       "The sort order for the levels of CLASS variables can be specifed in the ORDER= option in the CLASS statement. \n",
       "When ORDER=FORMATTED (which is the default) is in effect for numeric variables for which you have supplied no explicit format, the levels are ordered by their internal values. To order numeric classifcation levels that have no explicit format by their BEST12. formatted values, you can specify the BEST12. format explicitly for the CLASS variables. \n",
       "Table 4.9 shows how values of the ORDER= option are interpreted. \n",
       "Table 4.9 Interpretation of Values of ORDER= Option \n",
       "Value of ORDER=  Levels Sorted By  \n",
       "FORMATTED  External formatted value, except for numeric variables that have no explicit format, which are sorted by their unformatted (internal) value. The sort order is machine-dependent.  \n",
       "FREQ  Descending frequency count (levels that have the most observations come frst in the order)  \n",
       "INTERNAL  Unformatted value. The sort order is machine-dependent.  \n",
       "\n",
       "For more information about sort order, see the chapter about the SORT procedure in the Base SAS Procedures Guide and the discussion of BY-group processing in the Grouping Data section of SAS Programmers Guide: Essentials. \n",
       "When the MISSING option is specifed in\n",
       "\n",
       "**Source 10** (Rank: N/A)\n",
       " of the log posterior at the posterior mode (Gelman et al. 2004, Appendix B; Schervish 1995, Section 7.4). That is why a normal proposal distribution often works well in practice. The proposal distribution for random walk Metropolis in SAS Econometrics procedures is always the normal distribution. It is generated in the following fashion: \n",
       "\u0012prop \u0018 N.\u0012cur curCOVcur\n",
       ";c / \n",
       "where \u0012prop is the proposed parameter vector, \u0012cur is the previous value of the parameters in the Markov chain, ccur is a scale parameter that is tuned in the tuning phase of the algorithm, and COVcur is an approximation of the posterior covariance that is also tuned in the tuning phase of the algorithm. When there are multiple parameter blocksthat is, in a Metropolis within Gibbs schemeeach block is tuned separately, each with its own ccur and COVcur. In particular for a scalar block, COVcur is also a scalaran approximation to the posterior variance of the only parameter in that block. \n",
       "The tuning phase consists of nstage tuning stages, each ntune iterations long. Between tuning stages, two sorts of adjustments to the proposal distribution are made: adjustments to the scale parameter ccur and adjustments to the proposal covariance matrix COVcur. \n",
       "Scale Tuning \n",
       "The acceptance rate of a Metropolis chain is closely related to its sampling effciency. For a random walk Metropolis algorithm, a high acceptance rate means that most new samples occur right around the current parameter value. Their frequent acceptance means that the Markov chain is moving rather slowly and not fully exploring the parameter space. A low acceptance rate means that the proposed samples are often rejected; hence the chain is not moving much. An effcient random walk Metropolis sampler has an acceptance rate that is neither too high nor too low. The scale parameter ccur in the proposal distribution effectively controls this acceptance probability. Roberts, Gelman, and Gilks (1997) show that if both the target density and the proposal density are normal, the optimal acceptance probability for the Markov chain should be around 0.45 in a one-dimensional problem and should asymptotically approach 0.234 in higher-dimensional problems. The corresponding optimal scale is 2.38. \n",
       "Because of the nature of stochastic simulations, it is impossible to fne-tune a set of variables so that the Metropolis chain has exactly the target acceptance rate that you want. In addition, Roberts and Rosenthal (2001) empirically demonstrate that an acceptance rate between 0.15 and 0.5 is at least 80% effcient, so there is really no need to fne-tune the algorithms to reach an acceptance probability that is within a small tolerance of the optimal values. SAS Econometrics procedures tune the scale parameter so that the observed acceptance rate falls within some tolerance of the target acceptance rate. Let pobs denote the observed acceptance rate in a given tuning stage, let idenote the target acceptance rate, and let \u000fdenote the tolerance. \n",
       "obs obs\n",
       "Then if p <i. \u000f, the value of ccur is decreased. On the other hand, if p >iC \u000f, then the value of ccur is increased. Typically \u000f D 0:075is a good choice, i D 0:45is a good choice for blocks of parameters larger than three or four dimensions, and i D 0:234is a good choice for scalar blocks. \n",
       "When ccur is adjusted between tuning stages, it is done using the following scheme:1 \n",
       "ccur \u0001 .1.i=2/ \n",
       "new D\n",
       "c .1 obs=2/ \n",
       ".p \n",
       "where ccur is the scale from the previous tuning stage; cnew is the new scale for the next stage or any further sampling; pobs is the observed acceptance rate in the previous tuning stage; and iis the target acceptance rate. A good choice for the initial value of ccur is 2.38 because it is optimal for a normal model with a normal proposal. \n",
       "\n",
       "Covariance Tuning \n",
       "When the proposal scale parameter is adjusted between stages, the proposal covariance matrix is also adjusted. To do this, SAS Econometrics procedures take a weighted average of the previously used covariance matrix and the observed covariance matrix in the last tuning stage. The formula to update the covariance matrix is \n",
       "COVnew D COVobs C .1. w/COVcur \n",
       "w \n",
       "1 Roberts, Gelman, and Gilks (1997) and Roberts and Rosenthal (2001) demonstrate that the relationship between acceptance \n",
       "\u0010 p \u0011 probability and scale in a random walk Metropolis scheme is p D 2 . Ic=2 , where cis the scale; p is the acceptance rate;  \n",
       "is the cumulative distribution function of a standard normal distribution; and I \u0011 Ef.f0.x/=f.x//2, where f.x/is the density function of samples. This relationship determines the updating scheme, with I replaced by the identity matrix to simplify calculation. \n",
       "where w is the tuning weight, between 0 and 1; COVcur is the covariance matrix that was used in the previous tuning stage; COVobs is the covariance matrix that was observed in the previous tuning stage; and COVnew is the new covariance matrix that will be used for the next tuning stage or any further sampling. Larger weights cause the sampler to adapt the proposal covariance matrix faster during the tuning process, but if this value is too large, the sampler might never settle into a good choice. Typically, a good tuning weight is w D 0:75. If the initial value of COVcur is very different from the posterior covariance matrix, many \n",
       "tuning stages might be\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "=== END OF CONTEXT ===\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Analyze the context data and token usage that the LLM receives\n",
    "import tiktoken\n",
    "from IPython.display import Markdown, display\n",
    "\n",
    "def analyze_context_and_tokens(result, search_engine):\n",
    "    \"\"\"Analyze the context data and count tokens for different message components.\"\"\"\n",
    "    \n",
    "    # Get the token encoder\n",
    "    encoding = tiktoken.encoding_for_model(\"gpt-4\")\n",
    "    \n",
    "    print(\"=== CONTEXT DATA ANALYSIS ===\\n\")\n",
    "    \n",
    "    # Show context data statistics\n",
    "    if \"entities\" in result.context_data:\n",
    "        entities_df = result.context_data[\"entities\"]\n",
    "        print(f\"📊 Entities in context: {len(entities_df)} entities\")\n",
    "        print(f\"   - Columns: {list(entities_df.columns)}\")\n",
    "        print(f\"   - Sample entity: {entities_df.iloc[0]['entity'] if len(entities_df) > 0 else 'None'}\")\n",
    "    \n",
    "    if \"relationships\" in result.context_data:\n",
    "        relationships_df = result.context_data[\"relationships\"]\n",
    "        print(f\"📊 Relationships in context: {len(relationships_df)} relationships\")\n",
    "        print(f\"   - Columns: {list(relationships_df.columns)}\")\n",
    "        print(f\"   - Sample relationship: {relationships_df.iloc[0]['source']} -> {relationships_df.iloc[0]['target'] if len(relationships_df) > 0 else 'None'}\")\n",
    "    \n",
    "    if \"reports\" in result.context_data:\n",
    "        reports_df = result.context_data[\"reports\"]\n",
    "        print(f\"📊 Community reports in context: {len(reports_df)} reports\")\n",
    "    \n",
    "    if \"sources\" in result.context_data:\n",
    "        sources_df = result.context_data[\"sources\"]\n",
    "        print(f\"📊 Text sources in context: {len(sources_df)} text units\")\n",
    "    \n",
    "    print(\"\\n=== TOKEN ANALYSIS ===\\n\")\n",
    "    \n",
    "    # Reconstruct the context that was sent to the LLM\n",
    "    # This is an approximation of what the LocalSearch builds\n",
    "    \n",
    "    # 1. System prompt\n",
    "    with open(\"/home/chuaxu/projects/graphrag/ragsas/prompts/local_search_system_prompt.txt\", \"r\") as f:\n",
    "        system_prompt = f.read()\n",
    "\n",
    "    # 2. Context data formatting (full version of what LocalSearch does)\n",
    "    context_parts = []\n",
    "    \n",
    "    # Add entities context (FULL - no truncation)\n",
    "    if \"entities\" in result.context_data and len(result.context_data[\"entities\"]) > 0:\n",
    "        entities_context = \"## Relevant Entities:\\n\\n\"\n",
    "        for _, entity in result.context_data[\"entities\"].iterrows():  # Show ALL entities\n",
    "            desc = entity.get('description', 'No description')\n",
    "            rank = entity.get('rank', 'N/A')\n",
    "            entities_context += f\"**{entity['entity']}** (Rank: {rank})\\n\"\n",
    "            entities_context += f\"Description: {desc}\\n\\n\"\n",
    "        context_parts.append(entities_context)\n",
    "    \n",
    "    # Add relationships context (FULL - no truncation)\n",
    "    if \"relationships\" in result.context_data and len(result.context_data[\"relationships\"]) > 0:\n",
    "        relationships_context = \"## Relevant Relationships:\\n\\n\"\n",
    "        for _, rel in result.context_data[\"relationships\"].iterrows():  # Show ALL relationships\n",
    "            desc = rel.get('description', 'No description')\n",
    "            weight = rel.get('weight', 'N/A')\n",
    "            relationships_context += f\"**{rel['source']} → {rel['target']}** (Weight: {weight})\\n\"\n",
    "            relationships_context += f\"Description: {desc}\\n\\n\"\n",
    "        context_parts.append(relationships_context)\n",
    "    \n",
    "    # Add community reports context (FULL - no truncation)\n",
    "    if \"reports\" in result.context_data and len(result.context_data[\"reports\"]) > 0:\n",
    "        reports_context = \"## Relevant Community Reports:\\n\\n\"\n",
    "        for _, report in result.context_data[\"reports\"].iterrows():  # Show ALL reports\n",
    "            title = report.get('title', 'Untitled Report')\n",
    "            content = report.get('content', report.get('summary', 'No content'))\n",
    "            rank = report.get('rank', 'N/A')\n",
    "            reports_context += f\"**{title}** (Rank: {rank})\\n\"\n",
    "            reports_context += f\"{content}\\n\\n\"\n",
    "        context_parts.append(reports_context)\n",
    "    \n",
    "    # Add sources context (FULL - no truncation)\n",
    "    if \"sources\" in result.context_data and len(result.context_data[\"sources\"]) > 0:\n",
    "        sources_context = \"## Relevant Text Sources:\\n\\n\"\n",
    "        for _, source in result.context_data[\"sources\"].iterrows():  # Show ALL sources\n",
    "            text_content = source.get('text', source.get('content', 'No content'))\n",
    "            source_id = source.get('id', 'Unknown')\n",
    "            rank = source.get('rank', 'N/A')\n",
    "            sources_context += f\"**Source {source_id}** (Rank: {rank})\\n\"\n",
    "            sources_context += f\"{text_content}\\n\\n\"\n",
    "        context_parts.append(sources_context)\n",
    "    \n",
    "    # 3. User question\n",
    "    user_question = question  # From the previous cell\n",
    "    \n",
    "    # Combine all context\n",
    "    full_context = \"\\n\".join(context_parts)\n",
    "    \n",
    "    # 4. Calculate tokens for each component\n",
    "    system_prompt_tokens = len(encoding.encode(system_prompt))\n",
    "    context_tokens = len(encoding.encode(full_context))\n",
    "    user_question_tokens = len(encoding.encode(user_question))\n",
    "    response_tokens = len(encoding.encode(result.response))\n",
    "    \n",
    "    total_input_tokens = system_prompt_tokens + context_tokens + user_question_tokens\n",
    "    total_tokens = total_input_tokens + response_tokens\n",
    "    \n",
    "    print(f\"🔢 Token Breakdown:\")\n",
    "    print(f\"   - System prompt: {system_prompt_tokens:,} tokens\")\n",
    "    print(f\"   - Context data: {context_tokens:,} tokens\")\n",
    "    print(f\"   - User question: {user_question_tokens:,} tokens\")\n",
    "    print(f\"   - Total INPUT: {total_input_tokens:,} tokens\")\n",
    "    print(f\"   - Response: {response_tokens:,} tokens\")\n",
    "    print(f\"   - TOTAL MESSAGE: {total_tokens:,} tokens\")\n",
    "    \n",
    "    # Show model limits\n",
    "    model_limit = 128_000  # GPT-4o limit\n",
    "    print(f\"\\n📏 Model Capacity:\")\n",
    "    print(f\"   - Model limit: {model_limit:,} tokens\")\n",
    "    print(f\"   - Used: {total_tokens:,} tokens ({total_tokens/model_limit*100:.1f}%)\")\n",
    "    print(f\"   - Remaining: {model_limit - total_tokens:,} tokens\")\n",
    "    \n",
    "    if total_tokens > model_limit:\n",
    "        print(\"   ⚠️  WARNING: Token count exceeds model limit!\")\n",
    "    elif total_tokens > model_limit * 0.9:\n",
    "        print(\"   ⚠️  WARNING: Token count is near model limit!\")\n",
    "    else:\n",
    "        print(\"   ✅ Token count is within safe limits\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"=== FULL CONTEXT SENT TO LLM ===\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Display the complete context in a nice formatted way\n",
    "    print(f\"\\n� SYSTEM PROMPT (Skipped)\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    print(f\"\\n🔵 USER QUESTION:\")\n",
    "    print(\"-\" * 40)\n",
    "    print(user_question)\n",
    "    \n",
    "    print(f\"\\n🔵 CONTEXT DATA ({context_tokens:,} tokens):\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    # Display full context as Markdown for better formatting\n",
    "    display(Markdown(full_context))\n",
    "    \n",
    "    print(\"=\"*80)\n",
    "    print(\"=== END OF CONTEXT ===\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    return {\n",
    "        \"system_prompt_tokens\": system_prompt_tokens,\n",
    "        \"context_tokens\": context_tokens,\n",
    "        \"user_question_tokens\": user_question_tokens,\n",
    "        \"response_tokens\": response_tokens,\n",
    "        \"total_tokens\": total_tokens,\n",
    "        \"context_data\": result.context_data,\n",
    "        \"full_context\": full_context\n",
    "    }\n",
    "\n",
    "# Run the analysis\n",
    "token_analysis = analyze_context_and_tokens(result, search_engine)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Understanding Relationship Weights in GraphRAG\n",
    "\n",
    "In GraphRAG, the **weight** property on relationships represents the **strength** or **importance** of the connection between two entities. Here's what it means:\n",
    "\n",
    "## What is Weight?\n",
    "\n",
    "**Weight** is a numerical value that indicates:\n",
    "- **Frequency**: How often two entities appear together in the source documents\n",
    "- **Co-occurrence strength**: The statistical significance of their relationship\n",
    "- **Semantic closeness**: How tightly connected the entities are in the knowledge graph\n",
    "\n",
    "## How is Weight Calculated?\n",
    "\n",
    "The weight is typically derived from:\n",
    "1. **Text co-occurrence**: How many times the entities appear in the same text units/chunks\n",
    "2. **Window proximity**: How close the entities appear to each other in the text\n",
    "3. **Relationship strength**: The confidence level of the extracted relationship\n",
    "4. **Document frequency**: Across how many documents the relationship appears\n",
    "\n",
    "## Weight Values\n",
    "\n",
    "- **Higher weights** (e.g., 8.0, 10.0): Strong, frequently occurring relationships\n",
    "- **Lower weights** (e.g., 1.0, 2.0): Weaker or less frequent relationships\n",
    "- **Weight = 1.0**: Often the default/minimum weight for detected relationships\n",
    "\n",
    "## Usage in GraphRAG\n",
    "\n",
    "Weights are used for:\n",
    "- **Ranking relationships**: More important relationships get higher priority in search results\n",
    "- **Graph visualization**: Thicker edges represent stronger relationships (as seen in the yfiles visualization)\n",
    "- **Context selection**: Higher-weight relationships are more likely to be included in LLM context\n",
    "- **Graph algorithms**: Centrality and community detection algorithms use weights to identify key entities\n",
    "\n",
    "Let's examine the weights in our current result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== RELATIONSHIP WEIGHT ANALYSIS ===\n",
      "\n",
      "📊 Weight Statistics:\n",
      "   - Total relationships: 20\n",
      "   - Relationships with weights: 20\n",
      "   - Weight range: 1.00 to 23.00\n",
      "   - Average weight: 7.35\n",
      "   - Median weight: 8.00\n",
      "\n",
      "🔝 Top 5 Strongest Relationships (by weight):\n",
      "   PROC CCDM → SAS ECONOMETRICS (Weight: 23.00)\n",
      "      Description: PROC CCDM is a specialized procedure integrated within the SAS Econometrics software suite, designed...\n",
      "\n",
      "   SAS ECONOMETRICS → SAS/ETS (Weight: 16.00)\n",
      "      Description: SAS Econometrics and SAS/ETS are software suites designed to facilitate econometric and time series ...\n",
      "\n",
      "   SAS INSTITUTE INC. → SAS ECONOMETRICS (Weight: 9.00)\n",
      "      Description: SAS Institute Inc. developed and provides the SAS Econometrics suite...\n",
      "\n",
      "   SAS → SAS ECONOMETRICS (Weight: 9.00)\n",
      "      Description: SAS Econometrics is a product within the SAS software suite...\n",
      "\n",
      "   SAS ECONOMETRICS → UCM PROCEDURE (Weight: 9.00)\n",
      "      Description: The UCM Procedure is a part of the SAS Econometrics software suite...\n",
      "\n",
      "🔻 Bottom 5 Weakest Relationships (by weight):\n",
      "   SAS → SAS ECONOMETRICS PROCEDURES (Weight: 1.00)\n",
      "      Description: SAS Econometrics Procedures are part of the SAS software suite for econometric analysis...\n",
      "\n",
      "   SAS/ETS → UCM PROCEDURE (Weight: 1.00)\n",
      "      Description: The UCM Procedure is a part of the SAS/ETS software suite...\n",
      "\n",
      "   PROC DEEPPRICE → MODEL (Weight: 1.00)\n",
      "      Description: PROC DEEPPRICE includes a MODEL statement for specifying outcome models and DNN fitting...\n",
      "\n",
      "   SAS/ETS → PROC UCM (Weight: 1.00)\n",
      "      Description: PROC UCM is a procedure available in the SAS/ETS software suite...\n",
      "\n",
      "   PROC SEVSELECT → SAS ECONOMETRICS (Weight: 1.00)\n",
      "      Description: PROC SEVSELECT is a procedure within SAS Econometrics...\n",
      "\n",
      "📈 Weight Distribution:\n",
      "   - Weight 0-1: 0 relationships (0.0%)\n",
      "   - Weight 1-2: 5 relationships (25.0%)\n",
      "   - Weight 2-5: 0 relationships (0.0%)\n",
      "   - Weight 5-10: 13 relationships (65.0%)\n",
      "   - Weight 10+: 2 relationships (10.0%)\n"
     ]
    }
   ],
   "source": [
    "# Analyze relationship weights in our current search result\n",
    "print(\"=== RELATIONSHIP WEIGHT ANALYSIS ===\\n\")\n",
    "\n",
    "if \"relationships\" in result.context_data:\n",
    "    relationships_df = result.context_data[\"relationships\"]\n",
    "    \n",
    "    if 'weight' in relationships_df.columns:\n",
    "        # Convert weights to numeric, handling any string values\n",
    "        relationships_df['weight_numeric'] = pd.to_numeric(relationships_df['weight'], errors='coerce')\n",
    "        weights = relationships_df['weight_numeric'].dropna()\n",
    "        \n",
    "        if len(weights) > 0:\n",
    "            print(f\"📊 Weight Statistics:\")\n",
    "            print(f\"   - Total relationships: {len(relationships_df)}\")\n",
    "            print(f\"   - Relationships with weights: {len(weights)}\")\n",
    "            print(f\"   - Weight range: {weights.min():.2f} to {weights.max():.2f}\")\n",
    "            print(f\"   - Average weight: {weights.mean():.2f}\")\n",
    "            print(f\"   - Median weight: {weights.median():.2f}\")\n",
    "            \n",
    "            print(f\"\\n🔝 Top 5 Strongest Relationships (by weight):\")\n",
    "            top_relationships = relationships_df.nlargest(5, 'weight_numeric')[['source', 'target', 'weight_numeric', 'description']]\n",
    "            for idx, row in top_relationships.iterrows():\n",
    "                weight_val = row['weight_numeric']\n",
    "                if pd.notna(weight_val):\n",
    "                    print(f\"   {row['source']} → {row['target']} (Weight: {weight_val:.2f})\")\n",
    "                else:\n",
    "                    print(f\"   {row['source']} → {row['target']} (Weight: N/A)\")\n",
    "                desc = str(row['description'])[:100] if pd.notna(row['description']) else \"No description\"\n",
    "                print(f\"      Description: {desc}...\")\n",
    "                print()\n",
    "            \n",
    "            print(f\"🔻 Bottom 5 Weakest Relationships (by weight):\")\n",
    "            bottom_relationships = relationships_df.nsmallest(5, 'weight_numeric')[['source', 'target', 'weight_numeric', 'description']]\n",
    "            for idx, row in bottom_relationships.iterrows():\n",
    "                weight_val = row['weight_numeric']\n",
    "                if pd.notna(weight_val):\n",
    "                    print(f\"   {row['source']} → {row['target']} (Weight: {weight_val:.2f})\")\n",
    "                else:\n",
    "                    print(f\"   {row['source']} → {row['target']} (Weight: N/A)\")\n",
    "                desc = str(row['description'])[:100] if pd.notna(row['description']) else \"No description\"\n",
    "                print(f\"      Description: {desc}...\")\n",
    "                print()\n",
    "            \n",
    "            # Weight distribution\n",
    "            print(f\"📈 Weight Distribution:\")\n",
    "            weight_bins = [0, 1, 2, 5, 10, float('inf')]\n",
    "            weight_labels = ['0-1', '1-2', '2-5', '5-10', '10+']\n",
    "            \n",
    "            for i, (low, high) in enumerate(zip(weight_bins[:-1], weight_bins[1:])):\n",
    "                if high == float('inf'):\n",
    "                    count = len(weights[weights >= low])\n",
    "                    label = weight_labels[i]\n",
    "                else:\n",
    "                    count = len(weights[(weights >= low) & (weights < high)])\n",
    "                    label = weight_labels[i]\n",
    "                \n",
    "                percentage = count / len(weights) * 100 if len(weights) > 0 else 0\n",
    "                print(f\"   - Weight {label}: {count} relationships ({percentage:.1f}%)\")\n",
    "        else:\n",
    "            print(\"❌ No valid numeric weights found in relationships data\")\n",
    "            print(f\"Sample weight values: {relationships_df['weight'].head().tolist()}\")\n",
    "    else:\n",
    "        print(\"❌ No 'weight' column found in relationships data\")\n",
    "        print(f\"Available columns: {list(relationships_df.columns)}\")\n",
    "else:\n",
    "    print(\"❌ No relationships found in context data\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.11.13)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
